{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQrdp+k0R4xQfKjzoxTnJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelDashti/Smart-Chatbot/blob/master/AIChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9-4o1GQaVXK",
        "outputId": "e7f4a8b3-f56e-40b3-b39b-326aacdb7958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Smart-Chatbot'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MelDashti/Smart-Chatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/')"
      ],
      "metadata": {
        "id": "CkBr_Tm8bkN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bva93-IznKoi",
        "outputId": "1b7cf91e-1b58-47f3-cecc-f57c7e18228c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Using cached trl-0.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (0.34.2)\n",
            "Collecting datasets>=2.21.0 (from trl)\n",
            "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.3)\n",
            "Collecting transformers>=4.46.0 (from trl)\n",
            "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (4.66.6)\n",
            "Collecting xxhash (from datasets>=2.21.0->trl)\n",
            "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl)\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl)\n",
            "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.10.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.9.11)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers>=4.46.0->trl)\n",
            "  Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.21.0->trl) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (3.0.2)\n",
            "Downloading trl-0.12.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, tokenizers, transformers, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 tokenizers-0.20.3 transformers-4.46.2 trl-0.12.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import all the necessary libraries."
      ],
      "metadata": {
        "id": "jHwNrW7snGzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "PBB6UDGhjSDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Saving model\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "ZOElg_rUm8Pj",
        "outputId": "7d20a0de-dead-40fa-cbcc-2cfeb111f331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'trl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-47bd71ed1ebe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextStreamer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_templates\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_chat_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WE are using lama with 1B parameters.\n",
        "# Max length set at 5020, max num of tokens the model can handle in a single input\n",
        "\n",
        "max_seq_length = 1024  # imo its enough for a simple AI chatbot\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True, # enables efficient 4-bit quantization\n",
        "    dtype=None,\n",
        ")\n",
        "# we use parameter efficient fine tuning like we learnt in LLM which applied LORA techniques. This approach\n",
        "# focuses on fine tuning only specific layers or parts of the model, rather than the entire network.\n",
        "# r = 16 and lora_alpha = 16 adjusts the complexity and scaling of these adaptations.\n",
        "#  target modules specifies which layers of the model should be adapted, which include key components involed\n",
        "# in attention mechanisms like q_proj and k_proj and v_proj.\n",
        "# use_rslora activates Rank stabalized LORA, which improves the stability of the fine tuning process.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    # q_proj, k_proj, v_proj: Handle the query, key, and value projections in the attention mechanism, essential for capturing contextual information.\n",
        "    # up_proj, down_proj: Layers in feedforward networks. o_proj: Combines attention heads’ output. gate_proj: Controls flow in certain feedforward networks.\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 32,\n",
        "    loftq_config = None,\n",
        ")\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "Ob0ngnUstSMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Data For The Arol Group Chatbot.\n",
        "This part will be replaced after we data scrap the websites later."
      ],
      "metadata": {
        "id": "W-JOs6txqqtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [\n",
        "    {\n",
        "        \"Query\": \"What are the different types of bottle caps AROL Group manufactures?\",\n",
        "        \"Response\": \"AROL Group manufactures various types of bottle caps, including screw caps, crown caps, and corks, designed for different industries and bottle types.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"Can you explain how the capping machines work?\",\n",
        "        \"Response\": \"Capping machines from AROL use precision automation to apply caps on bottles at high speed, ensuring a tight and consistent seal to prevent leakage and maintain product quality.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"What industries does AROL Group provide solutions for?\",\n",
        "        \"Response\": \"AROL Group provides capping and sealing solutions for industries such as food & beverage, personal care, pharmaceuticals, and chemical products.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"How do AROL's machines ensure product safety?\",\n",
        "        \"Response\": \"AROL machines use advanced inspection systems to detect any defects in capping, ensuring product safety by maintaining a consistent and contamination-free seal.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"Does AROL offer maintenance support for their capping machines?\",\n",
        "        \"Response\": \"Yes, AROL offers comprehensive maintenance support, including regular inspections, troubleshooting, and part replacements to keep machines running efficiently.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"What are the environmental impacts of AROL's products?\",\n",
        "        \"Response\": \"AROL is committed to sustainability, designing energy-efficient machines and encouraging the use of recyclable caps to minimize environmental impact.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"Are AROL's machines customizable for different bottle sizes?\",\n",
        "        \"Response\": \"Yes, AROL's capping machines are highly customizable to fit a wide range of bottle sizes, cap types, and production line configurations.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"How does AROL ensure precision in high-speed bottling?\",\n",
        "        \"Response\": \"AROL ensures precision with advanced robotics and servo-controlled systems that maintain accuracy and consistency, even at high production speeds.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"What quality standards does AROL adhere to?\",\n",
        "        \"Response\": \"AROL adheres to strict international quality standards, including ISO certifications, to ensure the reliability and safety of its products.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"Does AROL offer training for operating their capping machines?\",\n",
        "        \"Response\": \"Yes, AROL offers training programs to help operators understand and efficiently use their capping machines, covering safety, maintenance, and operational techniques.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "0pjySTsGqKNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we convert the sample data to prompt template that we want to use for our chatbot."
      ],
      "metadata": {
        "id": "rUJHel1SrC4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# first we convert sample data to a DataFrame\n",
        "df = pd.DataFrame(sample_data)\n",
        "\n",
        "# prompt template\n",
        "data_prompt = \"\"\"\n",
        "You are a customer support assistant for AROL Group, specialized in bottle caps and capping technologies.\n",
        "Your goal is to provide accurate, clear, and helpful responses to queries regarding our products, processes, and technologies.\n",
        "\n",
        "### Input:\n",
        "{query}\n",
        "\n",
        "### Instructions:\n",
        "- Provide a concise and informative response regarding bottle cap manufacturing or capping technology.\n",
        "- If there are any technical details or product features mentioned, explain them in simple terms.\n",
        "- If the query highlights any concerns, provide recommendations or solutions.\n",
        "- Ensure the response is relevant to the specific question asked.\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Keep this for the end-of-sequence token\n",
        "\n",
        "\n",
        "def formatting_prompt(df):\n",
        "  inputs = df[\"Query\"]\n",
        "  outputs = df[\"Response\"]\n",
        "  texts = []\n",
        "  for input_, output in zip(inputs, outputs):\n",
        "    text = data_prompt.format(input_, output) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "  return {\"text\": texts,}\n",
        "\n",
        "formatted_data = formatting_prompt(df)\n",
        "# Convert formatted data into the form the model expects\n",
        "print(formatted_data[\"text\"][:2])  # Preview the first two formatted examples\n"
      ],
      "metadata": {
        "id": "lSuHCA3vq7jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = Dataset.from_pandas(df) # here we convert pandas dataframe into a hugging face dataset object\n",
        "training_data = training_data.map(formatting_prompt, batched=True) # Here we apply the formatting func to each element of the dataset using map method.\n"
      ],
      "metadata": {
        "id": "ko-VWGXAXJbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model with the Trainer API\n",
        "Goal: Use the Trainer API to actually fine-tune the model on the formatted dataset. This step leverages all previous configurations for efficient training.\n",
        "\n",
        "Process:\n",
        "\n",
        "The formatted data is fed into the Trainer as input for model training.\n",
        "The Trainer uses LoRA fine-tuning to adjust only specific layers, optimizing performance while keeping memory usage low.\n",
        "Purpose: This final step leverages all previous configurations and formatted data to train the model. The Trainer applies gradient updates to the specified layers according to LoRA parameters, optimizing the model for the task without requiring massive resources."
      ],
      "metadata": {
        "id": "ktcXA5dvXWjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=training_data,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=40,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "HOUh_2gzXL8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}