{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelDashti/Smart-Chatbot/blob/master/AIChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9-4o1GQaVXK",
        "outputId": "e7f4a8b3-f56e-40b3-b39b-326aacdb7958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Smart-Chatbot'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MelDashti/Smart-Chatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/') # Here we set the working directory\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"dataset.csv\")  # CSV\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_json(\"dataset.json\")  # JSON\n",
        "\n",
        "with open(\"dataset.txt\", \"r\") as f: # TXT\n",
        "    data = f.readlines()\n"
      ],
      "metadata": {
        "id": "CkBr_Tm8bkN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install trl\n",
        "!pip install unsloth\n",
        "\n",
        "# Standard library imports\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from datasets import Dataset\n",
        "\n",
        "# Configure warnings and matplotlib\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Set device (GPU if available, otherwise CPU)\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "id": "ZOElg_rUm8Pj",
        "outputId": "7d20a0de-dead-40fa-cbcc-2cfeb111f331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'trl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-47bd71ed1ebe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextStreamer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_templates\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_chat_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WE are using lama with 1B parameters.\n",
        "# Max length set at 5020, max num of tokens the model can handle in a single input\n",
        "\n",
        "max_seq_length = 1024  # imo its enough for a simple AI chatbot\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True, # enables efficient 4-bit quantization\n",
        "    dtype=None,\n",
        ")\n",
        "# we use parameter efficient fine tuning like we learnt in LLM which applied LORA techniques. This approach\n",
        "# focuses on fine tuning only specific layers or parts of the model, rather than the entire network.\n",
        "# r = 16 and lora_alpha = 16 adjusts the complexity and scaling of these adaptations.\n",
        "#  target modules specifies which layers of the model should be adapted, which include key components involed\n",
        "# in attention mechanisms like q_proj and k_proj and v_proj.\n",
        "# use_rslora activates Rank stabalized LORA, which improves the stability of the fine tuning process.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    # q_proj, k_proj, v_proj: Handle the query, key, and value projections in the attention mechanism, essential for capturing contextual information.\n",
        "    # up_proj, down_proj: Layers in feedforward networks. o_proj: Combines attention headsâ€™ output. gate_proj: Controls flow in certain feedforward networks.\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state = 32,\n",
        "    loftq_config = None,\n",
        ")\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "id": "Ob0ngnUstSMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Data For The Arol Group Chatbot.\n",
        "This part will be replaced after we data scrap the websites later."
      ],
      "metadata": {
        "id": "W-JOs6txqqtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = [\n",
        "    {\n",
        "        \"Query\": \"What are the different types of bottle caps AROL Group manufactures?\",\n",
        "        \"Response\": \"AROL Group manufactures various types of bottle caps, including screw caps, crown caps, and corks, designed for different industries and bottle types.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"Can you explain how the capping machines work?\",\n",
        "        \"Response\": \"Capping machines from AROL use precision automation to apply caps on bottles at high speed, ensuring a tight and consistent seal to prevent leakage and maintain product quality.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"What industries does AROL Group provide solutions for?\",\n",
        "        \"Response\": \"AROL Group provides capping and sealing solutions for industries such as food & beverage, personal care, pharmaceuticals, and chemical products.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"How do AROL's machines ensure product safety?\",\n",
        "        \"Response\": \"AROL machines use advanced inspection systems to detect any defects in capping, ensuring product safety by maintaining a consistent and contamination-free seal.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"Does AROL offer maintenance support for their capping machines?\",\n",
        "        \"Response\": \"Yes, AROL offers comprehensive maintenance support, including regular inspections, troubleshooting, and part replacements to keep machines running efficiently.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"What are the environmental impacts of AROL's products?\",\n",
        "        \"Response\": \"AROL is committed to sustainability, designing energy-efficient machines and encouraging the use of recyclable caps to minimize environmental impact.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"Are AROL's machines customizable for different bottle sizes?\",\n",
        "        \"Response\": \"Yes, AROL's capping machines are highly customizable to fit a wide range of bottle sizes, cap types, and production line configurations.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"How does AROL ensure precision in high-speed bottling?\",\n",
        "        \"Response\": \"AROL ensures precision with advanced robotics and servo-controlled systems that maintain accuracy and consistency, even at high production speeds.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"What quality standards does AROL adhere to?\",\n",
        "        \"Response\": \"AROL adheres to strict international quality standards, including ISO certifications, to ensure the reliability and safety of its products.\"\n",
        "    },\n",
        "    {\n",
        "        \"Query\": \"Does AROL offer training for operating their capping machines?\",\n",
        "        \"Response\": \"Yes, AROL offers training programs to help operators understand and efficiently use their capping machines, covering safety, maintenance, and operational techniques.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "0pjySTsGqKNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we convert the sample data to prompt template that we want to use for our chatbot."
      ],
      "metadata": {
        "id": "rUJHel1SrC4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# first we convert sample data to a DataFrame\n",
        "df = pd.DataFrame(sample_data)\n",
        "\n",
        "# prompt template\n",
        "data_prompt = \"\"\"\n",
        "You are a customer support assistant for AROL Group, specialized in bottle caps and capping technologies.\n",
        "Your goal is to provide accurate, clear, and helpful responses to queries regarding our products, processes, and technologies.\n",
        "\n",
        "### Input:\n",
        "{query}\n",
        "\n",
        "### Instructions:\n",
        "- Provide a concise and informative response regarding bottle cap manufacturing or capping technology.\n",
        "- If there are any technical details or product features mentioned, explain them in simple terms.\n",
        "- If the query highlights any concerns, provide recommendations or solutions.\n",
        "- Ensure the response is relevant to the specific question asked.\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Keep this for the end-of-sequence token\n",
        "\n",
        "\n",
        "def formatting_prompt(df):\n",
        "  inputs = df[\"Query\"]\n",
        "  outputs = df[\"Response\"]\n",
        "  texts = []\n",
        "  for input_, output in zip(inputs, outputs):\n",
        "    text = data_prompt.format(input_, output) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "  return {\"text\": texts,}\n",
        "\n",
        "formatted_data = formatting_prompt(df)\n",
        "# Convert formatted data into the form the model expects\n",
        "print(formatted_data[\"text\"][:2])  # Preview the first two formatted examples\n"
      ],
      "metadata": {
        "id": "lSuHCA3vq7jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = Dataset.from_pandas(df) # here we convert pandas dataframe into a hugging face dataset object\n",
        "training_data = training_data.map(formatting_prompt, batched=True) # Here we apply the formatting func to each element of the dataset using map method.\n"
      ],
      "metadata": {
        "id": "ko-VWGXAXJbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model with the Trainer API\n",
        "Goal: Use the Trainer API to actually fine-tune the model on the formatted dataset. This step leverages all previous configurations for efficient training.\n",
        "\n",
        "Process:\n",
        "\n",
        "The formatted data is fed into the Trainer as input for model training.\n",
        "The Trainer uses LoRA fine-tuning to adjust only specific layers, optimizing performance while keeping memory usage low.\n",
        "Purpose: This final step leverages all previous configurations and formatted data to train the model. The Trainer applies gradient updates to the specified layers according to LoRA parameters, optimizing the model for the task without requiring massive resources."
      ],
      "metadata": {
        "id": "ktcXA5dvXWjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=training_data,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=40,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "HOUh_2gzXL8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}