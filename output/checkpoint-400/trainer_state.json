{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 39.02439024390244,
  "eval_steps": 500,
  "global_step": 400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 0.6448410749435425,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.0803,
      "step": 1
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 0.6553206443786621,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.0761,
      "step": 2
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 0.6072316765785217,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.0692,
      "step": 3
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 0.5928677320480347,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.9995,
      "step": 4
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.561011791229248,
      "learning_rate": 0.00015,
      "loss": 0.966,
      "step": 5
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 0.559076726436615,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.9484,
      "step": 6
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 0.5759463310241699,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.9022,
      "step": 7
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 0.6330788731575012,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.8518,
      "step": 8
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.7191322445869446,
      "learning_rate": 0.00027,
      "loss": 0.806,
      "step": 9
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.8969778418540955,
      "learning_rate": 0.0003,
      "loss": 0.7485,
      "step": 10
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 2.1416563987731934,
      "learning_rate": 0.0002992307692307692,
      "loss": 1.2004,
      "step": 11
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 1.0260063409805298,
      "learning_rate": 0.00029846153846153846,
      "loss": 0.6452,
      "step": 12
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 1.0223547220230103,
      "learning_rate": 0.0002976923076923077,
      "loss": 0.5802,
      "step": 13
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 0.8375058174133301,
      "learning_rate": 0.0002969230769230769,
      "loss": 0.5323,
      "step": 14
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 0.7667170763015747,
      "learning_rate": 0.00029615384615384616,
      "loss": 0.501,
      "step": 15
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 0.7796440124511719,
      "learning_rate": 0.0002953846153846154,
      "loss": 0.4935,
      "step": 16
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 0.6573350429534912,
      "learning_rate": 0.0002946153846153846,
      "loss": 0.4783,
      "step": 17
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 0.6264709830284119,
      "learning_rate": 0.0002938461538461538,
      "loss": 0.4439,
      "step": 18
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 0.6179354786872864,
      "learning_rate": 0.00029307692307692303,
      "loss": 0.4261,
      "step": 19
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 0.6694778203964233,
      "learning_rate": 0.0002923076923076923,
      "loss": 0.4404,
      "step": 20
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 1.191928744316101,
      "learning_rate": 0.0002915384615384615,
      "loss": 0.7056,
      "step": 21
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 0.5882424116134644,
      "learning_rate": 0.00029076923076923073,
      "loss": 0.3994,
      "step": 22
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 0.5501312613487244,
      "learning_rate": 0.00029,
      "loss": 0.3832,
      "step": 23
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 0.578950047492981,
      "learning_rate": 0.0002892307692307692,
      "loss": 0.3368,
      "step": 24
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 0.5751059651374817,
      "learning_rate": 0.00028846153846153843,
      "loss": 0.3769,
      "step": 25
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 0.515271008014679,
      "learning_rate": 0.00028769230769230765,
      "loss": 0.3714,
      "step": 26
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 0.5880807638168335,
      "learning_rate": 0.0002869230769230769,
      "loss": 0.336,
      "step": 27
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 0.5769121646881104,
      "learning_rate": 0.00028615384615384614,
      "loss": 0.3515,
      "step": 28
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 0.5423992872238159,
      "learning_rate": 0.00028538461538461535,
      "loss": 0.3315,
      "step": 29
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 0.5411848425865173,
      "learning_rate": 0.00028461538461538457,
      "loss": 0.3275,
      "step": 30
    },
    {
      "epoch": 3.024390243902439,
      "grad_norm": 0.9943540692329407,
      "learning_rate": 0.0002838461538461538,
      "loss": 0.5284,
      "step": 31
    },
    {
      "epoch": 3.1219512195121952,
      "grad_norm": 0.5418949723243713,
      "learning_rate": 0.00028307692307692306,
      "loss": 0.2931,
      "step": 32
    },
    {
      "epoch": 3.2195121951219514,
      "grad_norm": 0.5049504041671753,
      "learning_rate": 0.00028230769230769227,
      "loss": 0.2887,
      "step": 33
    },
    {
      "epoch": 3.317073170731707,
      "grad_norm": 0.5203952193260193,
      "learning_rate": 0.0002815384615384615,
      "loss": 0.2909,
      "step": 34
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 0.5657156109809875,
      "learning_rate": 0.00028076923076923076,
      "loss": 0.2718,
      "step": 35
    },
    {
      "epoch": 3.5121951219512195,
      "grad_norm": 0.5293582081794739,
      "learning_rate": 0.00028,
      "loss": 0.2999,
      "step": 36
    },
    {
      "epoch": 3.6097560975609757,
      "grad_norm": 0.5593445301055908,
      "learning_rate": 0.0002792307692307692,
      "loss": 0.2909,
      "step": 37
    },
    {
      "epoch": 3.7073170731707314,
      "grad_norm": 0.6021500825881958,
      "learning_rate": 0.00027846153846153846,
      "loss": 0.2818,
      "step": 38
    },
    {
      "epoch": 3.8048780487804876,
      "grad_norm": 0.5481913685798645,
      "learning_rate": 0.0002776923076923077,
      "loss": 0.2742,
      "step": 39
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 0.5634917616844177,
      "learning_rate": 0.0002769230769230769,
      "loss": 0.2669,
      "step": 40
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.1523191928863525,
      "learning_rate": 0.0002761538461538461,
      "loss": 0.4707,
      "step": 41
    },
    {
      "epoch": 4.097560975609756,
      "grad_norm": 0.5521900057792664,
      "learning_rate": 0.00027538461538461533,
      "loss": 0.2489,
      "step": 42
    },
    {
      "epoch": 4.195121951219512,
      "grad_norm": 0.5935333371162415,
      "learning_rate": 0.0002746153846153846,
      "loss": 0.2608,
      "step": 43
    },
    {
      "epoch": 4.2926829268292686,
      "grad_norm": 0.5528760552406311,
      "learning_rate": 0.0002738461538461538,
      "loss": 0.2424,
      "step": 44
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 0.5291313529014587,
      "learning_rate": 0.00027307692307692303,
      "loss": 0.2463,
      "step": 45
    },
    {
      "epoch": 4.487804878048781,
      "grad_norm": 0.5359466075897217,
      "learning_rate": 0.0002723076923076923,
      "loss": 0.2372,
      "step": 46
    },
    {
      "epoch": 4.585365853658536,
      "grad_norm": 0.5474058389663696,
      "learning_rate": 0.0002715384615384615,
      "loss": 0.2251,
      "step": 47
    },
    {
      "epoch": 4.682926829268292,
      "grad_norm": 0.568253219127655,
      "learning_rate": 0.00027076923076923073,
      "loss": 0.2246,
      "step": 48
    },
    {
      "epoch": 4.780487804878049,
      "grad_norm": 0.5299636721611023,
      "learning_rate": 0.00027,
      "loss": 0.2259,
      "step": 49
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 0.5551742911338806,
      "learning_rate": 0.0002692307692307692,
      "loss": 0.2322,
      "step": 50
    },
    {
      "epoch": 4.975609756097561,
      "grad_norm": 0.5764652490615845,
      "learning_rate": 0.00026846153846153844,
      "loss": 0.2188,
      "step": 51
    },
    {
      "epoch": 5.073170731707317,
      "grad_norm": 1.1675645112991333,
      "learning_rate": 0.0002676923076923077,
      "loss": 0.3787,
      "step": 52
    },
    {
      "epoch": 5.170731707317073,
      "grad_norm": 0.5036147236824036,
      "learning_rate": 0.00026692307692307687,
      "loss": 0.1981,
      "step": 53
    },
    {
      "epoch": 5.2682926829268295,
      "grad_norm": 0.5544227957725525,
      "learning_rate": 0.00026615384615384614,
      "loss": 0.2026,
      "step": 54
    },
    {
      "epoch": 5.365853658536586,
      "grad_norm": 0.636958658695221,
      "learning_rate": 0.00026538461538461536,
      "loss": 0.1911,
      "step": 55
    },
    {
      "epoch": 5.463414634146342,
      "grad_norm": 0.5816514492034912,
      "learning_rate": 0.00026461538461538457,
      "loss": 0.1971,
      "step": 56
    },
    {
      "epoch": 5.560975609756097,
      "grad_norm": 0.6842355132102966,
      "learning_rate": 0.00026384615384615384,
      "loss": 0.2149,
      "step": 57
    },
    {
      "epoch": 5.658536585365853,
      "grad_norm": 0.6406846046447754,
      "learning_rate": 0.00026307692307692306,
      "loss": 0.212,
      "step": 58
    },
    {
      "epoch": 5.7560975609756095,
      "grad_norm": 0.5310630202293396,
      "learning_rate": 0.0002623076923076923,
      "loss": 0.1961,
      "step": 59
    },
    {
      "epoch": 5.853658536585366,
      "grad_norm": 0.5690909624099731,
      "learning_rate": 0.00026153846153846154,
      "loss": 0.208,
      "step": 60
    },
    {
      "epoch": 5.951219512195122,
      "grad_norm": 0.48387718200683594,
      "learning_rate": 0.00026076923076923076,
      "loss": 0.1776,
      "step": 61
    },
    {
      "epoch": 6.048780487804878,
      "grad_norm": 1.254341959953308,
      "learning_rate": 0.00026,
      "loss": 0.3382,
      "step": 62
    },
    {
      "epoch": 6.146341463414634,
      "grad_norm": 0.5778391361236572,
      "learning_rate": 0.0002592307692307692,
      "loss": 0.1885,
      "step": 63
    },
    {
      "epoch": 6.2439024390243905,
      "grad_norm": 0.5925433039665222,
      "learning_rate": 0.00025846153846153846,
      "loss": 0.167,
      "step": 64
    },
    {
      "epoch": 6.341463414634147,
      "grad_norm": 0.6107469201087952,
      "learning_rate": 0.0002576923076923077,
      "loss": 0.1704,
      "step": 65
    },
    {
      "epoch": 6.439024390243903,
      "grad_norm": 0.6885983347892761,
      "learning_rate": 0.0002569230769230769,
      "loss": 0.1699,
      "step": 66
    },
    {
      "epoch": 6.536585365853659,
      "grad_norm": 0.6648727655410767,
      "learning_rate": 0.0002561538461538461,
      "loss": 0.1643,
      "step": 67
    },
    {
      "epoch": 6.634146341463414,
      "grad_norm": 0.6736704707145691,
      "learning_rate": 0.00025538461538461533,
      "loss": 0.1723,
      "step": 68
    },
    {
      "epoch": 6.7317073170731705,
      "grad_norm": 0.5547583699226379,
      "learning_rate": 0.0002546153846153846,
      "loss": 0.1604,
      "step": 69
    },
    {
      "epoch": 6.829268292682927,
      "grad_norm": 0.5472483038902283,
      "learning_rate": 0.0002538461538461538,
      "loss": 0.1662,
      "step": 70
    },
    {
      "epoch": 6.926829268292683,
      "grad_norm": 0.6016271710395813,
      "learning_rate": 0.00025307692307692303,
      "loss": 0.1594,
      "step": 71
    },
    {
      "epoch": 7.024390243902439,
      "grad_norm": 1.387947678565979,
      "learning_rate": 0.0002523076923076923,
      "loss": 0.3234,
      "step": 72
    },
    {
      "epoch": 7.121951219512195,
      "grad_norm": 0.5165477991104126,
      "learning_rate": 0.0002515384615384615,
      "loss": 0.1407,
      "step": 73
    },
    {
      "epoch": 7.219512195121951,
      "grad_norm": 0.7015456557273865,
      "learning_rate": 0.00025076923076923073,
      "loss": 0.1573,
      "step": 74
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 0.7020180225372314,
      "learning_rate": 0.00025,
      "loss": 0.1424,
      "step": 75
    },
    {
      "epoch": 7.414634146341464,
      "grad_norm": 0.760909378528595,
      "learning_rate": 0.0002492307692307692,
      "loss": 0.1413,
      "step": 76
    },
    {
      "epoch": 7.512195121951219,
      "grad_norm": 0.6655891537666321,
      "learning_rate": 0.00024846153846153844,
      "loss": 0.1361,
      "step": 77
    },
    {
      "epoch": 7.609756097560975,
      "grad_norm": 0.676379382610321,
      "learning_rate": 0.00024769230769230765,
      "loss": 0.1418,
      "step": 78
    },
    {
      "epoch": 7.7073170731707314,
      "grad_norm": 0.6460595726966858,
      "learning_rate": 0.00024692307692307687,
      "loss": 0.1399,
      "step": 79
    },
    {
      "epoch": 7.804878048780488,
      "grad_norm": 0.608098030090332,
      "learning_rate": 0.00024615384615384614,
      "loss": 0.1605,
      "step": 80
    },
    {
      "epoch": 7.902439024390244,
      "grad_norm": 0.5878331065177917,
      "learning_rate": 0.00024538461538461536,
      "loss": 0.1467,
      "step": 81
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.212976336479187,
      "learning_rate": 0.0002446153846153846,
      "loss": 0.2295,
      "step": 82
    },
    {
      "epoch": 8.097560975609756,
      "grad_norm": 0.5707969665527344,
      "learning_rate": 0.00024384615384615382,
      "loss": 0.1281,
      "step": 83
    },
    {
      "epoch": 8.195121951219512,
      "grad_norm": 0.6834900975227356,
      "learning_rate": 0.00024307692307692306,
      "loss": 0.1182,
      "step": 84
    },
    {
      "epoch": 8.292682926829269,
      "grad_norm": 0.7989528775215149,
      "learning_rate": 0.0002423076923076923,
      "loss": 0.1318,
      "step": 85
    },
    {
      "epoch": 8.390243902439025,
      "grad_norm": 0.7357165217399597,
      "learning_rate": 0.00024153846153846152,
      "loss": 0.1286,
      "step": 86
    },
    {
      "epoch": 8.487804878048781,
      "grad_norm": 0.6129620671272278,
      "learning_rate": 0.00024076923076923076,
      "loss": 0.1119,
      "step": 87
    },
    {
      "epoch": 8.585365853658537,
      "grad_norm": 0.6492916345596313,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.12,
      "step": 88
    },
    {
      "epoch": 8.682926829268293,
      "grad_norm": 0.7219799160957336,
      "learning_rate": 0.00023923076923076922,
      "loss": 0.1241,
      "step": 89
    },
    {
      "epoch": 8.78048780487805,
      "grad_norm": 0.6480810642242432,
      "learning_rate": 0.00023846153846153844,
      "loss": 0.1227,
      "step": 90
    },
    {
      "epoch": 8.878048780487806,
      "grad_norm": 0.69047611951828,
      "learning_rate": 0.00023769230769230765,
      "loss": 0.1248,
      "step": 91
    },
    {
      "epoch": 8.975609756097562,
      "grad_norm": 0.6785233020782471,
      "learning_rate": 0.0002369230769230769,
      "loss": 0.1197,
      "step": 92
    },
    {
      "epoch": 9.073170731707316,
      "grad_norm": 2.810790777206421,
      "learning_rate": 0.00023615384615384611,
      "loss": 0.2003,
      "step": 93
    },
    {
      "epoch": 9.170731707317072,
      "grad_norm": 0.5982910990715027,
      "learning_rate": 0.00023538461538461536,
      "loss": 0.0968,
      "step": 94
    },
    {
      "epoch": 9.268292682926829,
      "grad_norm": 0.6254074573516846,
      "learning_rate": 0.0002346153846153846,
      "loss": 0.0993,
      "step": 95
    },
    {
      "epoch": 9.365853658536585,
      "grad_norm": 0.7720667123794556,
      "learning_rate": 0.00023384615384615382,
      "loss": 0.116,
      "step": 96
    },
    {
      "epoch": 9.463414634146341,
      "grad_norm": 0.6318007111549377,
      "learning_rate": 0.00023307692307692306,
      "loss": 0.0943,
      "step": 97
    },
    {
      "epoch": 9.560975609756097,
      "grad_norm": 0.6732233166694641,
      "learning_rate": 0.0002323076923076923,
      "loss": 0.0963,
      "step": 98
    },
    {
      "epoch": 9.658536585365853,
      "grad_norm": 0.757533848285675,
      "learning_rate": 0.00023153846153846152,
      "loss": 0.1024,
      "step": 99
    },
    {
      "epoch": 9.75609756097561,
      "grad_norm": 0.6793942451477051,
      "learning_rate": 0.00023076923076923076,
      "loss": 0.1046,
      "step": 100
    },
    {
      "epoch": 9.853658536585366,
      "grad_norm": 0.5930845737457275,
      "learning_rate": 0.00023,
      "loss": 0.1015,
      "step": 101
    },
    {
      "epoch": 9.951219512195122,
      "grad_norm": 0.6696867942810059,
      "learning_rate": 0.0002292307692307692,
      "loss": 0.1068,
      "step": 102
    },
    {
      "epoch": 10.048780487804878,
      "grad_norm": 1.590831995010376,
      "learning_rate": 0.00022846153846153844,
      "loss": 0.1672,
      "step": 103
    },
    {
      "epoch": 10.146341463414634,
      "grad_norm": 0.6497500538825989,
      "learning_rate": 0.00022769230769230766,
      "loss": 0.0918,
      "step": 104
    },
    {
      "epoch": 10.24390243902439,
      "grad_norm": 0.6746799349784851,
      "learning_rate": 0.0002269230769230769,
      "loss": 0.078,
      "step": 105
    },
    {
      "epoch": 10.341463414634147,
      "grad_norm": 0.6035870313644409,
      "learning_rate": 0.00022615384615384614,
      "loss": 0.0862,
      "step": 106
    },
    {
      "epoch": 10.439024390243903,
      "grad_norm": 0.6227266788482666,
      "learning_rate": 0.00022538461538461536,
      "loss": 0.0802,
      "step": 107
    },
    {
      "epoch": 10.536585365853659,
      "grad_norm": 0.7988134026527405,
      "learning_rate": 0.0002246153846153846,
      "loss": 0.0871,
      "step": 108
    },
    {
      "epoch": 10.634146341463415,
      "grad_norm": 0.678182065486908,
      "learning_rate": 0.00022384615384615382,
      "loss": 0.0882,
      "step": 109
    },
    {
      "epoch": 10.731707317073171,
      "grad_norm": 0.5979560613632202,
      "learning_rate": 0.00022307692307692306,
      "loss": 0.087,
      "step": 110
    },
    {
      "epoch": 10.829268292682928,
      "grad_norm": 0.8288485407829285,
      "learning_rate": 0.0002223076923076923,
      "loss": 0.0993,
      "step": 111
    },
    {
      "epoch": 10.926829268292684,
      "grad_norm": 0.7112923860549927,
      "learning_rate": 0.00022153846153846152,
      "loss": 0.0943,
      "step": 112
    },
    {
      "epoch": 11.024390243902438,
      "grad_norm": 0.9925838112831116,
      "learning_rate": 0.00022076923076923076,
      "loss": 0.1323,
      "step": 113
    },
    {
      "epoch": 11.121951219512194,
      "grad_norm": 0.5681711435317993,
      "learning_rate": 0.00021999999999999995,
      "loss": 0.0748,
      "step": 114
    },
    {
      "epoch": 11.21951219512195,
      "grad_norm": 0.6688063144683838,
      "learning_rate": 0.0002192307692307692,
      "loss": 0.0718,
      "step": 115
    },
    {
      "epoch": 11.317073170731707,
      "grad_norm": 0.7892563939094543,
      "learning_rate": 0.00021846153846153844,
      "loss": 0.0846,
      "step": 116
    },
    {
      "epoch": 11.414634146341463,
      "grad_norm": 0.6059169173240662,
      "learning_rate": 0.00021769230769230766,
      "loss": 0.0755,
      "step": 117
    },
    {
      "epoch": 11.512195121951219,
      "grad_norm": 0.6865572929382324,
      "learning_rate": 0.0002169230769230769,
      "loss": 0.077,
      "step": 118
    },
    {
      "epoch": 11.609756097560975,
      "grad_norm": 0.6729240417480469,
      "learning_rate": 0.00021615384615384614,
      "loss": 0.082,
      "step": 119
    },
    {
      "epoch": 11.707317073170731,
      "grad_norm": 0.7262803316116333,
      "learning_rate": 0.00021538461538461536,
      "loss": 0.0803,
      "step": 120
    },
    {
      "epoch": 11.804878048780488,
      "grad_norm": 0.7287474274635315,
      "learning_rate": 0.0002146153846153846,
      "loss": 0.0764,
      "step": 121
    },
    {
      "epoch": 11.902439024390244,
      "grad_norm": 0.6609286069869995,
      "learning_rate": 0.00021384615384615385,
      "loss": 0.0797,
      "step": 122
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.5008183717727661,
      "learning_rate": 0.00021307692307692306,
      "loss": 0.1289,
      "step": 123
    },
    {
      "epoch": 12.097560975609756,
      "grad_norm": 0.6631208658218384,
      "learning_rate": 0.0002123076923076923,
      "loss": 0.0682,
      "step": 124
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 0.6178029775619507,
      "learning_rate": 0.00021153846153846152,
      "loss": 0.0695,
      "step": 125
    },
    {
      "epoch": 12.292682926829269,
      "grad_norm": 0.7862261533737183,
      "learning_rate": 0.00021076923076923074,
      "loss": 0.065,
      "step": 126
    },
    {
      "epoch": 12.390243902439025,
      "grad_norm": 0.583063006401062,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.0621,
      "step": 127
    },
    {
      "epoch": 12.487804878048781,
      "grad_norm": 0.6640996932983398,
      "learning_rate": 0.0002092307692307692,
      "loss": 0.0693,
      "step": 128
    },
    {
      "epoch": 12.585365853658537,
      "grad_norm": 0.680751383304596,
      "learning_rate": 0.00020846153846153844,
      "loss": 0.0677,
      "step": 129
    },
    {
      "epoch": 12.682926829268293,
      "grad_norm": 0.6471074223518372,
      "learning_rate": 0.00020769230769230766,
      "loss": 0.0704,
      "step": 130
    },
    {
      "epoch": 12.78048780487805,
      "grad_norm": 0.6723058819770813,
      "learning_rate": 0.0002069230769230769,
      "loss": 0.0713,
      "step": 131
    },
    {
      "epoch": 12.878048780487806,
      "grad_norm": 0.7519401907920837,
      "learning_rate": 0.00020615384615384614,
      "loss": 0.0652,
      "step": 132
    },
    {
      "epoch": 12.975609756097562,
      "grad_norm": 0.6359977126121521,
      "learning_rate": 0.00020538461538461536,
      "loss": 0.0658,
      "step": 133
    },
    {
      "epoch": 13.073170731707316,
      "grad_norm": 1.303280234336853,
      "learning_rate": 0.0002046153846153846,
      "loss": 0.1207,
      "step": 134
    },
    {
      "epoch": 13.170731707317072,
      "grad_norm": 0.5166420936584473,
      "learning_rate": 0.00020384615384615385,
      "loss": 0.0522,
      "step": 135
    },
    {
      "epoch": 13.268292682926829,
      "grad_norm": 0.6531989574432373,
      "learning_rate": 0.00020307692307692306,
      "loss": 0.0641,
      "step": 136
    },
    {
      "epoch": 13.365853658536585,
      "grad_norm": 0.7574787139892578,
      "learning_rate": 0.0002023076923076923,
      "loss": 0.0617,
      "step": 137
    },
    {
      "epoch": 13.463414634146341,
      "grad_norm": 0.5650545954704285,
      "learning_rate": 0.0002015384615384615,
      "loss": 0.0569,
      "step": 138
    },
    {
      "epoch": 13.560975609756097,
      "grad_norm": 0.6324869990348816,
      "learning_rate": 0.00020076923076923074,
      "loss": 0.0529,
      "step": 139
    },
    {
      "epoch": 13.658536585365853,
      "grad_norm": 0.638867199420929,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.0594,
      "step": 140
    },
    {
      "epoch": 13.75609756097561,
      "grad_norm": 0.5554178953170776,
      "learning_rate": 0.0001992307692307692,
      "loss": 0.0556,
      "step": 141
    },
    {
      "epoch": 13.853658536585366,
      "grad_norm": 0.5964410901069641,
      "learning_rate": 0.00019846153846153844,
      "loss": 0.0604,
      "step": 142
    },
    {
      "epoch": 13.951219512195122,
      "grad_norm": 0.568067729473114,
      "learning_rate": 0.00019769230769230769,
      "loss": 0.0582,
      "step": 143
    },
    {
      "epoch": 14.048780487804878,
      "grad_norm": 1.2722012996673584,
      "learning_rate": 0.0001969230769230769,
      "loss": 0.0983,
      "step": 144
    },
    {
      "epoch": 14.146341463414634,
      "grad_norm": 0.4651719927787781,
      "learning_rate": 0.00019615384615384615,
      "loss": 0.0494,
      "step": 145
    },
    {
      "epoch": 14.24390243902439,
      "grad_norm": 0.5758270025253296,
      "learning_rate": 0.00019538461538461536,
      "loss": 0.0513,
      "step": 146
    },
    {
      "epoch": 14.341463414634147,
      "grad_norm": 0.6327675580978394,
      "learning_rate": 0.0001946153846153846,
      "loss": 0.05,
      "step": 147
    },
    {
      "epoch": 14.439024390243903,
      "grad_norm": 0.59115070104599,
      "learning_rate": 0.00019384615384615385,
      "loss": 0.0514,
      "step": 148
    },
    {
      "epoch": 14.536585365853659,
      "grad_norm": 0.626075804233551,
      "learning_rate": 0.00019307692307692306,
      "loss": 0.0509,
      "step": 149
    },
    {
      "epoch": 14.634146341463415,
      "grad_norm": 0.5754878520965576,
      "learning_rate": 0.0001923076923076923,
      "loss": 0.0488,
      "step": 150
    },
    {
      "epoch": 14.731707317073171,
      "grad_norm": 0.57208651304245,
      "learning_rate": 0.0001915384615384615,
      "loss": 0.0527,
      "step": 151
    },
    {
      "epoch": 14.829268292682928,
      "grad_norm": 0.7079024314880371,
      "learning_rate": 0.00019076923076923074,
      "loss": 0.0494,
      "step": 152
    },
    {
      "epoch": 14.926829268292684,
      "grad_norm": 0.6792846322059631,
      "learning_rate": 0.00018999999999999998,
      "loss": 0.0534,
      "step": 153
    },
    {
      "epoch": 15.024390243902438,
      "grad_norm": 1.1171278953552246,
      "learning_rate": 0.0001892307692307692,
      "loss": 0.0943,
      "step": 154
    },
    {
      "epoch": 15.121951219512194,
      "grad_norm": 0.45328569412231445,
      "learning_rate": 0.00018846153846153844,
      "loss": 0.0442,
      "step": 155
    },
    {
      "epoch": 15.21951219512195,
      "grad_norm": 0.5362766981124878,
      "learning_rate": 0.00018769230769230769,
      "loss": 0.0431,
      "step": 156
    },
    {
      "epoch": 15.317073170731707,
      "grad_norm": 0.8749783635139465,
      "learning_rate": 0.0001869230769230769,
      "loss": 0.0481,
      "step": 157
    },
    {
      "epoch": 15.414634146341463,
      "grad_norm": 0.6473031640052795,
      "learning_rate": 0.00018615384615384615,
      "loss": 0.0475,
      "step": 158
    },
    {
      "epoch": 15.512195121951219,
      "grad_norm": 0.5886269211769104,
      "learning_rate": 0.0001853846153846154,
      "loss": 0.0486,
      "step": 159
    },
    {
      "epoch": 15.609756097560975,
      "grad_norm": 0.5337300300598145,
      "learning_rate": 0.0001846153846153846,
      "loss": 0.0482,
      "step": 160
    },
    {
      "epoch": 15.707317073170731,
      "grad_norm": 0.610851526260376,
      "learning_rate": 0.00018384615384615385,
      "loss": 0.0493,
      "step": 161
    },
    {
      "epoch": 15.804878048780488,
      "grad_norm": 0.540162980556488,
      "learning_rate": 0.00018307692307692307,
      "loss": 0.0448,
      "step": 162
    },
    {
      "epoch": 15.902439024390244,
      "grad_norm": 0.6199730634689331,
      "learning_rate": 0.00018230769230769228,
      "loss": 0.0489,
      "step": 163
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.9878643155097961,
      "learning_rate": 0.0001815384615384615,
      "loss": 0.0735,
      "step": 164
    },
    {
      "epoch": 16.097560975609756,
      "grad_norm": 0.6197853088378906,
      "learning_rate": 0.00018076923076923074,
      "loss": 0.039,
      "step": 165
    },
    {
      "epoch": 16.195121951219512,
      "grad_norm": 0.6077073812484741,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.0424,
      "step": 166
    },
    {
      "epoch": 16.29268292682927,
      "grad_norm": 0.5783897638320923,
      "learning_rate": 0.0001792307692307692,
      "loss": 0.0422,
      "step": 167
    },
    {
      "epoch": 16.390243902439025,
      "grad_norm": 0.6214062571525574,
      "learning_rate": 0.00017846153846153844,
      "loss": 0.0422,
      "step": 168
    },
    {
      "epoch": 16.48780487804878,
      "grad_norm": 0.6036978960037231,
      "learning_rate": 0.0001776923076923077,
      "loss": 0.0444,
      "step": 169
    },
    {
      "epoch": 16.585365853658537,
      "grad_norm": 0.4867756962776184,
      "learning_rate": 0.0001769230769230769,
      "loss": 0.0382,
      "step": 170
    },
    {
      "epoch": 16.682926829268293,
      "grad_norm": 0.5280579924583435,
      "learning_rate": 0.00017615384615384615,
      "loss": 0.0381,
      "step": 171
    },
    {
      "epoch": 16.78048780487805,
      "grad_norm": 0.5156222581863403,
      "learning_rate": 0.0001753846153846154,
      "loss": 0.0422,
      "step": 172
    },
    {
      "epoch": 16.878048780487806,
      "grad_norm": 0.48407506942749023,
      "learning_rate": 0.0001746153846153846,
      "loss": 0.0397,
      "step": 173
    },
    {
      "epoch": 16.975609756097562,
      "grad_norm": 0.5248292684555054,
      "learning_rate": 0.00017384615384615385,
      "loss": 0.038,
      "step": 174
    },
    {
      "epoch": 17.073170731707318,
      "grad_norm": 1.1186274290084839,
      "learning_rate": 0.00017307692307692304,
      "loss": 0.0673,
      "step": 175
    },
    {
      "epoch": 17.170731707317074,
      "grad_norm": 0.4169793725013733,
      "learning_rate": 0.00017230769230769228,
      "loss": 0.0365,
      "step": 176
    },
    {
      "epoch": 17.26829268292683,
      "grad_norm": 0.48622992634773254,
      "learning_rate": 0.00017153846153846153,
      "loss": 0.0308,
      "step": 177
    },
    {
      "epoch": 17.365853658536587,
      "grad_norm": 0.5776484608650208,
      "learning_rate": 0.00017076923076923074,
      "loss": 0.0393,
      "step": 178
    },
    {
      "epoch": 17.463414634146343,
      "grad_norm": 0.5090771913528442,
      "learning_rate": 0.00016999999999999999,
      "loss": 0.0353,
      "step": 179
    },
    {
      "epoch": 17.5609756097561,
      "grad_norm": 0.6273130178451538,
      "learning_rate": 0.0001692307692307692,
      "loss": 0.0356,
      "step": 180
    },
    {
      "epoch": 17.658536585365855,
      "grad_norm": 0.6060420870780945,
      "learning_rate": 0.00016846153846153844,
      "loss": 0.0396,
      "step": 181
    },
    {
      "epoch": 17.75609756097561,
      "grad_norm": 0.5413146018981934,
      "learning_rate": 0.0001676923076923077,
      "loss": 0.0361,
      "step": 182
    },
    {
      "epoch": 17.853658536585368,
      "grad_norm": 0.6052027344703674,
      "learning_rate": 0.0001669230769230769,
      "loss": 0.0365,
      "step": 183
    },
    {
      "epoch": 17.951219512195124,
      "grad_norm": 0.6527165770530701,
      "learning_rate": 0.00016615384615384615,
      "loss": 0.0336,
      "step": 184
    },
    {
      "epoch": 18.048780487804876,
      "grad_norm": 1.2061017751693726,
      "learning_rate": 0.0001653846153846154,
      "loss": 0.0592,
      "step": 185
    },
    {
      "epoch": 18.146341463414632,
      "grad_norm": 0.4278706908226013,
      "learning_rate": 0.0001646153846153846,
      "loss": 0.0294,
      "step": 186
    },
    {
      "epoch": 18.24390243902439,
      "grad_norm": 0.5466043949127197,
      "learning_rate": 0.00016384615384615382,
      "loss": 0.0313,
      "step": 187
    },
    {
      "epoch": 18.341463414634145,
      "grad_norm": 0.4049510061740875,
      "learning_rate": 0.00016307692307692304,
      "loss": 0.0284,
      "step": 188
    },
    {
      "epoch": 18.4390243902439,
      "grad_norm": 0.730546236038208,
      "learning_rate": 0.00016230769230769228,
      "loss": 0.0354,
      "step": 189
    },
    {
      "epoch": 18.536585365853657,
      "grad_norm": 0.6860878467559814,
      "learning_rate": 0.00016153846153846153,
      "loss": 0.0376,
      "step": 190
    },
    {
      "epoch": 18.634146341463413,
      "grad_norm": 0.5612767934799194,
      "learning_rate": 0.00016076923076923074,
      "loss": 0.0345,
      "step": 191
    },
    {
      "epoch": 18.73170731707317,
      "grad_norm": 0.5372324585914612,
      "learning_rate": 0.00015999999999999999,
      "loss": 0.0318,
      "step": 192
    },
    {
      "epoch": 18.829268292682926,
      "grad_norm": 0.4677859842777252,
      "learning_rate": 0.00015923076923076923,
      "loss": 0.0301,
      "step": 193
    },
    {
      "epoch": 18.926829268292682,
      "grad_norm": 0.572262704372406,
      "learning_rate": 0.00015846153846153845,
      "loss": 0.0335,
      "step": 194
    },
    {
      "epoch": 19.024390243902438,
      "grad_norm": 0.8274814486503601,
      "learning_rate": 0.0001576923076923077,
      "loss": 0.0516,
      "step": 195
    },
    {
      "epoch": 19.121951219512194,
      "grad_norm": 0.464664101600647,
      "learning_rate": 0.0001569230769230769,
      "loss": 0.028,
      "step": 196
    },
    {
      "epoch": 19.21951219512195,
      "grad_norm": 0.4618857800960541,
      "learning_rate": 0.00015615384615384615,
      "loss": 0.0281,
      "step": 197
    },
    {
      "epoch": 19.317073170731707,
      "grad_norm": 0.5322240591049194,
      "learning_rate": 0.0001553846153846154,
      "loss": 0.0276,
      "step": 198
    },
    {
      "epoch": 19.414634146341463,
      "grad_norm": 0.5509775876998901,
      "learning_rate": 0.00015461538461538458,
      "loss": 0.0256,
      "step": 199
    },
    {
      "epoch": 19.51219512195122,
      "grad_norm": 0.5277653336524963,
      "learning_rate": 0.00015384615384615382,
      "loss": 0.0264,
      "step": 200
    },
    {
      "epoch": 19.609756097560975,
      "grad_norm": 0.5954446196556091,
      "learning_rate": 0.00015307692307692304,
      "loss": 0.0286,
      "step": 201
    },
    {
      "epoch": 19.70731707317073,
      "grad_norm": 0.6688345074653625,
      "learning_rate": 0.00015230769230769228,
      "loss": 0.0302,
      "step": 202
    },
    {
      "epoch": 19.804878048780488,
      "grad_norm": 0.7561993598937988,
      "learning_rate": 0.00015153846153846153,
      "loss": 0.0307,
      "step": 203
    },
    {
      "epoch": 19.902439024390244,
      "grad_norm": 0.509018063545227,
      "learning_rate": 0.00015076923076923074,
      "loss": 0.0244,
      "step": 204
    },
    {
      "epoch": 20.0,
      "grad_norm": 1.17532479763031,
      "learning_rate": 0.00015,
      "loss": 0.0432,
      "step": 205
    },
    {
      "epoch": 20.097560975609756,
      "grad_norm": 0.5158922672271729,
      "learning_rate": 0.00014923076923076923,
      "loss": 0.0248,
      "step": 206
    },
    {
      "epoch": 20.195121951219512,
      "grad_norm": 0.5749315619468689,
      "learning_rate": 0.00014846153846153845,
      "loss": 0.0277,
      "step": 207
    },
    {
      "epoch": 20.29268292682927,
      "grad_norm": 0.4043825566768646,
      "learning_rate": 0.0001476923076923077,
      "loss": 0.0221,
      "step": 208
    },
    {
      "epoch": 20.390243902439025,
      "grad_norm": 0.6756507158279419,
      "learning_rate": 0.0001469230769230769,
      "loss": 0.0259,
      "step": 209
    },
    {
      "epoch": 20.48780487804878,
      "grad_norm": 0.5535097122192383,
      "learning_rate": 0.00014615384615384615,
      "loss": 0.0247,
      "step": 210
    },
    {
      "epoch": 20.585365853658537,
      "grad_norm": 0.5899824500083923,
      "learning_rate": 0.00014538461538461537,
      "loss": 0.0263,
      "step": 211
    },
    {
      "epoch": 20.682926829268293,
      "grad_norm": 0.44880980253219604,
      "learning_rate": 0.0001446153846153846,
      "loss": 0.021,
      "step": 212
    },
    {
      "epoch": 20.78048780487805,
      "grad_norm": 0.5202474594116211,
      "learning_rate": 0.00014384615384615383,
      "loss": 0.0241,
      "step": 213
    },
    {
      "epoch": 20.878048780487806,
      "grad_norm": 0.5734834671020508,
      "learning_rate": 0.00014307692307692307,
      "loss": 0.0264,
      "step": 214
    },
    {
      "epoch": 20.975609756097562,
      "grad_norm": 0.5087510943412781,
      "learning_rate": 0.00014230769230769228,
      "loss": 0.0237,
      "step": 215
    },
    {
      "epoch": 21.073170731707318,
      "grad_norm": 0.8367258906364441,
      "learning_rate": 0.00014153846153846153,
      "loss": 0.044,
      "step": 216
    },
    {
      "epoch": 21.170731707317074,
      "grad_norm": 0.308968722820282,
      "learning_rate": 0.00014076923076923074,
      "loss": 0.0211,
      "step": 217
    },
    {
      "epoch": 21.26829268292683,
      "grad_norm": 0.44527125358581543,
      "learning_rate": 0.00014,
      "loss": 0.019,
      "step": 218
    },
    {
      "epoch": 21.365853658536587,
      "grad_norm": 0.38222482800483704,
      "learning_rate": 0.00013923076923076923,
      "loss": 0.0186,
      "step": 219
    },
    {
      "epoch": 21.463414634146343,
      "grad_norm": 0.3904511034488678,
      "learning_rate": 0.00013846153846153845,
      "loss": 0.0203,
      "step": 220
    },
    {
      "epoch": 21.5609756097561,
      "grad_norm": 0.6874457001686096,
      "learning_rate": 0.00013769230769230766,
      "loss": 0.0209,
      "step": 221
    },
    {
      "epoch": 21.658536585365855,
      "grad_norm": 0.6580822467803955,
      "learning_rate": 0.0001369230769230769,
      "loss": 0.0211,
      "step": 222
    },
    {
      "epoch": 21.75609756097561,
      "grad_norm": 0.4777549207210541,
      "learning_rate": 0.00013615384615384615,
      "loss": 0.0227,
      "step": 223
    },
    {
      "epoch": 21.853658536585368,
      "grad_norm": 0.4926369786262512,
      "learning_rate": 0.00013538461538461537,
      "loss": 0.0197,
      "step": 224
    },
    {
      "epoch": 21.951219512195124,
      "grad_norm": 0.4861537516117096,
      "learning_rate": 0.0001346153846153846,
      "loss": 0.0225,
      "step": 225
    },
    {
      "epoch": 22.048780487804876,
      "grad_norm": 0.788888156414032,
      "learning_rate": 0.00013384615384615385,
      "loss": 0.029,
      "step": 226
    },
    {
      "epoch": 22.146341463414632,
      "grad_norm": 0.3035849630832672,
      "learning_rate": 0.00013307692307692307,
      "loss": 0.0172,
      "step": 227
    },
    {
      "epoch": 22.24390243902439,
      "grad_norm": 0.3156489431858063,
      "learning_rate": 0.00013230769230769229,
      "loss": 0.0169,
      "step": 228
    },
    {
      "epoch": 22.341463414634145,
      "grad_norm": 0.3942689597606659,
      "learning_rate": 0.00013153846153846153,
      "loss": 0.0161,
      "step": 229
    },
    {
      "epoch": 22.4390243902439,
      "grad_norm": 0.39626267552375793,
      "learning_rate": 0.00013076923076923077,
      "loss": 0.0164,
      "step": 230
    },
    {
      "epoch": 22.536585365853657,
      "grad_norm": 0.5092055201530457,
      "learning_rate": 0.00013,
      "loss": 0.0168,
      "step": 231
    },
    {
      "epoch": 22.634146341463413,
      "grad_norm": 0.4738965332508087,
      "learning_rate": 0.00012923076923076923,
      "loss": 0.0166,
      "step": 232
    },
    {
      "epoch": 22.73170731707317,
      "grad_norm": 0.41669967770576477,
      "learning_rate": 0.00012846153846153845,
      "loss": 0.0166,
      "step": 233
    },
    {
      "epoch": 22.829268292682926,
      "grad_norm": 0.4718036353588104,
      "learning_rate": 0.00012769230769230766,
      "loss": 0.0178,
      "step": 234
    },
    {
      "epoch": 22.926829268292682,
      "grad_norm": 0.5172944068908691,
      "learning_rate": 0.0001269230769230769,
      "loss": 0.0207,
      "step": 235
    },
    {
      "epoch": 23.024390243902438,
      "grad_norm": 1.3501332998275757,
      "learning_rate": 0.00012615384615384615,
      "loss": 0.0318,
      "step": 236
    },
    {
      "epoch": 23.121951219512194,
      "grad_norm": 0.32667118310928345,
      "learning_rate": 0.00012538461538461537,
      "loss": 0.0151,
      "step": 237
    },
    {
      "epoch": 23.21951219512195,
      "grad_norm": 0.35107067227363586,
      "learning_rate": 0.0001246153846153846,
      "loss": 0.0138,
      "step": 238
    },
    {
      "epoch": 23.317073170731707,
      "grad_norm": 0.5523710250854492,
      "learning_rate": 0.00012384615384615383,
      "loss": 0.0159,
      "step": 239
    },
    {
      "epoch": 23.414634146341463,
      "grad_norm": 0.5085922479629517,
      "learning_rate": 0.00012307692307692307,
      "loss": 0.0158,
      "step": 240
    },
    {
      "epoch": 23.51219512195122,
      "grad_norm": 0.31964075565338135,
      "learning_rate": 0.0001223076923076923,
      "loss": 0.0122,
      "step": 241
    },
    {
      "epoch": 23.609756097560975,
      "grad_norm": 0.4439011812210083,
      "learning_rate": 0.00012153846153846153,
      "loss": 0.017,
      "step": 242
    },
    {
      "epoch": 23.70731707317073,
      "grad_norm": 0.5141040682792664,
      "learning_rate": 0.00012076923076923076,
      "loss": 0.0154,
      "step": 243
    },
    {
      "epoch": 23.804878048780488,
      "grad_norm": 0.6440830826759338,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.0194,
      "step": 244
    },
    {
      "epoch": 23.902439024390244,
      "grad_norm": 0.4653363525867462,
      "learning_rate": 0.00011923076923076922,
      "loss": 0.0147,
      "step": 245
    },
    {
      "epoch": 24.0,
      "grad_norm": 1.027530550956726,
      "learning_rate": 0.00011846153846153845,
      "loss": 0.035,
      "step": 246
    },
    {
      "epoch": 24.097560975609756,
      "grad_norm": 0.40005868673324585,
      "learning_rate": 0.00011769230769230768,
      "loss": 0.0165,
      "step": 247
    },
    {
      "epoch": 24.195121951219512,
      "grad_norm": 0.6079102158546448,
      "learning_rate": 0.00011692307692307691,
      "loss": 0.0166,
      "step": 248
    },
    {
      "epoch": 24.29268292682927,
      "grad_norm": 0.6041595935821533,
      "learning_rate": 0.00011615384615384615,
      "loss": 0.0141,
      "step": 249
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 0.6225816607475281,
      "learning_rate": 0.00011538461538461538,
      "loss": 0.0156,
      "step": 250
    },
    {
      "epoch": 24.48780487804878,
      "grad_norm": 0.5584787726402283,
      "learning_rate": 0.0001146153846153846,
      "loss": 0.0141,
      "step": 251
    },
    {
      "epoch": 24.585365853658537,
      "grad_norm": 0.38440826535224915,
      "learning_rate": 0.00011384615384615383,
      "loss": 0.0145,
      "step": 252
    },
    {
      "epoch": 24.682926829268293,
      "grad_norm": 0.38150453567504883,
      "learning_rate": 0.00011307692307692307,
      "loss": 0.0129,
      "step": 253
    },
    {
      "epoch": 24.78048780487805,
      "grad_norm": 0.423163503408432,
      "learning_rate": 0.0001123076923076923,
      "loss": 0.0166,
      "step": 254
    },
    {
      "epoch": 24.878048780487806,
      "grad_norm": 0.49825629591941833,
      "learning_rate": 0.00011153846153846153,
      "loss": 0.0167,
      "step": 255
    },
    {
      "epoch": 24.975609756097562,
      "grad_norm": 0.5698624849319458,
      "learning_rate": 0.00011076923076923076,
      "loss": 0.0172,
      "step": 256
    },
    {
      "epoch": 25.073170731707318,
      "grad_norm": 1.0096489191055298,
      "learning_rate": 0.00010999999999999998,
      "loss": 0.0256,
      "step": 257
    },
    {
      "epoch": 25.170731707317074,
      "grad_norm": 0.4400133788585663,
      "learning_rate": 0.00010923076923076922,
      "loss": 0.0136,
      "step": 258
    },
    {
      "epoch": 25.26829268292683,
      "grad_norm": 0.6490089893341064,
      "learning_rate": 0.00010846153846153845,
      "loss": 0.0127,
      "step": 259
    },
    {
      "epoch": 25.365853658536587,
      "grad_norm": 0.5348425507545471,
      "learning_rate": 0.00010769230769230768,
      "loss": 0.0119,
      "step": 260
    },
    {
      "epoch": 25.463414634146343,
      "grad_norm": 0.40810921788215637,
      "learning_rate": 0.00010692307692307692,
      "loss": 0.0123,
      "step": 261
    },
    {
      "epoch": 25.5609756097561,
      "grad_norm": 0.38899528980255127,
      "learning_rate": 0.00010615384615384615,
      "loss": 0.0135,
      "step": 262
    },
    {
      "epoch": 25.658536585365855,
      "grad_norm": 0.41859889030456543,
      "learning_rate": 0.00010538461538461537,
      "loss": 0.0146,
      "step": 263
    },
    {
      "epoch": 25.75609756097561,
      "grad_norm": 0.3267703652381897,
      "learning_rate": 0.0001046153846153846,
      "loss": 0.0124,
      "step": 264
    },
    {
      "epoch": 25.853658536585368,
      "grad_norm": 0.3386346995830536,
      "learning_rate": 0.00010384615384615383,
      "loss": 0.0118,
      "step": 265
    },
    {
      "epoch": 25.951219512195124,
      "grad_norm": 0.45315420627593994,
      "learning_rate": 0.00010307692307692307,
      "loss": 0.0148,
      "step": 266
    },
    {
      "epoch": 26.048780487804876,
      "grad_norm": 0.4619515538215637,
      "learning_rate": 0.0001023076923076923,
      "loss": 0.0221,
      "step": 267
    },
    {
      "epoch": 26.146341463414632,
      "grad_norm": 0.22714221477508545,
      "learning_rate": 0.00010153846153846153,
      "loss": 0.0115,
      "step": 268
    },
    {
      "epoch": 26.24390243902439,
      "grad_norm": 0.27629074454307556,
      "learning_rate": 0.00010076923076923075,
      "loss": 0.0116,
      "step": 269
    },
    {
      "epoch": 26.341463414634145,
      "grad_norm": 0.3802269697189331,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.011,
      "step": 270
    },
    {
      "epoch": 26.4390243902439,
      "grad_norm": 0.6011992692947388,
      "learning_rate": 9.923076923076922e-05,
      "loss": 0.0119,
      "step": 271
    },
    {
      "epoch": 26.536585365853657,
      "grad_norm": 0.3202364444732666,
      "learning_rate": 9.846153846153845e-05,
      "loss": 0.0112,
      "step": 272
    },
    {
      "epoch": 26.634146341463413,
      "grad_norm": 0.40619340538978577,
      "learning_rate": 9.769230769230768e-05,
      "loss": 0.0122,
      "step": 273
    },
    {
      "epoch": 26.73170731707317,
      "grad_norm": 0.3156737685203552,
      "learning_rate": 9.692307692307692e-05,
      "loss": 0.0095,
      "step": 274
    },
    {
      "epoch": 26.829268292682926,
      "grad_norm": 0.4167722463607788,
      "learning_rate": 9.615384615384615e-05,
      "loss": 0.0128,
      "step": 275
    },
    {
      "epoch": 26.926829268292682,
      "grad_norm": 0.29126545786857605,
      "learning_rate": 9.538461538461537e-05,
      "loss": 0.0124,
      "step": 276
    },
    {
      "epoch": 27.024390243902438,
      "grad_norm": 0.49630215764045715,
      "learning_rate": 9.46153846153846e-05,
      "loss": 0.0162,
      "step": 277
    },
    {
      "epoch": 27.121951219512194,
      "grad_norm": 0.2831817865371704,
      "learning_rate": 9.384615384615384e-05,
      "loss": 0.0107,
      "step": 278
    },
    {
      "epoch": 27.21951219512195,
      "grad_norm": 0.18234775960445404,
      "learning_rate": 9.307692307692307e-05,
      "loss": 0.0097,
      "step": 279
    },
    {
      "epoch": 27.317073170731707,
      "grad_norm": 0.11177537590265274,
      "learning_rate": 9.23076923076923e-05,
      "loss": 0.0102,
      "step": 280
    },
    {
      "epoch": 27.414634146341463,
      "grad_norm": 0.14566920697689056,
      "learning_rate": 9.153846153846153e-05,
      "loss": 0.0087,
      "step": 281
    },
    {
      "epoch": 27.51219512195122,
      "grad_norm": 0.16214503347873688,
      "learning_rate": 9.076923076923075e-05,
      "loss": 0.0107,
      "step": 282
    },
    {
      "epoch": 27.609756097560975,
      "grad_norm": 0.18130260705947876,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.011,
      "step": 283
    },
    {
      "epoch": 27.70731707317073,
      "grad_norm": 0.22337448596954346,
      "learning_rate": 8.923076923076922e-05,
      "loss": 0.0101,
      "step": 284
    },
    {
      "epoch": 27.804878048780488,
      "grad_norm": 0.3616008758544922,
      "learning_rate": 8.846153846153845e-05,
      "loss": 0.0106,
      "step": 285
    },
    {
      "epoch": 27.902439024390244,
      "grad_norm": 0.1894887387752533,
      "learning_rate": 8.76923076923077e-05,
      "loss": 0.0114,
      "step": 286
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.5613545179367065,
      "learning_rate": 8.692307692307692e-05,
      "loss": 0.0169,
      "step": 287
    },
    {
      "epoch": 28.097560975609756,
      "grad_norm": 0.1152874007821083,
      "learning_rate": 8.615384615384614e-05,
      "loss": 0.0107,
      "step": 288
    },
    {
      "epoch": 28.195121951219512,
      "grad_norm": 0.10387139767408371,
      "learning_rate": 8.538461538461537e-05,
      "loss": 0.0083,
      "step": 289
    },
    {
      "epoch": 28.29268292682927,
      "grad_norm": 0.17541420459747314,
      "learning_rate": 8.46153846153846e-05,
      "loss": 0.0104,
      "step": 290
    },
    {
      "epoch": 28.390243902439025,
      "grad_norm": 0.21690663695335388,
      "learning_rate": 8.384615384615384e-05,
      "loss": 0.009,
      "step": 291
    },
    {
      "epoch": 28.48780487804878,
      "grad_norm": 0.1562502235174179,
      "learning_rate": 8.307692307692307e-05,
      "loss": 0.0085,
      "step": 292
    },
    {
      "epoch": 28.585365853658537,
      "grad_norm": 0.17858992516994476,
      "learning_rate": 8.23076923076923e-05,
      "loss": 0.0071,
      "step": 293
    },
    {
      "epoch": 28.682926829268293,
      "grad_norm": 0.12568458914756775,
      "learning_rate": 8.153846153846152e-05,
      "loss": 0.0118,
      "step": 294
    },
    {
      "epoch": 28.78048780487805,
      "grad_norm": 0.10154672712087631,
      "learning_rate": 8.076923076923076e-05,
      "loss": 0.0104,
      "step": 295
    },
    {
      "epoch": 28.878048780487806,
      "grad_norm": 0.12045256793498993,
      "learning_rate": 7.999999999999999e-05,
      "loss": 0.0102,
      "step": 296
    },
    {
      "epoch": 28.975609756097562,
      "grad_norm": 0.1890062391757965,
      "learning_rate": 7.923076923076922e-05,
      "loss": 0.0103,
      "step": 297
    },
    {
      "epoch": 29.073170731707318,
      "grad_norm": 0.3509915769100189,
      "learning_rate": 7.846153846153845e-05,
      "loss": 0.014,
      "step": 298
    },
    {
      "epoch": 29.170731707317074,
      "grad_norm": 0.09676269441843033,
      "learning_rate": 7.76923076923077e-05,
      "loss": 0.0097,
      "step": 299
    },
    {
      "epoch": 29.26829268292683,
      "grad_norm": 0.15193909406661987,
      "learning_rate": 7.692307692307691e-05,
      "loss": 0.008,
      "step": 300
    },
    {
      "epoch": 29.365853658536587,
      "grad_norm": 0.10250312834978104,
      "learning_rate": 7.615384615384614e-05,
      "loss": 0.0103,
      "step": 301
    },
    {
      "epoch": 29.463414634146343,
      "grad_norm": 0.11651790887117386,
      "learning_rate": 7.538461538461537e-05,
      "loss": 0.0082,
      "step": 302
    },
    {
      "epoch": 29.5609756097561,
      "grad_norm": 0.09761911630630493,
      "learning_rate": 7.461538461538462e-05,
      "loss": 0.0081,
      "step": 303
    },
    {
      "epoch": 29.658536585365855,
      "grad_norm": 0.12125904858112335,
      "learning_rate": 7.384615384615384e-05,
      "loss": 0.0081,
      "step": 304
    },
    {
      "epoch": 29.75609756097561,
      "grad_norm": 0.1834990233182907,
      "learning_rate": 7.307692307692307e-05,
      "loss": 0.0108,
      "step": 305
    },
    {
      "epoch": 29.853658536585368,
      "grad_norm": 0.14718423783779144,
      "learning_rate": 7.23076923076923e-05,
      "loss": 0.0097,
      "step": 306
    },
    {
      "epoch": 29.951219512195124,
      "grad_norm": 0.08872444927692413,
      "learning_rate": 7.153846153846153e-05,
      "loss": 0.008,
      "step": 307
    },
    {
      "epoch": 30.048780487804876,
      "grad_norm": 0.22495727241039276,
      "learning_rate": 7.076923076923076e-05,
      "loss": 0.0172,
      "step": 308
    },
    {
      "epoch": 30.146341463414632,
      "grad_norm": 0.10297979414463043,
      "learning_rate": 7e-05,
      "loss": 0.009,
      "step": 309
    },
    {
      "epoch": 30.24390243902439,
      "grad_norm": 0.07349923253059387,
      "learning_rate": 6.923076923076922e-05,
      "loss": 0.0091,
      "step": 310
    },
    {
      "epoch": 30.341463414634145,
      "grad_norm": 0.06471280008554459,
      "learning_rate": 6.846153846153845e-05,
      "loss": 0.0078,
      "step": 311
    },
    {
      "epoch": 30.4390243902439,
      "grad_norm": 0.1079481989145279,
      "learning_rate": 6.769230769230768e-05,
      "loss": 0.0087,
      "step": 312
    },
    {
      "epoch": 30.536585365853657,
      "grad_norm": 0.07193591445684433,
      "learning_rate": 6.692307692307693e-05,
      "loss": 0.0082,
      "step": 313
    },
    {
      "epoch": 30.634146341463413,
      "grad_norm": 0.10280711203813553,
      "learning_rate": 6.615384615384614e-05,
      "loss": 0.0079,
      "step": 314
    },
    {
      "epoch": 30.73170731707317,
      "grad_norm": 0.09613154828548431,
      "learning_rate": 6.538461538461539e-05,
      "loss": 0.0105,
      "step": 315
    },
    {
      "epoch": 30.829268292682926,
      "grad_norm": 0.09825538098812103,
      "learning_rate": 6.461538461538462e-05,
      "loss": 0.0066,
      "step": 316
    },
    {
      "epoch": 30.926829268292682,
      "grad_norm": 0.08507632464170456,
      "learning_rate": 6.384615384615383e-05,
      "loss": 0.0091,
      "step": 317
    },
    {
      "epoch": 31.024390243902438,
      "grad_norm": 0.26204434037208557,
      "learning_rate": 6.307692307692308e-05,
      "loss": 0.0188,
      "step": 318
    },
    {
      "epoch": 31.121951219512194,
      "grad_norm": 0.07975011318922043,
      "learning_rate": 6.23076923076923e-05,
      "loss": 0.0084,
      "step": 319
    },
    {
      "epoch": 31.21951219512195,
      "grad_norm": 0.07759762555360794,
      "learning_rate": 6.153846153846154e-05,
      "loss": 0.0102,
      "step": 320
    },
    {
      "epoch": 31.317073170731707,
      "grad_norm": 0.15985895693302155,
      "learning_rate": 6.0769230769230765e-05,
      "loss": 0.0085,
      "step": 321
    },
    {
      "epoch": 31.414634146341463,
      "grad_norm": 0.04363193362951279,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 0.0068,
      "step": 322
    },
    {
      "epoch": 31.51219512195122,
      "grad_norm": 0.08268717676401138,
      "learning_rate": 5.9230769230769225e-05,
      "loss": 0.0088,
      "step": 323
    },
    {
      "epoch": 31.609756097560975,
      "grad_norm": 0.06155639886856079,
      "learning_rate": 5.8461538461538454e-05,
      "loss": 0.0082,
      "step": 324
    },
    {
      "epoch": 31.70731707317073,
      "grad_norm": 0.06311964988708496,
      "learning_rate": 5.769230769230769e-05,
      "loss": 0.0094,
      "step": 325
    },
    {
      "epoch": 31.804878048780488,
      "grad_norm": 0.11191972345113754,
      "learning_rate": 5.6923076923076914e-05,
      "loss": 0.0087,
      "step": 326
    },
    {
      "epoch": 31.902439024390244,
      "grad_norm": 0.029549289494752884,
      "learning_rate": 5.615384615384615e-05,
      "loss": 0.0073,
      "step": 327
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.16594964265823364,
      "learning_rate": 5.538461538461538e-05,
      "loss": 0.0153,
      "step": 328
    },
    {
      "epoch": 32.09756097560975,
      "grad_norm": 0.06632982194423676,
      "learning_rate": 5.461538461538461e-05,
      "loss": 0.0079,
      "step": 329
    },
    {
      "epoch": 32.19512195121951,
      "grad_norm": 0.04317404702305794,
      "learning_rate": 5.384615384615384e-05,
      "loss": 0.0104,
      "step": 330
    },
    {
      "epoch": 32.292682926829265,
      "grad_norm": 0.044292423874139786,
      "learning_rate": 5.3076923076923076e-05,
      "loss": 0.0081,
      "step": 331
    },
    {
      "epoch": 32.390243902439025,
      "grad_norm": 0.056346312165260315,
      "learning_rate": 5.23076923076923e-05,
      "loss": 0.0086,
      "step": 332
    },
    {
      "epoch": 32.48780487804878,
      "grad_norm": 0.04658378288149834,
      "learning_rate": 5.1538461538461536e-05,
      "loss": 0.0085,
      "step": 333
    },
    {
      "epoch": 32.58536585365854,
      "grad_norm": 0.06577052175998688,
      "learning_rate": 5.0769230769230766e-05,
      "loss": 0.0077,
      "step": 334
    },
    {
      "epoch": 32.68292682926829,
      "grad_norm": 0.07037389278411865,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 0.0081,
      "step": 335
    },
    {
      "epoch": 32.78048780487805,
      "grad_norm": 0.11469626426696777,
      "learning_rate": 4.9230769230769225e-05,
      "loss": 0.0084,
      "step": 336
    },
    {
      "epoch": 32.8780487804878,
      "grad_norm": 0.05701368302106857,
      "learning_rate": 4.846153846153846e-05,
      "loss": 0.008,
      "step": 337
    },
    {
      "epoch": 32.97560975609756,
      "grad_norm": 0.11277736723423004,
      "learning_rate": 4.7692307692307685e-05,
      "loss": 0.009,
      "step": 338
    },
    {
      "epoch": 33.073170731707314,
      "grad_norm": 0.09308703988790512,
      "learning_rate": 4.692307692307692e-05,
      "loss": 0.0162,
      "step": 339
    },
    {
      "epoch": 33.170731707317074,
      "grad_norm": 0.04689300060272217,
      "learning_rate": 4.615384615384615e-05,
      "loss": 0.0067,
      "step": 340
    },
    {
      "epoch": 33.26829268292683,
      "grad_norm": 0.08225132524967194,
      "learning_rate": 4.5384615384615374e-05,
      "loss": 0.0107,
      "step": 341
    },
    {
      "epoch": 33.36585365853659,
      "grad_norm": 0.05762360990047455,
      "learning_rate": 4.461538461538461e-05,
      "loss": 0.0073,
      "step": 342
    },
    {
      "epoch": 33.46341463414634,
      "grad_norm": 0.04460536316037178,
      "learning_rate": 4.384615384615385e-05,
      "loss": 0.0074,
      "step": 343
    },
    {
      "epoch": 33.5609756097561,
      "grad_norm": 0.07546199858188629,
      "learning_rate": 4.307692307692307e-05,
      "loss": 0.0098,
      "step": 344
    },
    {
      "epoch": 33.65853658536585,
      "grad_norm": 0.08225828409194946,
      "learning_rate": 4.23076923076923e-05,
      "loss": 0.009,
      "step": 345
    },
    {
      "epoch": 33.75609756097561,
      "grad_norm": 0.09756005555391312,
      "learning_rate": 4.153846153846154e-05,
      "loss": 0.008,
      "step": 346
    },
    {
      "epoch": 33.853658536585364,
      "grad_norm": 0.059193674474954605,
      "learning_rate": 4.076923076923076e-05,
      "loss": 0.0078,
      "step": 347
    },
    {
      "epoch": 33.951219512195124,
      "grad_norm": 0.057836294174194336,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 0.0089,
      "step": 348
    },
    {
      "epoch": 34.048780487804876,
      "grad_norm": 0.07602037489414215,
      "learning_rate": 3.9230769230769226e-05,
      "loss": 0.0115,
      "step": 349
    },
    {
      "epoch": 34.146341463414636,
      "grad_norm": 0.06383092701435089,
      "learning_rate": 3.8461538461538456e-05,
      "loss": 0.0085,
      "step": 350
    },
    {
      "epoch": 34.24390243902439,
      "grad_norm": 0.06896553188562393,
      "learning_rate": 3.7692307692307686e-05,
      "loss": 0.0095,
      "step": 351
    },
    {
      "epoch": 34.34146341463415,
      "grad_norm": 0.07921052724123001,
      "learning_rate": 3.692307692307692e-05,
      "loss": 0.0097,
      "step": 352
    },
    {
      "epoch": 34.4390243902439,
      "grad_norm": 0.03821449726819992,
      "learning_rate": 3.615384615384615e-05,
      "loss": 0.0076,
      "step": 353
    },
    {
      "epoch": 34.53658536585366,
      "grad_norm": 0.06242699548602104,
      "learning_rate": 3.538461538461538e-05,
      "loss": 0.0093,
      "step": 354
    },
    {
      "epoch": 34.63414634146341,
      "grad_norm": 0.10142141580581665,
      "learning_rate": 3.461538461538461e-05,
      "loss": 0.008,
      "step": 355
    },
    {
      "epoch": 34.73170731707317,
      "grad_norm": 0.07516910135746002,
      "learning_rate": 3.384615384615384e-05,
      "loss": 0.0077,
      "step": 356
    },
    {
      "epoch": 34.829268292682926,
      "grad_norm": 0.06010580062866211,
      "learning_rate": 3.307692307692307e-05,
      "loss": 0.0086,
      "step": 357
    },
    {
      "epoch": 34.926829268292686,
      "grad_norm": 0.08749096095561981,
      "learning_rate": 3.230769230769231e-05,
      "loss": 0.0066,
      "step": 358
    },
    {
      "epoch": 35.02439024390244,
      "grad_norm": 0.13835380971431732,
      "learning_rate": 3.153846153846154e-05,
      "loss": 0.013,
      "step": 359
    },
    {
      "epoch": 35.1219512195122,
      "grad_norm": 0.04741403087973595,
      "learning_rate": 3.076923076923077e-05,
      "loss": 0.0092,
      "step": 360
    },
    {
      "epoch": 35.21951219512195,
      "grad_norm": 0.04279856011271477,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 0.0096,
      "step": 361
    },
    {
      "epoch": 35.31707317073171,
      "grad_norm": 0.04202897474169731,
      "learning_rate": 2.9230769230769227e-05,
      "loss": 0.0069,
      "step": 362
    },
    {
      "epoch": 35.41463414634146,
      "grad_norm": 0.08087175339460373,
      "learning_rate": 2.8461538461538457e-05,
      "loss": 0.0073,
      "step": 363
    },
    {
      "epoch": 35.51219512195122,
      "grad_norm": 0.07299298793077469,
      "learning_rate": 2.769230769230769e-05,
      "loss": 0.0083,
      "step": 364
    },
    {
      "epoch": 35.609756097560975,
      "grad_norm": 0.06566359847784042,
      "learning_rate": 2.692307692307692e-05,
      "loss": 0.0079,
      "step": 365
    },
    {
      "epoch": 35.707317073170735,
      "grad_norm": 0.06727254390716553,
      "learning_rate": 2.615384615384615e-05,
      "loss": 0.0081,
      "step": 366
    },
    {
      "epoch": 35.80487804878049,
      "grad_norm": 0.067051000893116,
      "learning_rate": 2.5384615384615383e-05,
      "loss": 0.0084,
      "step": 367
    },
    {
      "epoch": 35.90243902439025,
      "grad_norm": 0.09193821251392365,
      "learning_rate": 2.4615384615384613e-05,
      "loss": 0.0079,
      "step": 368
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.17674675583839417,
      "learning_rate": 2.3846153846153843e-05,
      "loss": 0.0142,
      "step": 369
    },
    {
      "epoch": 36.09756097560975,
      "grad_norm": 0.07128577679395676,
      "learning_rate": 2.3076923076923076e-05,
      "loss": 0.0075,
      "step": 370
    },
    {
      "epoch": 36.19512195121951,
      "grad_norm": 0.0823100209236145,
      "learning_rate": 2.2307692307692305e-05,
      "loss": 0.0093,
      "step": 371
    },
    {
      "epoch": 36.292682926829265,
      "grad_norm": 0.04082486405968666,
      "learning_rate": 2.1538461538461535e-05,
      "loss": 0.0087,
      "step": 372
    },
    {
      "epoch": 36.390243902439025,
      "grad_norm": 0.06084716320037842,
      "learning_rate": 2.076923076923077e-05,
      "loss": 0.0083,
      "step": 373
    },
    {
      "epoch": 36.48780487804878,
      "grad_norm": 0.06781242042779922,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 0.0089,
      "step": 374
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 0.0438215546309948,
      "learning_rate": 1.9230769230769228e-05,
      "loss": 0.0083,
      "step": 375
    },
    {
      "epoch": 36.68292682926829,
      "grad_norm": 0.05200992152094841,
      "learning_rate": 1.846153846153846e-05,
      "loss": 0.0074,
      "step": 376
    },
    {
      "epoch": 36.78048780487805,
      "grad_norm": 0.061480555683374405,
      "learning_rate": 1.769230769230769e-05,
      "loss": 0.008,
      "step": 377
    },
    {
      "epoch": 36.8780487804878,
      "grad_norm": 0.0802457258105278,
      "learning_rate": 1.692307692307692e-05,
      "loss": 0.0069,
      "step": 378
    },
    {
      "epoch": 36.97560975609756,
      "grad_norm": 0.07664385437965393,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 0.0086,
      "step": 379
    },
    {
      "epoch": 37.073170731707314,
      "grad_norm": 0.05720673128962517,
      "learning_rate": 1.5384615384615384e-05,
      "loss": 0.0122,
      "step": 380
    },
    {
      "epoch": 37.170731707317074,
      "grad_norm": 0.08058086037635803,
      "learning_rate": 1.4615384615384614e-05,
      "loss": 0.0068,
      "step": 381
    },
    {
      "epoch": 37.26829268292683,
      "grad_norm": 0.04761737585067749,
      "learning_rate": 1.3846153846153845e-05,
      "loss": 0.0087,
      "step": 382
    },
    {
      "epoch": 37.36585365853659,
      "grad_norm": 0.07073310017585754,
      "learning_rate": 1.3076923076923075e-05,
      "loss": 0.0089,
      "step": 383
    },
    {
      "epoch": 37.46341463414634,
      "grad_norm": 0.0732567086815834,
      "learning_rate": 1.2307692307692306e-05,
      "loss": 0.009,
      "step": 384
    },
    {
      "epoch": 37.5609756097561,
      "grad_norm": 0.0774049386382103,
      "learning_rate": 1.1538461538461538e-05,
      "loss": 0.0085,
      "step": 385
    },
    {
      "epoch": 37.65853658536585,
      "grad_norm": 0.0659705102443695,
      "learning_rate": 1.0769230769230768e-05,
      "loss": 0.0068,
      "step": 386
    },
    {
      "epoch": 37.75609756097561,
      "grad_norm": 0.06929603219032288,
      "learning_rate": 9.999999999999999e-06,
      "loss": 0.0079,
      "step": 387
    },
    {
      "epoch": 37.853658536585364,
      "grad_norm": 0.08159150928258896,
      "learning_rate": 9.23076923076923e-06,
      "loss": 0.0086,
      "step": 388
    },
    {
      "epoch": 37.951219512195124,
      "grad_norm": 0.065193772315979,
      "learning_rate": 8.46153846153846e-06,
      "loss": 0.0081,
      "step": 389
    },
    {
      "epoch": 38.048780487804876,
      "grad_norm": 0.059862565249204636,
      "learning_rate": 7.692307692307692e-06,
      "loss": 0.0131,
      "step": 390
    },
    {
      "epoch": 38.146341463414636,
      "grad_norm": 0.0563255250453949,
      "learning_rate": 6.9230769230769225e-06,
      "loss": 0.0074,
      "step": 391
    },
    {
      "epoch": 38.24390243902439,
      "grad_norm": 0.060202084481716156,
      "learning_rate": 6.153846153846153e-06,
      "loss": 0.0078,
      "step": 392
    },
    {
      "epoch": 38.34146341463415,
      "grad_norm": 0.047441162168979645,
      "learning_rate": 5.384615384615384e-06,
      "loss": 0.0069,
      "step": 393
    },
    {
      "epoch": 38.4390243902439,
      "grad_norm": 0.08272508531808853,
      "learning_rate": 4.615384615384615e-06,
      "loss": 0.0095,
      "step": 394
    },
    {
      "epoch": 38.53658536585366,
      "grad_norm": 0.06984218955039978,
      "learning_rate": 3.846153846153846e-06,
      "loss": 0.0085,
      "step": 395
    },
    {
      "epoch": 38.63414634146341,
      "grad_norm": 0.07881034910678864,
      "learning_rate": 3.0769230769230766e-06,
      "loss": 0.0102,
      "step": 396
    },
    {
      "epoch": 38.73170731707317,
      "grad_norm": 0.06265929341316223,
      "learning_rate": 2.3076923076923077e-06,
      "loss": 0.0059,
      "step": 397
    },
    {
      "epoch": 38.829268292682926,
      "grad_norm": 0.03236714005470276,
      "learning_rate": 1.5384615384615383e-06,
      "loss": 0.0103,
      "step": 398
    },
    {
      "epoch": 38.926829268292686,
      "grad_norm": 0.06134776026010513,
      "learning_rate": 7.692307692307691e-07,
      "loss": 0.0071,
      "step": 399
    },
    {
      "epoch": 39.02439024390244,
      "grad_norm": 0.15351279079914093,
      "learning_rate": 0.0,
      "loss": 0.0134,
      "step": 400
    }
  ],
  "logging_steps": 1,
  "max_steps": 400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.91998325686272e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
