{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 39.02439024390244,
  "eval_steps": 500,
  "global_step": 400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 0.671142578125,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.0616,
      "step": 1
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 0.7084060907363892,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.0513,
      "step": 2
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 0.8316338062286377,
      "learning_rate": 8.999999999999999e-05,
      "loss": 1.0634,
      "step": 3
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 0.5718464255332947,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.9809,
      "step": 4
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.5514833331108093,
      "learning_rate": 0.00015,
      "loss": 0.9432,
      "step": 5
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 0.5542969703674316,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.928,
      "step": 6
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 0.5645774006843567,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.8903,
      "step": 7
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 0.6518042087554932,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.8339,
      "step": 8
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.737522304058075,
      "learning_rate": 0.00027,
      "loss": 0.7998,
      "step": 9
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.9083003401756287,
      "learning_rate": 0.0003,
      "loss": 0.744,
      "step": 10
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 1.8116977214813232,
      "learning_rate": 0.0002992307692307692,
      "loss": 1.1925,
      "step": 11
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 0.9600601196289062,
      "learning_rate": 0.00029846153846153846,
      "loss": 0.6378,
      "step": 12
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 0.9136046171188354,
      "learning_rate": 0.0002976923076923077,
      "loss": 0.5782,
      "step": 13
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 0.8098431825637817,
      "learning_rate": 0.0002969230769230769,
      "loss": 0.5306,
      "step": 14
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 0.7027665376663208,
      "learning_rate": 0.00029615384615384616,
      "loss": 0.497,
      "step": 15
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 0.6787128448486328,
      "learning_rate": 0.0002953846153846154,
      "loss": 0.488,
      "step": 16
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 0.6435243487358093,
      "learning_rate": 0.0002946153846153846,
      "loss": 0.4736,
      "step": 17
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 0.6085623502731323,
      "learning_rate": 0.0002938461538461538,
      "loss": 0.4439,
      "step": 18
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 0.5788556933403015,
      "learning_rate": 0.00029307692307692303,
      "loss": 0.419,
      "step": 19
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 0.5908377766609192,
      "learning_rate": 0.0002923076923076923,
      "loss": 0.4351,
      "step": 20
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 1.1403177976608276,
      "learning_rate": 0.0002915384615384615,
      "loss": 0.6958,
      "step": 21
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 0.5751961469650269,
      "learning_rate": 0.00029076923076923073,
      "loss": 0.3917,
      "step": 22
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 0.527100145816803,
      "learning_rate": 0.00029,
      "loss": 0.3767,
      "step": 23
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 0.6133452653884888,
      "learning_rate": 0.0002892307692307692,
      "loss": 0.335,
      "step": 24
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 0.5550743341445923,
      "learning_rate": 0.00028846153846153843,
      "loss": 0.3737,
      "step": 25
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 0.5284997224807739,
      "learning_rate": 0.00028769230769230765,
      "loss": 0.3676,
      "step": 26
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 0.548393964767456,
      "learning_rate": 0.0002869230769230769,
      "loss": 0.3313,
      "step": 27
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 0.6245628595352173,
      "learning_rate": 0.00028615384615384614,
      "loss": 0.353,
      "step": 28
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 0.5275072455406189,
      "learning_rate": 0.00028538461538461535,
      "loss": 0.3301,
      "step": 29
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 0.5112590193748474,
      "learning_rate": 0.00028461538461538457,
      "loss": 0.3228,
      "step": 30
    },
    {
      "epoch": 3.024390243902439,
      "grad_norm": 1.0392409563064575,
      "learning_rate": 0.0002838461538461538,
      "loss": 0.5198,
      "step": 31
    },
    {
      "epoch": 3.1219512195121952,
      "grad_norm": 0.5537664890289307,
      "learning_rate": 0.00028307692307692306,
      "loss": 0.2903,
      "step": 32
    },
    {
      "epoch": 3.2195121951219514,
      "grad_norm": 0.5244333744049072,
      "learning_rate": 0.00028230769230769227,
      "loss": 0.2884,
      "step": 33
    },
    {
      "epoch": 3.317073170731707,
      "grad_norm": 0.505607545375824,
      "learning_rate": 0.0002815384615384615,
      "loss": 0.2834,
      "step": 34
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 0.56700599193573,
      "learning_rate": 0.00028076923076923076,
      "loss": 0.2721,
      "step": 35
    },
    {
      "epoch": 3.5121951219512195,
      "grad_norm": 0.5468815565109253,
      "learning_rate": 0.00028,
      "loss": 0.2985,
      "step": 36
    },
    {
      "epoch": 3.6097560975609757,
      "grad_norm": 0.5697494149208069,
      "learning_rate": 0.0002792307692307692,
      "loss": 0.2867,
      "step": 37
    },
    {
      "epoch": 3.7073170731707314,
      "grad_norm": 0.6153642535209656,
      "learning_rate": 0.00027846153846153846,
      "loss": 0.2814,
      "step": 38
    },
    {
      "epoch": 3.8048780487804876,
      "grad_norm": 0.5547342300415039,
      "learning_rate": 0.0002776923076923077,
      "loss": 0.2714,
      "step": 39
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 0.5045681595802307,
      "learning_rate": 0.0002769230769230769,
      "loss": 0.2587,
      "step": 40
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.5733579397201538,
      "learning_rate": 0.0002761538461538461,
      "loss": 0.4715,
      "step": 41
    },
    {
      "epoch": 4.097560975609756,
      "grad_norm": 0.5257881283760071,
      "learning_rate": 0.00027538461538461533,
      "loss": 0.2483,
      "step": 42
    },
    {
      "epoch": 4.195121951219512,
      "grad_norm": 0.5686198472976685,
      "learning_rate": 0.0002746153846153846,
      "loss": 0.26,
      "step": 43
    },
    {
      "epoch": 4.2926829268292686,
      "grad_norm": 0.5230434536933899,
      "learning_rate": 0.0002738461538461538,
      "loss": 0.2438,
      "step": 44
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 0.5369740128517151,
      "learning_rate": 0.00027307692307692303,
      "loss": 0.2456,
      "step": 45
    },
    {
      "epoch": 4.487804878048781,
      "grad_norm": 0.5583370327949524,
      "learning_rate": 0.0002723076923076923,
      "loss": 0.2312,
      "step": 46
    },
    {
      "epoch": 4.585365853658536,
      "grad_norm": 0.5641021132469177,
      "learning_rate": 0.0002715384615384615,
      "loss": 0.2261,
      "step": 47
    },
    {
      "epoch": 4.682926829268292,
      "grad_norm": 0.5338871479034424,
      "learning_rate": 0.00027076923076923073,
      "loss": 0.22,
      "step": 48
    },
    {
      "epoch": 4.780487804878049,
      "grad_norm": 0.5431174039840698,
      "learning_rate": 0.00027,
      "loss": 0.2231,
      "step": 49
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 0.5526816248893738,
      "learning_rate": 0.0002692307692307692,
      "loss": 0.2285,
      "step": 50
    },
    {
      "epoch": 4.975609756097561,
      "grad_norm": 0.536597728729248,
      "learning_rate": 0.00026846153846153844,
      "loss": 0.2158,
      "step": 51
    },
    {
      "epoch": 5.073170731707317,
      "grad_norm": 1.135835886001587,
      "learning_rate": 0.0002676923076923077,
      "loss": 0.3842,
      "step": 52
    },
    {
      "epoch": 5.170731707317073,
      "grad_norm": 0.532984733581543,
      "learning_rate": 0.00026692307692307687,
      "loss": 0.1976,
      "step": 53
    },
    {
      "epoch": 5.2682926829268295,
      "grad_norm": 0.6712321043014526,
      "learning_rate": 0.00026615384615384614,
      "loss": 0.2017,
      "step": 54
    },
    {
      "epoch": 5.365853658536586,
      "grad_norm": 0.5772542357444763,
      "learning_rate": 0.00026538461538461536,
      "loss": 0.1893,
      "step": 55
    },
    {
      "epoch": 5.463414634146342,
      "grad_norm": 0.5501004457473755,
      "learning_rate": 0.00026461538461538457,
      "loss": 0.1929,
      "step": 56
    },
    {
      "epoch": 5.560975609756097,
      "grad_norm": 0.6349871158599854,
      "learning_rate": 0.00026384615384615384,
      "loss": 0.2097,
      "step": 57
    },
    {
      "epoch": 5.658536585365853,
      "grad_norm": 0.7270243167877197,
      "learning_rate": 0.00026307692307692306,
      "loss": 0.2114,
      "step": 58
    },
    {
      "epoch": 5.7560975609756095,
      "grad_norm": 0.5376884341239929,
      "learning_rate": 0.0002623076923076923,
      "loss": 0.2002,
      "step": 59
    },
    {
      "epoch": 5.853658536585366,
      "grad_norm": 0.5493829250335693,
      "learning_rate": 0.00026153846153846154,
      "loss": 0.2065,
      "step": 60
    },
    {
      "epoch": 5.951219512195122,
      "grad_norm": 0.5016591548919678,
      "learning_rate": 0.00026076923076923076,
      "loss": 0.1781,
      "step": 61
    },
    {
      "epoch": 6.048780487804878,
      "grad_norm": 2.07698392868042,
      "learning_rate": 0.00026,
      "loss": 0.344,
      "step": 62
    },
    {
      "epoch": 6.146341463414634,
      "grad_norm": 0.5677909255027771,
      "learning_rate": 0.0002592307692307692,
      "loss": 0.192,
      "step": 63
    },
    {
      "epoch": 6.2439024390243905,
      "grad_norm": 0.5570665597915649,
      "learning_rate": 0.00025846153846153846,
      "loss": 0.1664,
      "step": 64
    },
    {
      "epoch": 6.341463414634147,
      "grad_norm": 0.5874233245849609,
      "learning_rate": 0.0002576923076923077,
      "loss": 0.1679,
      "step": 65
    },
    {
      "epoch": 6.439024390243903,
      "grad_norm": 0.7020028233528137,
      "learning_rate": 0.0002569230769230769,
      "loss": 0.1702,
      "step": 66
    },
    {
      "epoch": 6.536585365853659,
      "grad_norm": 0.6204599738121033,
      "learning_rate": 0.0002561538461538461,
      "loss": 0.161,
      "step": 67
    },
    {
      "epoch": 6.634146341463414,
      "grad_norm": 0.5555649399757385,
      "learning_rate": 0.00025538461538461533,
      "loss": 0.1683,
      "step": 68
    },
    {
      "epoch": 6.7317073170731705,
      "grad_norm": 0.5724385380744934,
      "learning_rate": 0.0002546153846153846,
      "loss": 0.1682,
      "step": 69
    },
    {
      "epoch": 6.829268292682927,
      "grad_norm": 0.6259167194366455,
      "learning_rate": 0.0002538461538461538,
      "loss": 0.1664,
      "step": 70
    },
    {
      "epoch": 6.926829268292683,
      "grad_norm": 0.6064708828926086,
      "learning_rate": 0.00025307692307692303,
      "loss": 0.1629,
      "step": 71
    },
    {
      "epoch": 7.024390243902439,
      "grad_norm": 1.3657511472702026,
      "learning_rate": 0.0002523076923076923,
      "loss": 0.3258,
      "step": 72
    },
    {
      "epoch": 7.121951219512195,
      "grad_norm": 0.5048367381095886,
      "learning_rate": 0.0002515384615384615,
      "loss": 0.1382,
      "step": 73
    },
    {
      "epoch": 7.219512195121951,
      "grad_norm": 0.6194593906402588,
      "learning_rate": 0.00025076923076923073,
      "loss": 0.1568,
      "step": 74
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 0.6548221707344055,
      "learning_rate": 0.00025,
      "loss": 0.1467,
      "step": 75
    },
    {
      "epoch": 7.414634146341464,
      "grad_norm": 0.816713273525238,
      "learning_rate": 0.0002492307692307692,
      "loss": 0.144,
      "step": 76
    },
    {
      "epoch": 7.512195121951219,
      "grad_norm": 0.6055901050567627,
      "learning_rate": 0.00024846153846153844,
      "loss": 0.1354,
      "step": 77
    },
    {
      "epoch": 7.609756097560975,
      "grad_norm": 0.6706947684288025,
      "learning_rate": 0.00024769230769230765,
      "loss": 0.1381,
      "step": 78
    },
    {
      "epoch": 7.7073170731707314,
      "grad_norm": 0.6298768520355225,
      "learning_rate": 0.00024692307692307687,
      "loss": 0.1384,
      "step": 79
    },
    {
      "epoch": 7.804878048780488,
      "grad_norm": 0.5907706618309021,
      "learning_rate": 0.00024615384615384614,
      "loss": 0.1595,
      "step": 80
    },
    {
      "epoch": 7.902439024390244,
      "grad_norm": 0.6141151785850525,
      "learning_rate": 0.00024538461538461536,
      "loss": 0.147,
      "step": 81
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.1924407482147217,
      "learning_rate": 0.0002446153846153846,
      "loss": 0.2501,
      "step": 82
    },
    {
      "epoch": 8.097560975609756,
      "grad_norm": 0.5279126763343811,
      "learning_rate": 0.00024384615384615382,
      "loss": 0.1282,
      "step": 83
    },
    {
      "epoch": 8.195121951219512,
      "grad_norm": 0.5631569623947144,
      "learning_rate": 0.00024307692307692306,
      "loss": 0.1132,
      "step": 84
    },
    {
      "epoch": 8.292682926829269,
      "grad_norm": 0.7641110420227051,
      "learning_rate": 0.0002423076923076923,
      "loss": 0.1314,
      "step": 85
    },
    {
      "epoch": 8.390243902439025,
      "grad_norm": 0.6767781972885132,
      "learning_rate": 0.00024153846153846152,
      "loss": 0.1259,
      "step": 86
    },
    {
      "epoch": 8.487804878048781,
      "grad_norm": 0.6028761267662048,
      "learning_rate": 0.00024076923076923076,
      "loss": 0.115,
      "step": 87
    },
    {
      "epoch": 8.585365853658537,
      "grad_norm": 0.6470917463302612,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.1202,
      "step": 88
    },
    {
      "epoch": 8.682926829268293,
      "grad_norm": 0.6636686325073242,
      "learning_rate": 0.00023923076923076922,
      "loss": 0.1291,
      "step": 89
    },
    {
      "epoch": 8.78048780487805,
      "grad_norm": 0.8454912900924683,
      "learning_rate": 0.00023846153846153844,
      "loss": 0.1251,
      "step": 90
    },
    {
      "epoch": 8.878048780487806,
      "grad_norm": 0.5862877368927002,
      "learning_rate": 0.00023769230769230765,
      "loss": 0.1191,
      "step": 91
    },
    {
      "epoch": 8.975609756097562,
      "grad_norm": 0.6324539184570312,
      "learning_rate": 0.0002369230769230769,
      "loss": 0.1195,
      "step": 92
    },
    {
      "epoch": 9.073170731707316,
      "grad_norm": 1.383712887763977,
      "learning_rate": 0.00023615384615384611,
      "loss": 0.2005,
      "step": 93
    },
    {
      "epoch": 9.170731707317072,
      "grad_norm": 0.6095142960548401,
      "learning_rate": 0.00023538461538461536,
      "loss": 0.0955,
      "step": 94
    },
    {
      "epoch": 9.268292682926829,
      "grad_norm": 0.6495273113250732,
      "learning_rate": 0.0002346153846153846,
      "loss": 0.1036,
      "step": 95
    },
    {
      "epoch": 9.365853658536585,
      "grad_norm": 0.6742532253265381,
      "learning_rate": 0.00023384615384615382,
      "loss": 0.1098,
      "step": 96
    },
    {
      "epoch": 9.463414634146341,
      "grad_norm": 0.7424139976501465,
      "learning_rate": 0.00023307692307692306,
      "loss": 0.1024,
      "step": 97
    },
    {
      "epoch": 9.560975609756097,
      "grad_norm": 0.7344977855682373,
      "learning_rate": 0.0002323076923076923,
      "loss": 0.1009,
      "step": 98
    },
    {
      "epoch": 9.658536585365853,
      "grad_norm": 0.7066766023635864,
      "learning_rate": 0.00023153846153846152,
      "loss": 0.1103,
      "step": 99
    },
    {
      "epoch": 9.75609756097561,
      "grad_norm": 0.6505128741264343,
      "learning_rate": 0.00023076923076923076,
      "loss": 0.1103,
      "step": 100
    },
    {
      "epoch": 9.853658536585366,
      "grad_norm": 0.6909196376800537,
      "learning_rate": 0.00023,
      "loss": 0.1068,
      "step": 101
    },
    {
      "epoch": 9.951219512195122,
      "grad_norm": 0.704465925693512,
      "learning_rate": 0.0002292307692307692,
      "loss": 0.1089,
      "step": 102
    },
    {
      "epoch": 10.048780487804878,
      "grad_norm": 1.4778029918670654,
      "learning_rate": 0.00022846153846153844,
      "loss": 0.1629,
      "step": 103
    },
    {
      "epoch": 10.146341463414634,
      "grad_norm": 0.7382885217666626,
      "learning_rate": 0.00022769230769230766,
      "loss": 0.0955,
      "step": 104
    },
    {
      "epoch": 10.24390243902439,
      "grad_norm": 0.7576610445976257,
      "learning_rate": 0.0002269230769230769,
      "loss": 0.0883,
      "step": 105
    },
    {
      "epoch": 10.341463414634147,
      "grad_norm": 0.720775842666626,
      "learning_rate": 0.00022615384615384614,
      "loss": 0.0952,
      "step": 106
    },
    {
      "epoch": 10.439024390243903,
      "grad_norm": 0.6352006793022156,
      "learning_rate": 0.00022538461538461536,
      "loss": 0.0822,
      "step": 107
    },
    {
      "epoch": 10.536585365853659,
      "grad_norm": 0.655957043170929,
      "learning_rate": 0.0002246153846153846,
      "loss": 0.0909,
      "step": 108
    },
    {
      "epoch": 10.634146341463415,
      "grad_norm": 0.6696830987930298,
      "learning_rate": 0.00022384615384615382,
      "loss": 0.0938,
      "step": 109
    },
    {
      "epoch": 10.731707317073171,
      "grad_norm": 0.6879867911338806,
      "learning_rate": 0.00022307692307692306,
      "loss": 0.0932,
      "step": 110
    },
    {
      "epoch": 10.829268292682928,
      "grad_norm": 0.7457046508789062,
      "learning_rate": 0.0002223076923076923,
      "loss": 0.0997,
      "step": 111
    },
    {
      "epoch": 10.926829268292684,
      "grad_norm": 0.6914639472961426,
      "learning_rate": 0.00022153846153846152,
      "loss": 0.0935,
      "step": 112
    },
    {
      "epoch": 11.024390243902438,
      "grad_norm": 1.2573693990707397,
      "learning_rate": 0.00022076923076923076,
      "loss": 0.142,
      "step": 113
    },
    {
      "epoch": 11.121951219512194,
      "grad_norm": 0.5501760840415955,
      "learning_rate": 0.00021999999999999995,
      "loss": 0.0769,
      "step": 114
    },
    {
      "epoch": 11.21951219512195,
      "grad_norm": 0.502940833568573,
      "learning_rate": 0.0002192307692307692,
      "loss": 0.0749,
      "step": 115
    },
    {
      "epoch": 11.317073170731707,
      "grad_norm": 0.8071064352989197,
      "learning_rate": 0.00021846153846153844,
      "loss": 0.0865,
      "step": 116
    },
    {
      "epoch": 11.414634146341463,
      "grad_norm": 0.7430896162986755,
      "learning_rate": 0.00021769230769230766,
      "loss": 0.0804,
      "step": 117
    },
    {
      "epoch": 11.512195121951219,
      "grad_norm": 0.7352606058120728,
      "learning_rate": 0.0002169230769230769,
      "loss": 0.0806,
      "step": 118
    },
    {
      "epoch": 11.609756097560975,
      "grad_norm": 0.5855184197425842,
      "learning_rate": 0.00021615384615384614,
      "loss": 0.0781,
      "step": 119
    },
    {
      "epoch": 11.707317073170731,
      "grad_norm": 0.7122922539710999,
      "learning_rate": 0.00021538461538461536,
      "loss": 0.0859,
      "step": 120
    },
    {
      "epoch": 11.804878048780488,
      "grad_norm": 0.7619449496269226,
      "learning_rate": 0.0002146153846153846,
      "loss": 0.0818,
      "step": 121
    },
    {
      "epoch": 11.902439024390244,
      "grad_norm": 0.7108321189880371,
      "learning_rate": 0.00021384615384615385,
      "loss": 0.082,
      "step": 122
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.2205560207366943,
      "learning_rate": 0.00021307692307692306,
      "loss": 0.1225,
      "step": 123
    },
    {
      "epoch": 12.097560975609756,
      "grad_norm": 0.6738246083259583,
      "learning_rate": 0.0002123076923076923,
      "loss": 0.0682,
      "step": 124
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 0.6797332167625427,
      "learning_rate": 0.00021153846153846152,
      "loss": 0.0703,
      "step": 125
    },
    {
      "epoch": 12.292682926829269,
      "grad_norm": 0.9588404297828674,
      "learning_rate": 0.00021076923076923074,
      "loss": 0.0705,
      "step": 126
    },
    {
      "epoch": 12.390243902439025,
      "grad_norm": 0.5276470184326172,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.0654,
      "step": 127
    },
    {
      "epoch": 12.487804878048781,
      "grad_norm": 0.62247633934021,
      "learning_rate": 0.0002092307692307692,
      "loss": 0.075,
      "step": 128
    },
    {
      "epoch": 12.585365853658537,
      "grad_norm": 0.6097065806388855,
      "learning_rate": 0.00020846153846153844,
      "loss": 0.072,
      "step": 129
    },
    {
      "epoch": 12.682926829268293,
      "grad_norm": 0.733567476272583,
      "learning_rate": 0.00020769230769230766,
      "loss": 0.0789,
      "step": 130
    },
    {
      "epoch": 12.78048780487805,
      "grad_norm": 0.6940736174583435,
      "learning_rate": 0.0002069230769230769,
      "loss": 0.0725,
      "step": 131
    },
    {
      "epoch": 12.878048780487806,
      "grad_norm": 0.644993007183075,
      "learning_rate": 0.00020615384615384614,
      "loss": 0.0696,
      "step": 132
    },
    {
      "epoch": 12.975609756097562,
      "grad_norm": 0.6912133097648621,
      "learning_rate": 0.00020538461538461536,
      "loss": 0.0689,
      "step": 133
    },
    {
      "epoch": 13.073170731707316,
      "grad_norm": 1.4255023002624512,
      "learning_rate": 0.0002046153846153846,
      "loss": 0.1256,
      "step": 134
    },
    {
      "epoch": 13.170731707317072,
      "grad_norm": 0.5343582034111023,
      "learning_rate": 0.00020384615384615385,
      "loss": 0.059,
      "step": 135
    },
    {
      "epoch": 13.268292682926829,
      "grad_norm": 0.6651083827018738,
      "learning_rate": 0.00020307692307692306,
      "loss": 0.0648,
      "step": 136
    },
    {
      "epoch": 13.365853658536585,
      "grad_norm": 0.625031590461731,
      "learning_rate": 0.0002023076923076923,
      "loss": 0.0674,
      "step": 137
    },
    {
      "epoch": 13.463414634146341,
      "grad_norm": 0.7349858283996582,
      "learning_rate": 0.0002015384615384615,
      "loss": 0.0671,
      "step": 138
    },
    {
      "epoch": 13.560975609756097,
      "grad_norm": 0.5634793639183044,
      "learning_rate": 0.00020076923076923074,
      "loss": 0.0582,
      "step": 139
    },
    {
      "epoch": 13.658536585365853,
      "grad_norm": 0.63026362657547,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.0656,
      "step": 140
    },
    {
      "epoch": 13.75609756097561,
      "grad_norm": 0.629285991191864,
      "learning_rate": 0.0001992307692307692,
      "loss": 0.0621,
      "step": 141
    },
    {
      "epoch": 13.853658536585366,
      "grad_norm": 0.7438265085220337,
      "learning_rate": 0.00019846153846153844,
      "loss": 0.0688,
      "step": 142
    },
    {
      "epoch": 13.951219512195122,
      "grad_norm": 0.7238684892654419,
      "learning_rate": 0.00019769230769230769,
      "loss": 0.066,
      "step": 143
    },
    {
      "epoch": 14.048780487804878,
      "grad_norm": 1.0583491325378418,
      "learning_rate": 0.0001969230769230769,
      "loss": 0.104,
      "step": 144
    },
    {
      "epoch": 14.146341463414634,
      "grad_norm": 0.423403263092041,
      "learning_rate": 0.00019615384615384615,
      "loss": 0.0517,
      "step": 145
    },
    {
      "epoch": 14.24390243902439,
      "grad_norm": 0.5006770491600037,
      "learning_rate": 0.00019538461538461536,
      "loss": 0.0533,
      "step": 146
    },
    {
      "epoch": 14.341463414634147,
      "grad_norm": 0.6806641221046448,
      "learning_rate": 0.0001946153846153846,
      "loss": 0.055,
      "step": 147
    },
    {
      "epoch": 14.439024390243903,
      "grad_norm": 0.7466436624526978,
      "learning_rate": 0.00019384615384615385,
      "loss": 0.0603,
      "step": 148
    },
    {
      "epoch": 14.536585365853659,
      "grad_norm": 0.6906982660293579,
      "learning_rate": 0.00019307692307692306,
      "loss": 0.0588,
      "step": 149
    },
    {
      "epoch": 14.634146341463415,
      "grad_norm": 0.7460903525352478,
      "learning_rate": 0.0001923076923076923,
      "loss": 0.0632,
      "step": 150
    },
    {
      "epoch": 14.731707317073171,
      "grad_norm": 0.6330215334892273,
      "learning_rate": 0.0001915384615384615,
      "loss": 0.0599,
      "step": 151
    },
    {
      "epoch": 14.829268292682928,
      "grad_norm": 0.6250990033149719,
      "learning_rate": 0.00019076923076923074,
      "loss": 0.0539,
      "step": 152
    },
    {
      "epoch": 14.926829268292684,
      "grad_norm": 0.6228105425834656,
      "learning_rate": 0.00018999999999999998,
      "loss": 0.0577,
      "step": 153
    },
    {
      "epoch": 15.024390243902438,
      "grad_norm": 1.210959792137146,
      "learning_rate": 0.0001892307692307692,
      "loss": 0.0976,
      "step": 154
    },
    {
      "epoch": 15.121951219512194,
      "grad_norm": 0.48972031474113464,
      "learning_rate": 0.00018846153846153844,
      "loss": 0.0495,
      "step": 155
    },
    {
      "epoch": 15.21951219512195,
      "grad_norm": 0.5224996209144592,
      "learning_rate": 0.00018769230769230769,
      "loss": 0.0465,
      "step": 156
    },
    {
      "epoch": 15.317073170731707,
      "grad_norm": 0.6484120488166809,
      "learning_rate": 0.0001869230769230769,
      "loss": 0.0503,
      "step": 157
    },
    {
      "epoch": 15.414634146341463,
      "grad_norm": 0.6339535117149353,
      "learning_rate": 0.00018615384615384615,
      "loss": 0.0524,
      "step": 158
    },
    {
      "epoch": 15.512195121951219,
      "grad_norm": 0.7197233438491821,
      "learning_rate": 0.0001853846153846154,
      "loss": 0.0539,
      "step": 159
    },
    {
      "epoch": 15.609756097560975,
      "grad_norm": 0.6404421925544739,
      "learning_rate": 0.0001846153846153846,
      "loss": 0.0528,
      "step": 160
    },
    {
      "epoch": 15.707317073170731,
      "grad_norm": 0.63566654920578,
      "learning_rate": 0.00018384615384615385,
      "loss": 0.0514,
      "step": 161
    },
    {
      "epoch": 15.804878048780488,
      "grad_norm": 0.5203186273574829,
      "learning_rate": 0.00018307692307692307,
      "loss": 0.0468,
      "step": 162
    },
    {
      "epoch": 15.902439024390244,
      "grad_norm": 0.6506998538970947,
      "learning_rate": 0.00018230769230769228,
      "loss": 0.0544,
      "step": 163
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.9554619193077087,
      "learning_rate": 0.0001815384615384615,
      "loss": 0.0799,
      "step": 164
    },
    {
      "epoch": 16.097560975609756,
      "grad_norm": 0.6139424443244934,
      "learning_rate": 0.00018076923076923074,
      "loss": 0.0435,
      "step": 165
    },
    {
      "epoch": 16.195121951219512,
      "grad_norm": 0.49532243609428406,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.0442,
      "step": 166
    },
    {
      "epoch": 16.29268292682927,
      "grad_norm": 0.6888472437858582,
      "learning_rate": 0.0001792307692307692,
      "loss": 0.0452,
      "step": 167
    },
    {
      "epoch": 16.390243902439025,
      "grad_norm": 0.5952242612838745,
      "learning_rate": 0.00017846153846153844,
      "loss": 0.0445,
      "step": 168
    },
    {
      "epoch": 16.48780487804878,
      "grad_norm": 0.4841260313987732,
      "learning_rate": 0.0001776923076923077,
      "loss": 0.0427,
      "step": 169
    },
    {
      "epoch": 16.585365853658537,
      "grad_norm": 0.4836975932121277,
      "learning_rate": 0.0001769230769230769,
      "loss": 0.0387,
      "step": 170
    },
    {
      "epoch": 16.682926829268293,
      "grad_norm": 0.5039295554161072,
      "learning_rate": 0.00017615384615384615,
      "loss": 0.0404,
      "step": 171
    },
    {
      "epoch": 16.78048780487805,
      "grad_norm": 0.6273213624954224,
      "learning_rate": 0.0001753846153846154,
      "loss": 0.0491,
      "step": 172
    },
    {
      "epoch": 16.878048780487806,
      "grad_norm": 0.5240232944488525,
      "learning_rate": 0.0001746153846153846,
      "loss": 0.0446,
      "step": 173
    },
    {
      "epoch": 16.975609756097562,
      "grad_norm": 0.5863611102104187,
      "learning_rate": 0.00017384615384615385,
      "loss": 0.0457,
      "step": 174
    },
    {
      "epoch": 17.073170731707318,
      "grad_norm": 1.214839220046997,
      "learning_rate": 0.00017307692307692304,
      "loss": 0.0723,
      "step": 175
    },
    {
      "epoch": 17.170731707317074,
      "grad_norm": 0.4168987572193146,
      "learning_rate": 0.00017230769230769228,
      "loss": 0.0388,
      "step": 176
    },
    {
      "epoch": 17.26829268292683,
      "grad_norm": 0.5379031300544739,
      "learning_rate": 0.00017153846153846153,
      "loss": 0.0354,
      "step": 177
    },
    {
      "epoch": 17.365853658536587,
      "grad_norm": 0.6733187437057495,
      "learning_rate": 0.00017076923076923074,
      "loss": 0.04,
      "step": 178
    },
    {
      "epoch": 17.463414634146343,
      "grad_norm": 0.6255972385406494,
      "learning_rate": 0.00016999999999999999,
      "loss": 0.0379,
      "step": 179
    },
    {
      "epoch": 17.5609756097561,
      "grad_norm": 0.5477468967437744,
      "learning_rate": 0.0001692307692307692,
      "loss": 0.0355,
      "step": 180
    },
    {
      "epoch": 17.658536585365855,
      "grad_norm": 0.5914890766143799,
      "learning_rate": 0.00016846153846153844,
      "loss": 0.0388,
      "step": 181
    },
    {
      "epoch": 17.75609756097561,
      "grad_norm": 0.5334988236427307,
      "learning_rate": 0.0001676923076923077,
      "loss": 0.0386,
      "step": 182
    },
    {
      "epoch": 17.853658536585368,
      "grad_norm": 0.5556900501251221,
      "learning_rate": 0.0001669230769230769,
      "loss": 0.04,
      "step": 183
    },
    {
      "epoch": 17.951219512195124,
      "grad_norm": 0.525851309299469,
      "learning_rate": 0.00016615384615384615,
      "loss": 0.038,
      "step": 184
    },
    {
      "epoch": 18.048780487804876,
      "grad_norm": 1.108210563659668,
      "learning_rate": 0.0001653846153846154,
      "loss": 0.0673,
      "step": 185
    },
    {
      "epoch": 18.146341463414632,
      "grad_norm": 0.40857237577438354,
      "learning_rate": 0.0001646153846153846,
      "loss": 0.0333,
      "step": 186
    },
    {
      "epoch": 18.24390243902439,
      "grad_norm": 0.45162689685821533,
      "learning_rate": 0.00016384615384615382,
      "loss": 0.0303,
      "step": 187
    },
    {
      "epoch": 18.341463414634145,
      "grad_norm": 0.5644371509552002,
      "learning_rate": 0.00016307692307692304,
      "loss": 0.0323,
      "step": 188
    },
    {
      "epoch": 18.4390243902439,
      "grad_norm": 0.7188900113105774,
      "learning_rate": 0.00016230769230769228,
      "loss": 0.0342,
      "step": 189
    },
    {
      "epoch": 18.536585365853657,
      "grad_norm": 0.5177865028381348,
      "learning_rate": 0.00016153846153846153,
      "loss": 0.0351,
      "step": 190
    },
    {
      "epoch": 18.634146341463413,
      "grad_norm": 0.5620750784873962,
      "learning_rate": 0.00016076923076923074,
      "loss": 0.034,
      "step": 191
    },
    {
      "epoch": 18.73170731707317,
      "grad_norm": 0.48446351289749146,
      "learning_rate": 0.00015999999999999999,
      "loss": 0.0329,
      "step": 192
    },
    {
      "epoch": 18.829268292682926,
      "grad_norm": 0.6095507740974426,
      "learning_rate": 0.00015923076923076923,
      "loss": 0.0349,
      "step": 193
    },
    {
      "epoch": 18.926829268292682,
      "grad_norm": 0.597571849822998,
      "learning_rate": 0.00015846153846153845,
      "loss": 0.038,
      "step": 194
    },
    {
      "epoch": 19.024390243902438,
      "grad_norm": 0.9382821321487427,
      "learning_rate": 0.0001576923076923077,
      "loss": 0.0541,
      "step": 195
    },
    {
      "epoch": 19.121951219512194,
      "grad_norm": 0.5788553357124329,
      "learning_rate": 0.0001569230769230769,
      "loss": 0.0309,
      "step": 196
    },
    {
      "epoch": 19.21951219512195,
      "grad_norm": 0.5028957724571228,
      "learning_rate": 0.00015615384615384615,
      "loss": 0.0342,
      "step": 197
    },
    {
      "epoch": 19.317073170731707,
      "grad_norm": 0.6116137504577637,
      "learning_rate": 0.0001553846153846154,
      "loss": 0.0293,
      "step": 198
    },
    {
      "epoch": 19.414634146341463,
      "grad_norm": 0.5189238786697388,
      "learning_rate": 0.00015461538461538458,
      "loss": 0.0272,
      "step": 199
    },
    {
      "epoch": 19.51219512195122,
      "grad_norm": 0.5376201272010803,
      "learning_rate": 0.00015384615384615382,
      "loss": 0.0296,
      "step": 200
    },
    {
      "epoch": 19.609756097560975,
      "grad_norm": 0.7392184138298035,
      "learning_rate": 0.00015307692307692304,
      "loss": 0.0366,
      "step": 201
    },
    {
      "epoch": 19.70731707317073,
      "grad_norm": 0.6146701574325562,
      "learning_rate": 0.00015230769230769228,
      "loss": 0.0323,
      "step": 202
    },
    {
      "epoch": 19.804878048780488,
      "grad_norm": 0.6201171278953552,
      "learning_rate": 0.00015153846153846153,
      "loss": 0.0318,
      "step": 203
    },
    {
      "epoch": 19.902439024390244,
      "grad_norm": 0.4902511239051819,
      "learning_rate": 0.00015076923076923074,
      "loss": 0.0314,
      "step": 204
    },
    {
      "epoch": 20.0,
      "grad_norm": 1.3540265560150146,
      "learning_rate": 0.00015,
      "loss": 0.0523,
      "step": 205
    },
    {
      "epoch": 20.097560975609756,
      "grad_norm": 0.41921260952949524,
      "learning_rate": 0.00014923076923076923,
      "loss": 0.0254,
      "step": 206
    },
    {
      "epoch": 20.195121951219512,
      "grad_norm": 0.43475791811943054,
      "learning_rate": 0.00014846153846153845,
      "loss": 0.026,
      "step": 207
    },
    {
      "epoch": 20.29268292682927,
      "grad_norm": 0.5099043846130371,
      "learning_rate": 0.0001476923076923077,
      "loss": 0.0246,
      "step": 208
    },
    {
      "epoch": 20.390243902439025,
      "grad_norm": 0.7238960266113281,
      "learning_rate": 0.0001469230769230769,
      "loss": 0.0294,
      "step": 209
    },
    {
      "epoch": 20.48780487804878,
      "grad_norm": 0.784655749797821,
      "learning_rate": 0.00014615384615384615,
      "loss": 0.0307,
      "step": 210
    },
    {
      "epoch": 20.585365853658537,
      "grad_norm": 0.6259937286376953,
      "learning_rate": 0.00014538461538461537,
      "loss": 0.0283,
      "step": 211
    },
    {
      "epoch": 20.682926829268293,
      "grad_norm": 0.5335004329681396,
      "learning_rate": 0.0001446153846153846,
      "loss": 0.0277,
      "step": 212
    },
    {
      "epoch": 20.78048780487805,
      "grad_norm": 0.4414210617542267,
      "learning_rate": 0.00014384615384615383,
      "loss": 0.0258,
      "step": 213
    },
    {
      "epoch": 20.878048780487806,
      "grad_norm": 0.4176359474658966,
      "learning_rate": 0.00014307692307692307,
      "loss": 0.0287,
      "step": 214
    },
    {
      "epoch": 20.975609756097562,
      "grad_norm": 0.4807770550251007,
      "learning_rate": 0.00014230769230769228,
      "loss": 0.0286,
      "step": 215
    },
    {
      "epoch": 21.073170731707318,
      "grad_norm": 1.4006587266921997,
      "learning_rate": 0.00014153846153846153,
      "loss": 0.0578,
      "step": 216
    },
    {
      "epoch": 21.170731707317074,
      "grad_norm": 0.48218414187431335,
      "learning_rate": 0.00014076923076923074,
      "loss": 0.0256,
      "step": 217
    },
    {
      "epoch": 21.26829268292683,
      "grad_norm": 0.3771654963493347,
      "learning_rate": 0.00014,
      "loss": 0.0211,
      "step": 218
    },
    {
      "epoch": 21.365853658536587,
      "grad_norm": 0.5208067297935486,
      "learning_rate": 0.00013923076923076923,
      "loss": 0.0206,
      "step": 219
    },
    {
      "epoch": 21.463414634146343,
      "grad_norm": 0.49411705136299133,
      "learning_rate": 0.00013846153846153845,
      "loss": 0.0214,
      "step": 220
    },
    {
      "epoch": 21.5609756097561,
      "grad_norm": 0.5107963681221008,
      "learning_rate": 0.00013769230769230766,
      "loss": 0.0225,
      "step": 221
    },
    {
      "epoch": 21.658536585365855,
      "grad_norm": 0.48827677965164185,
      "learning_rate": 0.0001369230769230769,
      "loss": 0.0212,
      "step": 222
    },
    {
      "epoch": 21.75609756097561,
      "grad_norm": 0.5707181692123413,
      "learning_rate": 0.00013615384615384615,
      "loss": 0.0267,
      "step": 223
    },
    {
      "epoch": 21.853658536585368,
      "grad_norm": 0.7697780132293701,
      "learning_rate": 0.00013538461538461537,
      "loss": 0.0262,
      "step": 224
    },
    {
      "epoch": 21.951219512195124,
      "grad_norm": 0.48068559169769287,
      "learning_rate": 0.0001346153846153846,
      "loss": 0.0232,
      "step": 225
    },
    {
      "epoch": 22.048780487804876,
      "grad_norm": 0.7979193925857544,
      "learning_rate": 0.00013384615384615385,
      "loss": 0.0356,
      "step": 226
    },
    {
      "epoch": 22.146341463414632,
      "grad_norm": 0.3533024489879608,
      "learning_rate": 0.00013307692307692307,
      "loss": 0.0209,
      "step": 227
    },
    {
      "epoch": 22.24390243902439,
      "grad_norm": 0.3869479298591614,
      "learning_rate": 0.00013230769230769229,
      "loss": 0.0194,
      "step": 228
    },
    {
      "epoch": 22.341463414634145,
      "grad_norm": 0.5782992839813232,
      "learning_rate": 0.00013153846153846153,
      "loss": 0.0181,
      "step": 229
    },
    {
      "epoch": 22.4390243902439,
      "grad_norm": 0.42131221294403076,
      "learning_rate": 0.00013076923076923077,
      "loss": 0.0184,
      "step": 230
    },
    {
      "epoch": 22.536585365853657,
      "grad_norm": 0.5194705724716187,
      "learning_rate": 0.00013,
      "loss": 0.0208,
      "step": 231
    },
    {
      "epoch": 22.634146341463413,
      "grad_norm": 0.7514957785606384,
      "learning_rate": 0.00012923076923076923,
      "loss": 0.0236,
      "step": 232
    },
    {
      "epoch": 22.73170731707317,
      "grad_norm": 0.4253517687320709,
      "learning_rate": 0.00012846153846153845,
      "loss": 0.0183,
      "step": 233
    },
    {
      "epoch": 22.829268292682926,
      "grad_norm": 0.5269649624824524,
      "learning_rate": 0.00012769230769230766,
      "loss": 0.0198,
      "step": 234
    },
    {
      "epoch": 22.926829268292682,
      "grad_norm": 0.5264593958854675,
      "learning_rate": 0.0001269230769230769,
      "loss": 0.0231,
      "step": 235
    },
    {
      "epoch": 23.024390243902438,
      "grad_norm": 1.401538372039795,
      "learning_rate": 0.00012615384615384615,
      "loss": 0.0399,
      "step": 236
    },
    {
      "epoch": 23.121951219512194,
      "grad_norm": 0.528030276298523,
      "learning_rate": 0.00012538461538461537,
      "loss": 0.0182,
      "step": 237
    },
    {
      "epoch": 23.21951219512195,
      "grad_norm": 0.4533112347126007,
      "learning_rate": 0.0001246153846153846,
      "loss": 0.0191,
      "step": 238
    },
    {
      "epoch": 23.317073170731707,
      "grad_norm": 0.4578198194503784,
      "learning_rate": 0.00012384615384615383,
      "loss": 0.0195,
      "step": 239
    },
    {
      "epoch": 23.414634146341463,
      "grad_norm": 0.38858354091644287,
      "learning_rate": 0.00012307692307692307,
      "loss": 0.0186,
      "step": 240
    },
    {
      "epoch": 23.51219512195122,
      "grad_norm": 0.5801317095756531,
      "learning_rate": 0.0001223076923076923,
      "loss": 0.0161,
      "step": 241
    },
    {
      "epoch": 23.609756097560975,
      "grad_norm": 0.5632249712944031,
      "learning_rate": 0.00012153846153846153,
      "loss": 0.0205,
      "step": 242
    },
    {
      "epoch": 23.70731707317073,
      "grad_norm": 0.5434114336967468,
      "learning_rate": 0.00012076923076923076,
      "loss": 0.0186,
      "step": 243
    },
    {
      "epoch": 23.804878048780488,
      "grad_norm": 0.48785775899887085,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.019,
      "step": 244
    },
    {
      "epoch": 23.902439024390244,
      "grad_norm": 0.41814860701560974,
      "learning_rate": 0.00011923076923076922,
      "loss": 0.018,
      "step": 245
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.8619291186332703,
      "learning_rate": 0.00011846153846153845,
      "loss": 0.0347,
      "step": 246
    },
    {
      "epoch": 24.097560975609756,
      "grad_norm": 0.3878897726535797,
      "learning_rate": 0.00011769230769230768,
      "loss": 0.016,
      "step": 247
    },
    {
      "epoch": 24.195121951219512,
      "grad_norm": 1.0587047338485718,
      "learning_rate": 0.00011692307692307691,
      "loss": 0.0184,
      "step": 248
    },
    {
      "epoch": 24.29268292682927,
      "grad_norm": 0.45837581157684326,
      "learning_rate": 0.00011615384615384615,
      "loss": 0.0126,
      "step": 249
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 0.35563671588897705,
      "learning_rate": 0.00011538461538461538,
      "loss": 0.0146,
      "step": 250
    },
    {
      "epoch": 24.48780487804878,
      "grad_norm": 0.6001518964767456,
      "learning_rate": 0.0001146153846153846,
      "loss": 0.0147,
      "step": 251
    },
    {
      "epoch": 24.585365853658537,
      "grad_norm": 0.4460151195526123,
      "learning_rate": 0.00011384615384615383,
      "loss": 0.0163,
      "step": 252
    },
    {
      "epoch": 24.682926829268293,
      "grad_norm": 0.6010026931762695,
      "learning_rate": 0.00011307692307692307,
      "loss": 0.0184,
      "step": 253
    },
    {
      "epoch": 24.78048780487805,
      "grad_norm": 0.5507592558860779,
      "learning_rate": 0.0001123076923076923,
      "loss": 0.0171,
      "step": 254
    },
    {
      "epoch": 24.878048780487806,
      "grad_norm": 0.37353751063346863,
      "learning_rate": 0.00011153846153846153,
      "loss": 0.0164,
      "step": 255
    },
    {
      "epoch": 24.975609756097562,
      "grad_norm": 0.5877020359039307,
      "learning_rate": 0.00011076923076923076,
      "loss": 0.0202,
      "step": 256
    },
    {
      "epoch": 25.073170731707318,
      "grad_norm": 0.5540556907653809,
      "learning_rate": 0.00010999999999999998,
      "loss": 0.0243,
      "step": 257
    },
    {
      "epoch": 25.170731707317074,
      "grad_norm": 0.34732139110565186,
      "learning_rate": 0.00010923076923076922,
      "loss": 0.0143,
      "step": 258
    },
    {
      "epoch": 25.26829268292683,
      "grad_norm": 0.36438003182411194,
      "learning_rate": 0.00010846153846153845,
      "loss": 0.0119,
      "step": 259
    },
    {
      "epoch": 25.365853658536587,
      "grad_norm": 0.4619157910346985,
      "learning_rate": 0.00010769230769230768,
      "loss": 0.0125,
      "step": 260
    },
    {
      "epoch": 25.463414634146343,
      "grad_norm": 0.3390811085700989,
      "learning_rate": 0.00010692307692307692,
      "loss": 0.0126,
      "step": 261
    },
    {
      "epoch": 25.5609756097561,
      "grad_norm": 0.4298611283302307,
      "learning_rate": 0.00010615384615384615,
      "loss": 0.0153,
      "step": 262
    },
    {
      "epoch": 25.658536585365855,
      "grad_norm": 0.6820714473724365,
      "learning_rate": 0.00010538461538461537,
      "loss": 0.0168,
      "step": 263
    },
    {
      "epoch": 25.75609756097561,
      "grad_norm": 0.3639431297779083,
      "learning_rate": 0.0001046153846153846,
      "loss": 0.0131,
      "step": 264
    },
    {
      "epoch": 25.853658536585368,
      "grad_norm": 0.42219892144203186,
      "learning_rate": 0.00010384615384615383,
      "loss": 0.0142,
      "step": 265
    },
    {
      "epoch": 25.951219512195124,
      "grad_norm": 0.8678929209709167,
      "learning_rate": 0.00010307692307692307,
      "loss": 0.0196,
      "step": 266
    },
    {
      "epoch": 26.048780487804876,
      "grad_norm": 0.9334624409675598,
      "learning_rate": 0.0001023076923076923,
      "loss": 0.0247,
      "step": 267
    },
    {
      "epoch": 26.146341463414632,
      "grad_norm": 0.34598731994628906,
      "learning_rate": 0.00010153846153846153,
      "loss": 0.0132,
      "step": 268
    },
    {
      "epoch": 26.24390243902439,
      "grad_norm": 0.6229459047317505,
      "learning_rate": 0.00010076923076923075,
      "loss": 0.0161,
      "step": 269
    },
    {
      "epoch": 26.341463414634145,
      "grad_norm": 0.7016772627830505,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.0144,
      "step": 270
    },
    {
      "epoch": 26.4390243902439,
      "grad_norm": 0.3479190170764923,
      "learning_rate": 9.923076923076922e-05,
      "loss": 0.0125,
      "step": 271
    },
    {
      "epoch": 26.536585365853657,
      "grad_norm": 0.5214119553565979,
      "learning_rate": 9.846153846153845e-05,
      "loss": 0.0141,
      "step": 272
    },
    {
      "epoch": 26.634146341463413,
      "grad_norm": 0.5245120525360107,
      "learning_rate": 9.769230769230768e-05,
      "loss": 0.0138,
      "step": 273
    },
    {
      "epoch": 26.73170731707317,
      "grad_norm": 0.6754303574562073,
      "learning_rate": 9.692307692307692e-05,
      "loss": 0.0133,
      "step": 274
    },
    {
      "epoch": 26.829268292682926,
      "grad_norm": 0.30768051743507385,
      "learning_rate": 9.615384615384615e-05,
      "loss": 0.013,
      "step": 275
    },
    {
      "epoch": 26.926829268292682,
      "grad_norm": 0.39478030800819397,
      "learning_rate": 9.538461538461537e-05,
      "loss": 0.0139,
      "step": 276
    },
    {
      "epoch": 27.024390243902438,
      "grad_norm": 0.557672381401062,
      "learning_rate": 9.46153846153846e-05,
      "loss": 0.0173,
      "step": 277
    },
    {
      "epoch": 27.121951219512194,
      "grad_norm": 0.27742791175842285,
      "learning_rate": 9.384615384615384e-05,
      "loss": 0.0114,
      "step": 278
    },
    {
      "epoch": 27.21951219512195,
      "grad_norm": 0.3513450622558594,
      "learning_rate": 9.307692307692307e-05,
      "loss": 0.0125,
      "step": 279
    },
    {
      "epoch": 27.317073170731707,
      "grad_norm": 0.2305544912815094,
      "learning_rate": 9.23076923076923e-05,
      "loss": 0.0118,
      "step": 280
    },
    {
      "epoch": 27.414634146341463,
      "grad_norm": 0.24469336867332458,
      "learning_rate": 9.153846153846153e-05,
      "loss": 0.0104,
      "step": 281
    },
    {
      "epoch": 27.51219512195122,
      "grad_norm": 0.3253844678401947,
      "learning_rate": 9.076923076923075e-05,
      "loss": 0.0121,
      "step": 282
    },
    {
      "epoch": 27.609756097560975,
      "grad_norm": 0.4229481518268585,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.0132,
      "step": 283
    },
    {
      "epoch": 27.70731707317073,
      "grad_norm": 0.4246838390827179,
      "learning_rate": 8.923076923076922e-05,
      "loss": 0.012,
      "step": 284
    },
    {
      "epoch": 27.804878048780488,
      "grad_norm": 0.4446027874946594,
      "learning_rate": 8.846153846153845e-05,
      "loss": 0.0109,
      "step": 285
    },
    {
      "epoch": 27.902439024390244,
      "grad_norm": 0.32618746161460876,
      "learning_rate": 8.76923076923077e-05,
      "loss": 0.0121,
      "step": 286
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.4539920389652252,
      "learning_rate": 8.692307692307692e-05,
      "loss": 0.0172,
      "step": 287
    },
    {
      "epoch": 28.097560975609756,
      "grad_norm": 0.37440332770347595,
      "learning_rate": 8.615384615384614e-05,
      "loss": 0.0117,
      "step": 288
    },
    {
      "epoch": 28.195121951219512,
      "grad_norm": 0.3923374116420746,
      "learning_rate": 8.538461538461537e-05,
      "loss": 0.0103,
      "step": 289
    },
    {
      "epoch": 28.29268292682927,
      "grad_norm": 0.203386127948761,
      "learning_rate": 8.46153846153846e-05,
      "loss": 0.0107,
      "step": 290
    },
    {
      "epoch": 28.390243902439025,
      "grad_norm": 0.2144327610731125,
      "learning_rate": 8.384615384615384e-05,
      "loss": 0.0097,
      "step": 291
    },
    {
      "epoch": 28.48780487804878,
      "grad_norm": 0.15066662430763245,
      "learning_rate": 8.307692307692307e-05,
      "loss": 0.0088,
      "step": 292
    },
    {
      "epoch": 28.585365853658537,
      "grad_norm": 0.28252846002578735,
      "learning_rate": 8.23076923076923e-05,
      "loss": 0.0086,
      "step": 293
    },
    {
      "epoch": 28.682926829268293,
      "grad_norm": 0.49847692251205444,
      "learning_rate": 8.153846153846152e-05,
      "loss": 0.0135,
      "step": 294
    },
    {
      "epoch": 28.78048780487805,
      "grad_norm": 0.32717862725257874,
      "learning_rate": 8.076923076923076e-05,
      "loss": 0.0116,
      "step": 295
    },
    {
      "epoch": 28.878048780487806,
      "grad_norm": 0.2347848266363144,
      "learning_rate": 7.999999999999999e-05,
      "loss": 0.0118,
      "step": 296
    },
    {
      "epoch": 28.975609756097562,
      "grad_norm": 0.2876203656196594,
      "learning_rate": 7.923076923076922e-05,
      "loss": 0.0104,
      "step": 297
    },
    {
      "epoch": 29.073170731707318,
      "grad_norm": 0.39799925684928894,
      "learning_rate": 7.846153846153845e-05,
      "loss": 0.017,
      "step": 298
    },
    {
      "epoch": 29.170731707317074,
      "grad_norm": 0.36335423588752747,
      "learning_rate": 7.76923076923077e-05,
      "loss": 0.011,
      "step": 299
    },
    {
      "epoch": 29.26829268292683,
      "grad_norm": 0.11539699137210846,
      "learning_rate": 7.692307692307691e-05,
      "loss": 0.0089,
      "step": 300
    },
    {
      "epoch": 29.365853658536587,
      "grad_norm": 0.29130634665489197,
      "learning_rate": 7.615384615384614e-05,
      "loss": 0.0111,
      "step": 301
    },
    {
      "epoch": 29.463414634146343,
      "grad_norm": 0.17048969864845276,
      "learning_rate": 7.538461538461537e-05,
      "loss": 0.0093,
      "step": 302
    },
    {
      "epoch": 29.5609756097561,
      "grad_norm": 0.1706543117761612,
      "learning_rate": 7.461538461538462e-05,
      "loss": 0.0085,
      "step": 303
    },
    {
      "epoch": 29.658536585365855,
      "grad_norm": 0.18929798901081085,
      "learning_rate": 7.384615384615384e-05,
      "loss": 0.0081,
      "step": 304
    },
    {
      "epoch": 29.75609756097561,
      "grad_norm": 0.1968149095773697,
      "learning_rate": 7.307692307692307e-05,
      "loss": 0.0107,
      "step": 305
    },
    {
      "epoch": 29.853658536585368,
      "grad_norm": 0.15030011534690857,
      "learning_rate": 7.23076923076923e-05,
      "loss": 0.0098,
      "step": 306
    },
    {
      "epoch": 29.951219512195124,
      "grad_norm": 0.1661863625049591,
      "learning_rate": 7.153846153846153e-05,
      "loss": 0.0084,
      "step": 307
    },
    {
      "epoch": 30.048780487804876,
      "grad_norm": 0.26703640818595886,
      "learning_rate": 7.076923076923076e-05,
      "loss": 0.017,
      "step": 308
    },
    {
      "epoch": 30.146341463414632,
      "grad_norm": 0.10514672100543976,
      "learning_rate": 7e-05,
      "loss": 0.01,
      "step": 309
    },
    {
      "epoch": 30.24390243902439,
      "grad_norm": 0.10629284381866455,
      "learning_rate": 6.923076923076922e-05,
      "loss": 0.0099,
      "step": 310
    },
    {
      "epoch": 30.341463414634145,
      "grad_norm": 0.11021574586629868,
      "learning_rate": 6.846153846153845e-05,
      "loss": 0.0083,
      "step": 311
    },
    {
      "epoch": 30.4390243902439,
      "grad_norm": 0.12104418128728867,
      "learning_rate": 6.769230769230768e-05,
      "loss": 0.009,
      "step": 312
    },
    {
      "epoch": 30.536585365853657,
      "grad_norm": 0.21493114531040192,
      "learning_rate": 6.692307692307693e-05,
      "loss": 0.0092,
      "step": 313
    },
    {
      "epoch": 30.634146341463413,
      "grad_norm": 0.1298796385526657,
      "learning_rate": 6.615384615384614e-05,
      "loss": 0.0075,
      "step": 314
    },
    {
      "epoch": 30.73170731707317,
      "grad_norm": 0.11719249188899994,
      "learning_rate": 6.538461538461539e-05,
      "loss": 0.0109,
      "step": 315
    },
    {
      "epoch": 30.829268292682926,
      "grad_norm": 0.1024632602930069,
      "learning_rate": 6.461538461538462e-05,
      "loss": 0.0071,
      "step": 316
    },
    {
      "epoch": 30.926829268292682,
      "grad_norm": 0.06942161172628403,
      "learning_rate": 6.384615384615383e-05,
      "loss": 0.0089,
      "step": 317
    },
    {
      "epoch": 31.024390243902438,
      "grad_norm": 0.3130529820919037,
      "learning_rate": 6.307692307692308e-05,
      "loss": 0.0189,
      "step": 318
    },
    {
      "epoch": 31.121951219512194,
      "grad_norm": 0.09862227737903595,
      "learning_rate": 6.23076923076923e-05,
      "loss": 0.0084,
      "step": 319
    },
    {
      "epoch": 31.21951219512195,
      "grad_norm": 0.08867961913347244,
      "learning_rate": 6.153846153846154e-05,
      "loss": 0.0099,
      "step": 320
    },
    {
      "epoch": 31.317073170731707,
      "grad_norm": 0.12731844186782837,
      "learning_rate": 6.0769230769230765e-05,
      "loss": 0.0088,
      "step": 321
    },
    {
      "epoch": 31.414634146341463,
      "grad_norm": 0.04321788623929024,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 0.0066,
      "step": 322
    },
    {
      "epoch": 31.51219512195122,
      "grad_norm": 0.09758171439170837,
      "learning_rate": 5.9230769230769225e-05,
      "loss": 0.0094,
      "step": 323
    },
    {
      "epoch": 31.609756097560975,
      "grad_norm": 0.09418544173240662,
      "learning_rate": 5.8461538461538454e-05,
      "loss": 0.0086,
      "step": 324
    },
    {
      "epoch": 31.70731707317073,
      "grad_norm": 0.07174680382013321,
      "learning_rate": 5.769230769230769e-05,
      "loss": 0.0096,
      "step": 325
    },
    {
      "epoch": 31.804878048780488,
      "grad_norm": 0.12634803354740143,
      "learning_rate": 5.6923076923076914e-05,
      "loss": 0.0089,
      "step": 326
    },
    {
      "epoch": 31.902439024390244,
      "grad_norm": 0.037224430590867996,
      "learning_rate": 5.615384615384615e-05,
      "loss": 0.0073,
      "step": 327
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.25698646903038025,
      "learning_rate": 5.538461538461538e-05,
      "loss": 0.0169,
      "step": 328
    },
    {
      "epoch": 32.09756097560975,
      "grad_norm": 0.06757497787475586,
      "learning_rate": 5.461538461538461e-05,
      "loss": 0.0078,
      "step": 329
    },
    {
      "epoch": 32.19512195121951,
      "grad_norm": 0.05096253752708435,
      "learning_rate": 5.384615384615384e-05,
      "loss": 0.0102,
      "step": 330
    },
    {
      "epoch": 32.292682926829265,
      "grad_norm": 0.03569416701793671,
      "learning_rate": 5.3076923076923076e-05,
      "loss": 0.0083,
      "step": 331
    },
    {
      "epoch": 32.390243902439025,
      "grad_norm": 0.06499408930540085,
      "learning_rate": 5.23076923076923e-05,
      "loss": 0.0089,
      "step": 332
    },
    {
      "epoch": 32.48780487804878,
      "grad_norm": 0.06306137889623642,
      "learning_rate": 5.1538461538461536e-05,
      "loss": 0.0092,
      "step": 333
    },
    {
      "epoch": 32.58536585365854,
      "grad_norm": 0.1139460876584053,
      "learning_rate": 5.0769230769230766e-05,
      "loss": 0.0075,
      "step": 334
    },
    {
      "epoch": 32.68292682926829,
      "grad_norm": 0.09024975448846817,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 0.0088,
      "step": 335
    },
    {
      "epoch": 32.78048780487805,
      "grad_norm": 0.18196916580200195,
      "learning_rate": 4.9230769230769225e-05,
      "loss": 0.0083,
      "step": 336
    },
    {
      "epoch": 32.8780487804878,
      "grad_norm": 0.06331205368041992,
      "learning_rate": 4.846153846153846e-05,
      "loss": 0.0082,
      "step": 337
    },
    {
      "epoch": 32.97560975609756,
      "grad_norm": 0.09217624366283417,
      "learning_rate": 4.7692307692307685e-05,
      "loss": 0.0091,
      "step": 338
    },
    {
      "epoch": 33.073170731707314,
      "grad_norm": 0.10296977311372757,
      "learning_rate": 4.692307692307692e-05,
      "loss": 0.0175,
      "step": 339
    },
    {
      "epoch": 33.170731707317074,
      "grad_norm": 0.05239943414926529,
      "learning_rate": 4.615384615384615e-05,
      "loss": 0.0065,
      "step": 340
    },
    {
      "epoch": 33.26829268292683,
      "grad_norm": 0.10781168937683105,
      "learning_rate": 4.5384615384615374e-05,
      "loss": 0.011,
      "step": 341
    },
    {
      "epoch": 33.36585365853659,
      "grad_norm": 0.06009938567876816,
      "learning_rate": 4.461538461538461e-05,
      "loss": 0.0075,
      "step": 342
    },
    {
      "epoch": 33.46341463414634,
      "grad_norm": 0.039573755115270615,
      "learning_rate": 4.384615384615385e-05,
      "loss": 0.0074,
      "step": 343
    },
    {
      "epoch": 33.5609756097561,
      "grad_norm": 0.07493167370557785,
      "learning_rate": 4.307692307692307e-05,
      "loss": 0.0093,
      "step": 344
    },
    {
      "epoch": 33.65853658536585,
      "grad_norm": 0.09611959010362625,
      "learning_rate": 4.23076923076923e-05,
      "loss": 0.0096,
      "step": 345
    },
    {
      "epoch": 33.75609756097561,
      "grad_norm": 0.09170862287282944,
      "learning_rate": 4.153846153846154e-05,
      "loss": 0.0079,
      "step": 346
    },
    {
      "epoch": 33.853658536585364,
      "grad_norm": 0.09077279269695282,
      "learning_rate": 4.076923076923076e-05,
      "loss": 0.0092,
      "step": 347
    },
    {
      "epoch": 33.951219512195124,
      "grad_norm": 0.06963766366243362,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 0.0085,
      "step": 348
    },
    {
      "epoch": 34.048780487804876,
      "grad_norm": 0.07663600146770477,
      "learning_rate": 3.9230769230769226e-05,
      "loss": 0.0122,
      "step": 349
    },
    {
      "epoch": 34.146341463414636,
      "grad_norm": 0.07061886787414551,
      "learning_rate": 3.8461538461538456e-05,
      "loss": 0.0081,
      "step": 350
    },
    {
      "epoch": 34.24390243902439,
      "grad_norm": 0.09212130308151245,
      "learning_rate": 3.7692307692307686e-05,
      "loss": 0.0097,
      "step": 351
    },
    {
      "epoch": 34.34146341463415,
      "grad_norm": 0.0725470557808876,
      "learning_rate": 3.692307692307692e-05,
      "loss": 0.0094,
      "step": 352
    },
    {
      "epoch": 34.4390243902439,
      "grad_norm": 0.04200216010212898,
      "learning_rate": 3.615384615384615e-05,
      "loss": 0.0075,
      "step": 353
    },
    {
      "epoch": 34.53658536585366,
      "grad_norm": 0.07406559586524963,
      "learning_rate": 3.538461538461538e-05,
      "loss": 0.0101,
      "step": 354
    },
    {
      "epoch": 34.63414634146341,
      "grad_norm": 0.11629627645015717,
      "learning_rate": 3.461538461538461e-05,
      "loss": 0.0079,
      "step": 355
    },
    {
      "epoch": 34.73170731707317,
      "grad_norm": 0.05437459424138069,
      "learning_rate": 3.384615384615384e-05,
      "loss": 0.0086,
      "step": 356
    },
    {
      "epoch": 34.829268292682926,
      "grad_norm": 0.0656987801194191,
      "learning_rate": 3.307692307692307e-05,
      "loss": 0.0083,
      "step": 357
    },
    {
      "epoch": 34.926829268292686,
      "grad_norm": 0.08833327889442444,
      "learning_rate": 3.230769230769231e-05,
      "loss": 0.007,
      "step": 358
    },
    {
      "epoch": 35.02439024390244,
      "grad_norm": 0.13131287693977356,
      "learning_rate": 3.153846153846154e-05,
      "loss": 0.0122,
      "step": 359
    },
    {
      "epoch": 35.1219512195122,
      "grad_norm": 0.05630560964345932,
      "learning_rate": 3.076923076923077e-05,
      "loss": 0.0095,
      "step": 360
    },
    {
      "epoch": 35.21951219512195,
      "grad_norm": 0.04317708685994148,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 0.0092,
      "step": 361
    },
    {
      "epoch": 35.31707317073171,
      "grad_norm": 0.04411855712532997,
      "learning_rate": 2.9230769230769227e-05,
      "loss": 0.008,
      "step": 362
    },
    {
      "epoch": 35.41463414634146,
      "grad_norm": 0.09942380338907242,
      "learning_rate": 2.8461538461538457e-05,
      "loss": 0.0072,
      "step": 363
    },
    {
      "epoch": 35.51219512195122,
      "grad_norm": 0.0739799439907074,
      "learning_rate": 2.769230769230769e-05,
      "loss": 0.0079,
      "step": 364
    },
    {
      "epoch": 35.609756097560975,
      "grad_norm": 0.0718747079372406,
      "learning_rate": 2.692307692307692e-05,
      "loss": 0.0089,
      "step": 365
    },
    {
      "epoch": 35.707317073170735,
      "grad_norm": 0.06462643295526505,
      "learning_rate": 2.615384615384615e-05,
      "loss": 0.0079,
      "step": 366
    },
    {
      "epoch": 35.80487804878049,
      "grad_norm": 0.07135343551635742,
      "learning_rate": 2.5384615384615383e-05,
      "loss": 0.0082,
      "step": 367
    },
    {
      "epoch": 35.90243902439025,
      "grad_norm": 0.09431712329387665,
      "learning_rate": 2.4615384615384613e-05,
      "loss": 0.0083,
      "step": 368
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.17132970690727234,
      "learning_rate": 2.3846153846153843e-05,
      "loss": 0.0135,
      "step": 369
    },
    {
      "epoch": 36.09756097560975,
      "grad_norm": 0.06602884829044342,
      "learning_rate": 2.3076923076923076e-05,
      "loss": 0.007,
      "step": 370
    },
    {
      "epoch": 36.19512195121951,
      "grad_norm": 0.07771553844213486,
      "learning_rate": 2.2307692307692305e-05,
      "loss": 0.0096,
      "step": 371
    },
    {
      "epoch": 36.292682926829265,
      "grad_norm": 0.05588632449507713,
      "learning_rate": 2.1538461538461535e-05,
      "loss": 0.0086,
      "step": 372
    },
    {
      "epoch": 36.390243902439025,
      "grad_norm": 0.06398382037878036,
      "learning_rate": 2.076923076923077e-05,
      "loss": 0.0083,
      "step": 373
    },
    {
      "epoch": 36.48780487804878,
      "grad_norm": 0.06888148188591003,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 0.0097,
      "step": 374
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 0.06279905140399933,
      "learning_rate": 1.9230769230769228e-05,
      "loss": 0.0083,
      "step": 375
    },
    {
      "epoch": 36.68292682926829,
      "grad_norm": 0.050150930881500244,
      "learning_rate": 1.846153846153846e-05,
      "loss": 0.0071,
      "step": 376
    },
    {
      "epoch": 36.78048780487805,
      "grad_norm": 0.07842352241277695,
      "learning_rate": 1.769230769230769e-05,
      "loss": 0.0086,
      "step": 377
    },
    {
      "epoch": 36.8780487804878,
      "grad_norm": 0.09386775642633438,
      "learning_rate": 1.692307692307692e-05,
      "loss": 0.0072,
      "step": 378
    },
    {
      "epoch": 36.97560975609756,
      "grad_norm": 0.07680265605449677,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 0.0085,
      "step": 379
    },
    {
      "epoch": 37.073170731707314,
      "grad_norm": 0.06343361735343933,
      "learning_rate": 1.5384615384615384e-05,
      "loss": 0.0127,
      "step": 380
    },
    {
      "epoch": 37.170731707317074,
      "grad_norm": 0.08811239153146744,
      "learning_rate": 1.4615384615384614e-05,
      "loss": 0.0067,
      "step": 381
    },
    {
      "epoch": 37.26829268292683,
      "grad_norm": 0.06456034630537033,
      "learning_rate": 1.3846153846153845e-05,
      "loss": 0.0087,
      "step": 382
    },
    {
      "epoch": 37.36585365853659,
      "grad_norm": 0.06626971065998077,
      "learning_rate": 1.3076923076923075e-05,
      "loss": 0.0092,
      "step": 383
    },
    {
      "epoch": 37.46341463414634,
      "grad_norm": 0.0667530745267868,
      "learning_rate": 1.2307692307692306e-05,
      "loss": 0.0085,
      "step": 384
    },
    {
      "epoch": 37.5609756097561,
      "grad_norm": 0.07763713598251343,
      "learning_rate": 1.1538461538461538e-05,
      "loss": 0.008,
      "step": 385
    },
    {
      "epoch": 37.65853658536585,
      "grad_norm": 0.07022058963775635,
      "learning_rate": 1.0769230769230768e-05,
      "loss": 0.0074,
      "step": 386
    },
    {
      "epoch": 37.75609756097561,
      "grad_norm": 0.08103467524051666,
      "learning_rate": 9.999999999999999e-06,
      "loss": 0.0081,
      "step": 387
    },
    {
      "epoch": 37.853658536585364,
      "grad_norm": 0.10567379742860794,
      "learning_rate": 9.23076923076923e-06,
      "loss": 0.0092,
      "step": 388
    },
    {
      "epoch": 37.951219512195124,
      "grad_norm": 0.07351362705230713,
      "learning_rate": 8.46153846153846e-06,
      "loss": 0.0085,
      "step": 389
    },
    {
      "epoch": 38.048780487804876,
      "grad_norm": 0.07905704528093338,
      "learning_rate": 7.692307692307692e-06,
      "loss": 0.0127,
      "step": 390
    },
    {
      "epoch": 38.146341463414636,
      "grad_norm": 0.05416670814156532,
      "learning_rate": 6.9230769230769225e-06,
      "loss": 0.0071,
      "step": 391
    },
    {
      "epoch": 38.24390243902439,
      "grad_norm": 0.06443106383085251,
      "learning_rate": 6.153846153846153e-06,
      "loss": 0.0082,
      "step": 392
    },
    {
      "epoch": 38.34146341463415,
      "grad_norm": 0.0445035956799984,
      "learning_rate": 5.384615384615384e-06,
      "loss": 0.0069,
      "step": 393
    },
    {
      "epoch": 38.4390243902439,
      "grad_norm": 0.10065795481204987,
      "learning_rate": 4.615384615384615e-06,
      "loss": 0.0095,
      "step": 394
    },
    {
      "epoch": 38.53658536585366,
      "grad_norm": 0.07586763054132462,
      "learning_rate": 3.846153846153846e-06,
      "loss": 0.0086,
      "step": 395
    },
    {
      "epoch": 38.63414634146341,
      "grad_norm": 0.07953150570392609,
      "learning_rate": 3.0769230769230766e-06,
      "loss": 0.0099,
      "step": 396
    },
    {
      "epoch": 38.73170731707317,
      "grad_norm": 0.0622384287416935,
      "learning_rate": 2.3076923076923077e-06,
      "loss": 0.0057,
      "step": 397
    },
    {
      "epoch": 38.829268292682926,
      "grad_norm": 0.042760733515024185,
      "learning_rate": 1.5384615384615383e-06,
      "loss": 0.0098,
      "step": 398
    },
    {
      "epoch": 38.926829268292686,
      "grad_norm": 0.07468405365943909,
      "learning_rate": 7.692307692307691e-07,
      "loss": 0.0089,
      "step": 399
    },
    {
      "epoch": 39.02439024390244,
      "grad_norm": 0.14298740029335022,
      "learning_rate": 0.0,
      "loss": 0.0149,
      "step": 400
    }
  ],
  "logging_steps": 1,
  "max_steps": 400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.91998325686272e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
