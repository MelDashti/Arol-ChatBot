{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 39.02439024390244,
  "eval_steps": 500,
  "global_step": 400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 0.6353502869606018,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 1.0118,
      "step": 1
    },
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 0.5941437482833862,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 1.0018,
      "step": 2
    },
    {
      "epoch": 0.2926829268292683,
      "grad_norm": 0.5821704268455505,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.9763,
      "step": 3
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 0.5128695368766785,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.8878,
      "step": 4
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 0.5148411989212036,
      "learning_rate": 0.00015,
      "loss": 0.8245,
      "step": 5
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 0.5369587540626526,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.7645,
      "step": 6
    },
    {
      "epoch": 0.6829268292682927,
      "grad_norm": 0.552635133266449,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.6702,
      "step": 7
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 0.6458350419998169,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.5922,
      "step": 8
    },
    {
      "epoch": 0.8780487804878049,
      "grad_norm": 0.6846032738685608,
      "learning_rate": 0.00027,
      "loss": 0.5405,
      "step": 9
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 0.6482223272323608,
      "learning_rate": 0.0003,
      "loss": 0.4794,
      "step": 10
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 1.0305509567260742,
      "learning_rate": 0.0002992307692307692,
      "loss": 0.712,
      "step": 11
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 0.5483742356300354,
      "learning_rate": 0.00029846153846153846,
      "loss": 0.3844,
      "step": 12
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 0.8492389917373657,
      "learning_rate": 0.0002976923076923077,
      "loss": 0.3678,
      "step": 13
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 0.5498288869857788,
      "learning_rate": 0.0002969230769230769,
      "loss": 0.3265,
      "step": 14
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 0.44567984342575073,
      "learning_rate": 0.00029615384615384616,
      "loss": 0.3112,
      "step": 15
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 0.36951032280921936,
      "learning_rate": 0.0002953846153846154,
      "loss": 0.3134,
      "step": 16
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 0.3567514717578888,
      "learning_rate": 0.0002946153846153846,
      "loss": 0.2854,
      "step": 17
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 0.3671552836894989,
      "learning_rate": 0.0002938461538461538,
      "loss": 0.2822,
      "step": 18
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 0.46131065487861633,
      "learning_rate": 0.00029307692307692303,
      "loss": 0.2727,
      "step": 19
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 0.3893752992153168,
      "learning_rate": 0.0002923076923076923,
      "loss": 0.2853,
      "step": 20
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 0.7372502684593201,
      "learning_rate": 0.0002915384615384615,
      "loss": 0.422,
      "step": 21
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 0.35274094343185425,
      "learning_rate": 0.00029076923076923073,
      "loss": 0.2303,
      "step": 22
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 0.403041273355484,
      "learning_rate": 0.00029,
      "loss": 0.2419,
      "step": 23
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 0.39113858342170715,
      "learning_rate": 0.0002892307692307692,
      "loss": 0.2186,
      "step": 24
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 0.3494352400302887,
      "learning_rate": 0.00028846153846153843,
      "loss": 0.2348,
      "step": 25
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 0.37897419929504395,
      "learning_rate": 0.00028769230769230765,
      "loss": 0.238,
      "step": 26
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 0.3391599655151367,
      "learning_rate": 0.0002869230769230769,
      "loss": 0.201,
      "step": 27
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 0.3389294147491455,
      "learning_rate": 0.00028615384615384614,
      "loss": 0.2157,
      "step": 28
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 0.30162474513053894,
      "learning_rate": 0.00028538461538461535,
      "loss": 0.2092,
      "step": 29
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 0.3452723026275635,
      "learning_rate": 0.00028461538461538457,
      "loss": 0.2073,
      "step": 30
    },
    {
      "epoch": 3.024390243902439,
      "grad_norm": 0.591508150100708,
      "learning_rate": 0.0002838461538461538,
      "loss": 0.324,
      "step": 31
    },
    {
      "epoch": 3.1219512195121952,
      "grad_norm": 0.3128778040409088,
      "learning_rate": 0.00028307692307692306,
      "loss": 0.1782,
      "step": 32
    },
    {
      "epoch": 3.2195121951219514,
      "grad_norm": 0.2809571623802185,
      "learning_rate": 0.00028230769230769227,
      "loss": 0.1607,
      "step": 33
    },
    {
      "epoch": 3.317073170731707,
      "grad_norm": 0.29991310834884644,
      "learning_rate": 0.0002815384615384615,
      "loss": 0.1636,
      "step": 34
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 0.304531991481781,
      "learning_rate": 0.00028076923076923076,
      "loss": 0.1615,
      "step": 35
    },
    {
      "epoch": 3.5121951219512195,
      "grad_norm": 0.3198190927505493,
      "learning_rate": 0.00028,
      "loss": 0.1767,
      "step": 36
    },
    {
      "epoch": 3.6097560975609757,
      "grad_norm": 0.3873138725757599,
      "learning_rate": 0.0002792307692307692,
      "loss": 0.1832,
      "step": 37
    },
    {
      "epoch": 3.7073170731707314,
      "grad_norm": 0.35544106364250183,
      "learning_rate": 0.00027846153846153846,
      "loss": 0.1746,
      "step": 38
    },
    {
      "epoch": 3.8048780487804876,
      "grad_norm": 0.30960142612457275,
      "learning_rate": 0.0002776923076923077,
      "loss": 0.1713,
      "step": 39
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 0.28913572430610657,
      "learning_rate": 0.0002769230769230769,
      "loss": 0.1561,
      "step": 40
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.7009132504463196,
      "learning_rate": 0.0002761538461538461,
      "loss": 0.2989,
      "step": 41
    },
    {
      "epoch": 4.097560975609756,
      "grad_norm": 0.25641682744026184,
      "learning_rate": 0.00027538461538461533,
      "loss": 0.1368,
      "step": 42
    },
    {
      "epoch": 4.195121951219512,
      "grad_norm": 0.27338966727256775,
      "learning_rate": 0.0002746153846153846,
      "loss": 0.1476,
      "step": 43
    },
    {
      "epoch": 4.2926829268292686,
      "grad_norm": 0.27993515133857727,
      "learning_rate": 0.0002738461538461538,
      "loss": 0.1368,
      "step": 44
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 0.2848049998283386,
      "learning_rate": 0.00027307692307692303,
      "loss": 0.1321,
      "step": 45
    },
    {
      "epoch": 4.487804878048781,
      "grad_norm": 0.30399972200393677,
      "learning_rate": 0.0002723076923076923,
      "loss": 0.1353,
      "step": 46
    },
    {
      "epoch": 4.585365853658536,
      "grad_norm": 0.3482186198234558,
      "learning_rate": 0.0002715384615384615,
      "loss": 0.1284,
      "step": 47
    },
    {
      "epoch": 4.682926829268292,
      "grad_norm": 0.30648088455200195,
      "learning_rate": 0.00027076923076923073,
      "loss": 0.1279,
      "step": 48
    },
    {
      "epoch": 4.780487804878049,
      "grad_norm": 0.3264484107494354,
      "learning_rate": 0.00027,
      "loss": 0.1333,
      "step": 49
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 0.3278639018535614,
      "learning_rate": 0.0002692307692307692,
      "loss": 0.1336,
      "step": 50
    },
    {
      "epoch": 4.975609756097561,
      "grad_norm": 0.31977352499961853,
      "learning_rate": 0.00026846153846153844,
      "loss": 0.129,
      "step": 51
    },
    {
      "epoch": 5.073170731707317,
      "grad_norm": 0.6109018325805664,
      "learning_rate": 0.0002676923076923077,
      "loss": 0.2149,
      "step": 52
    },
    {
      "epoch": 5.170731707317073,
      "grad_norm": 0.26735684275627136,
      "learning_rate": 0.00026692307692307687,
      "loss": 0.1085,
      "step": 53
    },
    {
      "epoch": 5.2682926829268295,
      "grad_norm": 0.2594430148601532,
      "learning_rate": 0.00026615384615384614,
      "loss": 0.1033,
      "step": 54
    },
    {
      "epoch": 5.365853658536586,
      "grad_norm": 0.28008466958999634,
      "learning_rate": 0.00026538461538461536,
      "loss": 0.0998,
      "step": 55
    },
    {
      "epoch": 5.463414634146342,
      "grad_norm": 0.38349661231040955,
      "learning_rate": 0.00026461538461538457,
      "loss": 0.1129,
      "step": 56
    },
    {
      "epoch": 5.560975609756097,
      "grad_norm": 0.36401641368865967,
      "learning_rate": 0.00026384615384615384,
      "loss": 0.1155,
      "step": 57
    },
    {
      "epoch": 5.658536585365853,
      "grad_norm": 0.3436408042907715,
      "learning_rate": 0.00026307692307692306,
      "loss": 0.1193,
      "step": 58
    },
    {
      "epoch": 5.7560975609756095,
      "grad_norm": 0.3223567605018616,
      "learning_rate": 0.0002623076923076923,
      "loss": 0.108,
      "step": 59
    },
    {
      "epoch": 5.853658536585366,
      "grad_norm": 0.2984398603439331,
      "learning_rate": 0.00026153846153846154,
      "loss": 0.117,
      "step": 60
    },
    {
      "epoch": 5.951219512195122,
      "grad_norm": 0.24521450698375702,
      "learning_rate": 0.00026076923076923076,
      "loss": 0.0946,
      "step": 61
    },
    {
      "epoch": 6.048780487804878,
      "grad_norm": 0.5984331369400024,
      "learning_rate": 0.00026,
      "loss": 0.1906,
      "step": 62
    },
    {
      "epoch": 6.146341463414634,
      "grad_norm": 0.25824272632598877,
      "learning_rate": 0.0002592307692307692,
      "loss": 0.0966,
      "step": 63
    },
    {
      "epoch": 6.2439024390243905,
      "grad_norm": 0.23507468402385712,
      "learning_rate": 0.00025846153846153846,
      "loss": 0.0872,
      "step": 64
    },
    {
      "epoch": 6.341463414634147,
      "grad_norm": 0.2611607611179352,
      "learning_rate": 0.0002576923076923077,
      "loss": 0.0888,
      "step": 65
    },
    {
      "epoch": 6.439024390243903,
      "grad_norm": 0.315250039100647,
      "learning_rate": 0.0002569230769230769,
      "loss": 0.0852,
      "step": 66
    },
    {
      "epoch": 6.536585365853659,
      "grad_norm": 0.4066654443740845,
      "learning_rate": 0.0002561538461538461,
      "loss": 0.0963,
      "step": 67
    },
    {
      "epoch": 6.634146341463414,
      "grad_norm": 0.3436160683631897,
      "learning_rate": 0.00025538461538461533,
      "loss": 0.0949,
      "step": 68
    },
    {
      "epoch": 6.7317073170731705,
      "grad_norm": 0.2727121412754059,
      "learning_rate": 0.0002546153846153846,
      "loss": 0.0808,
      "step": 69
    },
    {
      "epoch": 6.829268292682927,
      "grad_norm": 0.29347240924835205,
      "learning_rate": 0.0002538461538461538,
      "loss": 0.0879,
      "step": 70
    },
    {
      "epoch": 6.926829268292683,
      "grad_norm": 0.3347218334674835,
      "learning_rate": 0.00025307692307692303,
      "loss": 0.0887,
      "step": 71
    },
    {
      "epoch": 7.024390243902439,
      "grad_norm": 0.6849841475486755,
      "learning_rate": 0.0002523076923076923,
      "loss": 0.1704,
      "step": 72
    },
    {
      "epoch": 7.121951219512195,
      "grad_norm": 0.214583158493042,
      "learning_rate": 0.0002515384615384615,
      "loss": 0.0642,
      "step": 73
    },
    {
      "epoch": 7.219512195121951,
      "grad_norm": 0.2922746539115906,
      "learning_rate": 0.00025076923076923073,
      "loss": 0.0841,
      "step": 74
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 0.2801038920879364,
      "learning_rate": 0.00025,
      "loss": 0.0738,
      "step": 75
    },
    {
      "epoch": 7.414634146341464,
      "grad_norm": 0.31025126576423645,
      "learning_rate": 0.0002492307692307692,
      "loss": 0.0761,
      "step": 76
    },
    {
      "epoch": 7.512195121951219,
      "grad_norm": 0.35864484310150146,
      "learning_rate": 0.00024846153846153844,
      "loss": 0.0825,
      "step": 77
    },
    {
      "epoch": 7.609756097560975,
      "grad_norm": 0.3555569648742676,
      "learning_rate": 0.00024769230769230765,
      "loss": 0.0785,
      "step": 78
    },
    {
      "epoch": 7.7073170731707314,
      "grad_norm": 0.31849542260169983,
      "learning_rate": 0.00024692307692307687,
      "loss": 0.0749,
      "step": 79
    },
    {
      "epoch": 7.804878048780488,
      "grad_norm": 0.3072981536388397,
      "learning_rate": 0.00024615384615384614,
      "loss": 0.0836,
      "step": 80
    },
    {
      "epoch": 7.902439024390244,
      "grad_norm": 0.3085952401161194,
      "learning_rate": 0.00024538461538461536,
      "loss": 0.0788,
      "step": 81
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.49034973978996277,
      "learning_rate": 0.0002446153846153846,
      "loss": 0.1193,
      "step": 82
    },
    {
      "epoch": 8.097560975609756,
      "grad_norm": 0.22617408633232117,
      "learning_rate": 0.00024384615384615382,
      "loss": 0.0605,
      "step": 83
    },
    {
      "epoch": 8.195121951219512,
      "grad_norm": 0.2426307052373886,
      "learning_rate": 0.00024307692307692306,
      "loss": 0.0618,
      "step": 84
    },
    {
      "epoch": 8.292682926829269,
      "grad_norm": 0.23189613223075867,
      "learning_rate": 0.0002423076923076923,
      "loss": 0.0616,
      "step": 85
    },
    {
      "epoch": 8.390243902439025,
      "grad_norm": 0.2746385931968689,
      "learning_rate": 0.00024153846153846152,
      "loss": 0.0625,
      "step": 86
    },
    {
      "epoch": 8.487804878048781,
      "grad_norm": 0.305817574262619,
      "learning_rate": 0.00024076923076923076,
      "loss": 0.0649,
      "step": 87
    },
    {
      "epoch": 8.585365853658537,
      "grad_norm": 0.3353534936904907,
      "learning_rate": 0.00023999999999999998,
      "loss": 0.0637,
      "step": 88
    },
    {
      "epoch": 8.682926829268293,
      "grad_norm": 0.33409151434898376,
      "learning_rate": 0.00023923076923076922,
      "loss": 0.0698,
      "step": 89
    },
    {
      "epoch": 8.78048780487805,
      "grad_norm": 0.2801624536514282,
      "learning_rate": 0.00023846153846153844,
      "loss": 0.0655,
      "step": 90
    },
    {
      "epoch": 8.878048780487806,
      "grad_norm": 0.26493823528289795,
      "learning_rate": 0.00023769230769230765,
      "loss": 0.0648,
      "step": 91
    },
    {
      "epoch": 8.975609756097562,
      "grad_norm": 0.3115381598472595,
      "learning_rate": 0.0002369230769230769,
      "loss": 0.0682,
      "step": 92
    },
    {
      "epoch": 9.073170731707316,
      "grad_norm": 0.5669675469398499,
      "learning_rate": 0.00023615384615384611,
      "loss": 0.108,
      "step": 93
    },
    {
      "epoch": 9.170731707317072,
      "grad_norm": 0.21880033612251282,
      "learning_rate": 0.00023538461538461536,
      "loss": 0.0471,
      "step": 94
    },
    {
      "epoch": 9.268292682926829,
      "grad_norm": 0.2513141930103302,
      "learning_rate": 0.0002346153846153846,
      "loss": 0.0515,
      "step": 95
    },
    {
      "epoch": 9.365853658536585,
      "grad_norm": 0.3145562708377838,
      "learning_rate": 0.00023384615384615382,
      "loss": 0.0539,
      "step": 96
    },
    {
      "epoch": 9.463414634146341,
      "grad_norm": 0.3652181029319763,
      "learning_rate": 0.00023307692307692306,
      "loss": 0.0543,
      "step": 97
    },
    {
      "epoch": 9.560975609756097,
      "grad_norm": 0.33765077590942383,
      "learning_rate": 0.0002323076923076923,
      "loss": 0.053,
      "step": 98
    },
    {
      "epoch": 9.658536585365853,
      "grad_norm": 0.34386759996414185,
      "learning_rate": 0.00023153846153846152,
      "loss": 0.0624,
      "step": 99
    },
    {
      "epoch": 9.75609756097561,
      "grad_norm": 0.33486461639404297,
      "learning_rate": 0.00023076923076923076,
      "loss": 0.0589,
      "step": 100
    },
    {
      "epoch": 9.853658536585366,
      "grad_norm": 0.2768108546733856,
      "learning_rate": 0.00023,
      "loss": 0.0532,
      "step": 101
    },
    {
      "epoch": 9.951219512195122,
      "grad_norm": 0.25021499395370483,
      "learning_rate": 0.0002292307692307692,
      "loss": 0.0522,
      "step": 102
    },
    {
      "epoch": 10.048780487804878,
      "grad_norm": 0.5676900148391724,
      "learning_rate": 0.00022846153846153844,
      "loss": 0.087,
      "step": 103
    },
    {
      "epoch": 10.146341463414634,
      "grad_norm": 0.2722342312335968,
      "learning_rate": 0.00022769230769230766,
      "loss": 0.0472,
      "step": 104
    },
    {
      "epoch": 10.24390243902439,
      "grad_norm": 0.24043311178684235,
      "learning_rate": 0.0002269230769230769,
      "loss": 0.046,
      "step": 105
    },
    {
      "epoch": 10.341463414634147,
      "grad_norm": 0.23106245696544647,
      "learning_rate": 0.00022615384615384614,
      "loss": 0.0408,
      "step": 106
    },
    {
      "epoch": 10.439024390243903,
      "grad_norm": 0.26012101769447327,
      "learning_rate": 0.00022538461538461536,
      "loss": 0.0462,
      "step": 107
    },
    {
      "epoch": 10.536585365853659,
      "grad_norm": 0.5341811776161194,
      "learning_rate": 0.0002246153846153846,
      "loss": 0.0598,
      "step": 108
    },
    {
      "epoch": 10.634146341463415,
      "grad_norm": 0.28770211338996887,
      "learning_rate": 0.00022384615384615382,
      "loss": 0.0456,
      "step": 109
    },
    {
      "epoch": 10.731707317073171,
      "grad_norm": 0.29063019156455994,
      "learning_rate": 0.00022307692307692306,
      "loss": 0.0488,
      "step": 110
    },
    {
      "epoch": 10.829268292682928,
      "grad_norm": 0.2827359735965729,
      "learning_rate": 0.0002223076923076923,
      "loss": 0.0493,
      "step": 111
    },
    {
      "epoch": 10.926829268292684,
      "grad_norm": 0.2397039383649826,
      "learning_rate": 0.00022153846153846152,
      "loss": 0.0482,
      "step": 112
    },
    {
      "epoch": 11.024390243902438,
      "grad_norm": 0.555141806602478,
      "learning_rate": 0.00022076923076923076,
      "loss": 0.0731,
      "step": 113
    },
    {
      "epoch": 11.121951219512194,
      "grad_norm": 0.20561666786670685,
      "learning_rate": 0.00021999999999999995,
      "loss": 0.0386,
      "step": 114
    },
    {
      "epoch": 11.21951219512195,
      "grad_norm": 0.1889486312866211,
      "learning_rate": 0.0002192307692307692,
      "loss": 0.0365,
      "step": 115
    },
    {
      "epoch": 11.317073170731707,
      "grad_norm": 0.22901903092861176,
      "learning_rate": 0.00021846153846153844,
      "loss": 0.041,
      "step": 116
    },
    {
      "epoch": 11.414634146341463,
      "grad_norm": 0.2565072476863861,
      "learning_rate": 0.00021769230769230766,
      "loss": 0.0369,
      "step": 117
    },
    {
      "epoch": 11.512195121951219,
      "grad_norm": 0.3431008458137512,
      "learning_rate": 0.0002169230769230769,
      "loss": 0.0418,
      "step": 118
    },
    {
      "epoch": 11.609756097560975,
      "grad_norm": 0.366828978061676,
      "learning_rate": 0.00021615384615384614,
      "loss": 0.0426,
      "step": 119
    },
    {
      "epoch": 11.707317073170731,
      "grad_norm": 0.3194809556007385,
      "learning_rate": 0.00021538461538461536,
      "loss": 0.043,
      "step": 120
    },
    {
      "epoch": 11.804878048780488,
      "grad_norm": 0.353086918592453,
      "learning_rate": 0.0002146153846153846,
      "loss": 0.0411,
      "step": 121
    },
    {
      "epoch": 11.902439024390244,
      "grad_norm": 0.3044506907463074,
      "learning_rate": 0.00021384615384615385,
      "loss": 0.0413,
      "step": 122
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.5246549248695374,
      "learning_rate": 0.00021307692307692306,
      "loss": 0.0667,
      "step": 123
    },
    {
      "epoch": 12.097560975609756,
      "grad_norm": 0.23964643478393555,
      "learning_rate": 0.0002123076923076923,
      "loss": 0.0321,
      "step": 124
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 0.20310494303703308,
      "learning_rate": 0.00021153846153846152,
      "loss": 0.0291,
      "step": 125
    },
    {
      "epoch": 12.292682926829269,
      "grad_norm": 0.31053638458251953,
      "learning_rate": 0.00021076923076923074,
      "loss": 0.0349,
      "step": 126
    },
    {
      "epoch": 12.390243902439025,
      "grad_norm": 0.27594342827796936,
      "learning_rate": 0.00020999999999999998,
      "loss": 0.0327,
      "step": 127
    },
    {
      "epoch": 12.487804878048781,
      "grad_norm": 0.2763948142528534,
      "learning_rate": 0.0002092307692307692,
      "loss": 0.0332,
      "step": 128
    },
    {
      "epoch": 12.585365853658537,
      "grad_norm": 0.29052606225013733,
      "learning_rate": 0.00020846153846153844,
      "loss": 0.0361,
      "step": 129
    },
    {
      "epoch": 12.682926829268293,
      "grad_norm": 0.29001569747924805,
      "learning_rate": 0.00020769230769230766,
      "loss": 0.0347,
      "step": 130
    },
    {
      "epoch": 12.78048780487805,
      "grad_norm": 0.2870313823223114,
      "learning_rate": 0.0002069230769230769,
      "loss": 0.0317,
      "step": 131
    },
    {
      "epoch": 12.878048780487806,
      "grad_norm": 0.32663607597351074,
      "learning_rate": 0.00020615384615384614,
      "loss": 0.0361,
      "step": 132
    },
    {
      "epoch": 12.975609756097562,
      "grad_norm": 0.2936077117919922,
      "learning_rate": 0.00020538461538461536,
      "loss": 0.0365,
      "step": 133
    },
    {
      "epoch": 13.073170731707316,
      "grad_norm": 0.6359151005744934,
      "learning_rate": 0.0002046153846153846,
      "loss": 0.0616,
      "step": 134
    },
    {
      "epoch": 13.170731707317072,
      "grad_norm": 0.17457644641399384,
      "learning_rate": 0.00020384615384615385,
      "loss": 0.0243,
      "step": 135
    },
    {
      "epoch": 13.268292682926829,
      "grad_norm": 0.2318376898765564,
      "learning_rate": 0.00020307692307692306,
      "loss": 0.0284,
      "step": 136
    },
    {
      "epoch": 13.365853658536585,
      "grad_norm": 0.3114320635795593,
      "learning_rate": 0.0002023076923076923,
      "loss": 0.0297,
      "step": 137
    },
    {
      "epoch": 13.463414634146341,
      "grad_norm": 0.29662105441093445,
      "learning_rate": 0.0002015384615384615,
      "loss": 0.0324,
      "step": 138
    },
    {
      "epoch": 13.560975609756097,
      "grad_norm": 0.2891201674938202,
      "learning_rate": 0.00020076923076923074,
      "loss": 0.0314,
      "step": 139
    },
    {
      "epoch": 13.658536585365853,
      "grad_norm": 0.34458407759666443,
      "learning_rate": 0.00019999999999999998,
      "loss": 0.0274,
      "step": 140
    },
    {
      "epoch": 13.75609756097561,
      "grad_norm": 0.2525627017021179,
      "learning_rate": 0.0001992307692307692,
      "loss": 0.0254,
      "step": 141
    },
    {
      "epoch": 13.853658536585366,
      "grad_norm": 0.26342180371284485,
      "learning_rate": 0.00019846153846153844,
      "loss": 0.0346,
      "step": 142
    },
    {
      "epoch": 13.951219512195122,
      "grad_norm": 0.293874591588974,
      "learning_rate": 0.00019769230769230769,
      "loss": 0.0323,
      "step": 143
    },
    {
      "epoch": 14.048780487804878,
      "grad_norm": 0.4042693078517914,
      "learning_rate": 0.0001969230769230769,
      "loss": 0.0465,
      "step": 144
    },
    {
      "epoch": 14.146341463414634,
      "grad_norm": 0.18206994235515594,
      "learning_rate": 0.00019615384615384615,
      "loss": 0.0219,
      "step": 145
    },
    {
      "epoch": 14.24390243902439,
      "grad_norm": 0.20323805510997772,
      "learning_rate": 0.00019538461538461536,
      "loss": 0.0196,
      "step": 146
    },
    {
      "epoch": 14.341463414634147,
      "grad_norm": 0.21590285003185272,
      "learning_rate": 0.0001946153846153846,
      "loss": 0.0216,
      "step": 147
    },
    {
      "epoch": 14.439024390243903,
      "grad_norm": 0.29992419481277466,
      "learning_rate": 0.00019384615384615385,
      "loss": 0.0245,
      "step": 148
    },
    {
      "epoch": 14.536585365853659,
      "grad_norm": 0.2733679711818695,
      "learning_rate": 0.00019307692307692306,
      "loss": 0.0206,
      "step": 149
    },
    {
      "epoch": 14.634146341463415,
      "grad_norm": 0.2472020387649536,
      "learning_rate": 0.0001923076923076923,
      "loss": 0.0214,
      "step": 150
    },
    {
      "epoch": 14.731707317073171,
      "grad_norm": 0.2162601202726364,
      "learning_rate": 0.0001915384615384615,
      "loss": 0.0205,
      "step": 151
    },
    {
      "epoch": 14.829268292682928,
      "grad_norm": 0.24090632796287537,
      "learning_rate": 0.00019076923076923074,
      "loss": 0.0214,
      "step": 152
    },
    {
      "epoch": 14.926829268292684,
      "grad_norm": 0.21387934684753418,
      "learning_rate": 0.00018999999999999998,
      "loss": 0.0213,
      "step": 153
    },
    {
      "epoch": 15.024390243902438,
      "grad_norm": 0.4023183286190033,
      "learning_rate": 0.0001892307692307692,
      "loss": 0.0344,
      "step": 154
    },
    {
      "epoch": 15.121951219512194,
      "grad_norm": 0.15913504362106323,
      "learning_rate": 0.00018846153846153844,
      "loss": 0.0125,
      "step": 155
    },
    {
      "epoch": 15.21951219512195,
      "grad_norm": 0.22729092836380005,
      "learning_rate": 0.00018769230769230769,
      "loss": 0.0143,
      "step": 156
    },
    {
      "epoch": 15.317073170731707,
      "grad_norm": 0.24796216189861298,
      "learning_rate": 0.0001869230769230769,
      "loss": 0.0167,
      "step": 157
    },
    {
      "epoch": 15.414634146341463,
      "grad_norm": 0.3197448253631592,
      "learning_rate": 0.00018615384615384615,
      "loss": 0.0163,
      "step": 158
    },
    {
      "epoch": 15.512195121951219,
      "grad_norm": 0.23498129844665527,
      "learning_rate": 0.0001853846153846154,
      "loss": 0.0141,
      "step": 159
    },
    {
      "epoch": 15.609756097560975,
      "grad_norm": 0.2597236633300781,
      "learning_rate": 0.0001846153846153846,
      "loss": 0.0149,
      "step": 160
    },
    {
      "epoch": 15.707317073170731,
      "grad_norm": 0.3980741798877716,
      "learning_rate": 0.00018384615384615385,
      "loss": 0.0224,
      "step": 161
    },
    {
      "epoch": 15.804878048780488,
      "grad_norm": 0.22076085209846497,
      "learning_rate": 0.00018307692307692307,
      "loss": 0.0161,
      "step": 162
    },
    {
      "epoch": 15.902439024390244,
      "grad_norm": 0.2351503074169159,
      "learning_rate": 0.00018230769230769228,
      "loss": 0.0162,
      "step": 163
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.3643546402454376,
      "learning_rate": 0.0001815384615384615,
      "loss": 0.0245,
      "step": 164
    },
    {
      "epoch": 16.097560975609756,
      "grad_norm": 0.17572416365146637,
      "learning_rate": 0.00018076923076923074,
      "loss": 0.0123,
      "step": 165
    },
    {
      "epoch": 16.195121951219512,
      "grad_norm": 0.21214132010936737,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.0145,
      "step": 166
    },
    {
      "epoch": 16.29268292682927,
      "grad_norm": 0.19454330205917358,
      "learning_rate": 0.0001792307692307692,
      "loss": 0.0136,
      "step": 167
    },
    {
      "epoch": 16.390243902439025,
      "grad_norm": 0.22544650733470917,
      "learning_rate": 0.00017846153846153844,
      "loss": 0.0132,
      "step": 168
    },
    {
      "epoch": 16.48780487804878,
      "grad_norm": 0.2061883807182312,
      "learning_rate": 0.0001776923076923077,
      "loss": 0.0113,
      "step": 169
    },
    {
      "epoch": 16.585365853658537,
      "grad_norm": 0.2980007231235504,
      "learning_rate": 0.0001769230769230769,
      "loss": 0.0134,
      "step": 170
    },
    {
      "epoch": 16.682926829268293,
      "grad_norm": 0.24052447080612183,
      "learning_rate": 0.00017615384615384615,
      "loss": 0.0118,
      "step": 171
    },
    {
      "epoch": 16.78048780487805,
      "grad_norm": 0.21132244169712067,
      "learning_rate": 0.0001753846153846154,
      "loss": 0.011,
      "step": 172
    },
    {
      "epoch": 16.878048780487806,
      "grad_norm": 0.3415759801864624,
      "learning_rate": 0.0001746153846153846,
      "loss": 0.0112,
      "step": 173
    },
    {
      "epoch": 16.975609756097562,
      "grad_norm": 0.2865447998046875,
      "learning_rate": 0.00017384615384615385,
      "loss": 0.0132,
      "step": 174
    },
    {
      "epoch": 17.073170731707318,
      "grad_norm": 0.7709035277366638,
      "learning_rate": 0.00017307692307692304,
      "loss": 0.0265,
      "step": 175
    },
    {
      "epoch": 17.170731707317074,
      "grad_norm": 0.15403997898101807,
      "learning_rate": 0.00017230769230769228,
      "loss": 0.0089,
      "step": 176
    },
    {
      "epoch": 17.26829268292683,
      "grad_norm": 0.2260921597480774,
      "learning_rate": 0.00017153846153846153,
      "loss": 0.0102,
      "step": 177
    },
    {
      "epoch": 17.365853658536587,
      "grad_norm": 0.2166260927915573,
      "learning_rate": 0.00017076923076923074,
      "loss": 0.0099,
      "step": 178
    },
    {
      "epoch": 17.463414634146343,
      "grad_norm": 0.2842712700366974,
      "learning_rate": 0.00016999999999999999,
      "loss": 0.0125,
      "step": 179
    },
    {
      "epoch": 17.5609756097561,
      "grad_norm": 0.2426612824201584,
      "learning_rate": 0.0001692307692307692,
      "loss": 0.0141,
      "step": 180
    },
    {
      "epoch": 17.658536585365855,
      "grad_norm": 0.30423033237457275,
      "learning_rate": 0.00016846153846153844,
      "loss": 0.0135,
      "step": 181
    },
    {
      "epoch": 17.75609756097561,
      "grad_norm": 0.2499975711107254,
      "learning_rate": 0.0001676923076923077,
      "loss": 0.0137,
      "step": 182
    },
    {
      "epoch": 17.853658536585368,
      "grad_norm": 0.21167519688606262,
      "learning_rate": 0.0001669230769230769,
      "loss": 0.0139,
      "step": 183
    },
    {
      "epoch": 17.951219512195124,
      "grad_norm": 0.26472729444503784,
      "learning_rate": 0.00016615384615384615,
      "loss": 0.0139,
      "step": 184
    },
    {
      "epoch": 18.048780487804876,
      "grad_norm": 0.4460192918777466,
      "learning_rate": 0.0001653846153846154,
      "loss": 0.0277,
      "step": 185
    },
    {
      "epoch": 18.146341463414632,
      "grad_norm": 0.1945877969264984,
      "learning_rate": 0.0001646153846153846,
      "loss": 0.0099,
      "step": 186
    },
    {
      "epoch": 18.24390243902439,
      "grad_norm": 0.16296257078647614,
      "learning_rate": 0.00016384615384615382,
      "loss": 0.0099,
      "step": 187
    },
    {
      "epoch": 18.341463414634145,
      "grad_norm": 0.1489580124616623,
      "learning_rate": 0.00016307692307692304,
      "loss": 0.0092,
      "step": 188
    },
    {
      "epoch": 18.4390243902439,
      "grad_norm": 0.17962147295475006,
      "learning_rate": 0.00016230769230769228,
      "loss": 0.0104,
      "step": 189
    },
    {
      "epoch": 18.536585365853657,
      "grad_norm": 0.2656932771205902,
      "learning_rate": 0.00016153846153846153,
      "loss": 0.0098,
      "step": 190
    },
    {
      "epoch": 18.634146341463413,
      "grad_norm": 0.18199315667152405,
      "learning_rate": 0.00016076923076923074,
      "loss": 0.0085,
      "step": 191
    },
    {
      "epoch": 18.73170731707317,
      "grad_norm": 0.18228183686733246,
      "learning_rate": 0.00015999999999999999,
      "loss": 0.0105,
      "step": 192
    },
    {
      "epoch": 18.829268292682926,
      "grad_norm": 0.1644064038991928,
      "learning_rate": 0.00015923076923076923,
      "loss": 0.0063,
      "step": 193
    },
    {
      "epoch": 18.926829268292682,
      "grad_norm": 0.1509159952402115,
      "learning_rate": 0.00015846153846153845,
      "loss": 0.008,
      "step": 194
    },
    {
      "epoch": 19.024390243902438,
      "grad_norm": 0.2707347869873047,
      "learning_rate": 0.0001576923076923077,
      "loss": 0.0124,
      "step": 195
    },
    {
      "epoch": 19.121951219512194,
      "grad_norm": 0.19941405951976776,
      "learning_rate": 0.0001569230769230769,
      "loss": 0.0074,
      "step": 196
    },
    {
      "epoch": 19.21951219512195,
      "grad_norm": 0.14811298251152039,
      "learning_rate": 0.00015615384615384615,
      "loss": 0.006,
      "step": 197
    },
    {
      "epoch": 19.317073170731707,
      "grad_norm": 0.15298143029212952,
      "learning_rate": 0.0001553846153846154,
      "loss": 0.0061,
      "step": 198
    },
    {
      "epoch": 19.414634146341463,
      "grad_norm": 0.12869910895824432,
      "learning_rate": 0.00015461538461538458,
      "loss": 0.0058,
      "step": 199
    },
    {
      "epoch": 19.51219512195122,
      "grad_norm": 0.16457808017730713,
      "learning_rate": 0.00015384615384615382,
      "loss": 0.0065,
      "step": 200
    },
    {
      "epoch": 19.609756097560975,
      "grad_norm": 0.13335539400577545,
      "learning_rate": 0.00015307692307692304,
      "loss": 0.0055,
      "step": 201
    },
    {
      "epoch": 19.70731707317073,
      "grad_norm": 0.15908049046993256,
      "learning_rate": 0.00015230769230769228,
      "loss": 0.0071,
      "step": 202
    },
    {
      "epoch": 19.804878048780488,
      "grad_norm": 0.18201498687267303,
      "learning_rate": 0.00015153846153846153,
      "loss": 0.0062,
      "step": 203
    },
    {
      "epoch": 19.902439024390244,
      "grad_norm": 0.11706461012363434,
      "learning_rate": 0.00015076923076923074,
      "loss": 0.0051,
      "step": 204
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.21391364932060242,
      "learning_rate": 0.00015,
      "loss": 0.0083,
      "step": 205
    },
    {
      "epoch": 20.097560975609756,
      "grad_norm": 0.18242637813091278,
      "learning_rate": 0.00014923076923076923,
      "loss": 0.0061,
      "step": 206
    },
    {
      "epoch": 20.195121951219512,
      "grad_norm": 0.1223040446639061,
      "learning_rate": 0.00014846153846153845,
      "loss": 0.0049,
      "step": 207
    },
    {
      "epoch": 20.29268292682927,
      "grad_norm": 0.12085290998220444,
      "learning_rate": 0.0001476923076923077,
      "loss": 0.0044,
      "step": 208
    },
    {
      "epoch": 20.390243902439025,
      "grad_norm": 0.1390818953514099,
      "learning_rate": 0.0001469230769230769,
      "loss": 0.0041,
      "step": 209
    },
    {
      "epoch": 20.48780487804878,
      "grad_norm": 0.1521812379360199,
      "learning_rate": 0.00014615384615384615,
      "loss": 0.0048,
      "step": 210
    },
    {
      "epoch": 20.585365853658537,
      "grad_norm": 0.19469846785068512,
      "learning_rate": 0.00014538461538461537,
      "loss": 0.0063,
      "step": 211
    },
    {
      "epoch": 20.682926829268293,
      "grad_norm": 0.2664961516857147,
      "learning_rate": 0.0001446153846153846,
      "loss": 0.0083,
      "step": 212
    },
    {
      "epoch": 20.78048780487805,
      "grad_norm": 0.10694804787635803,
      "learning_rate": 0.00014384615384615383,
      "loss": 0.0042,
      "step": 213
    },
    {
      "epoch": 20.878048780487806,
      "grad_norm": 0.1559046059846878,
      "learning_rate": 0.00014307692307692307,
      "loss": 0.0055,
      "step": 214
    },
    {
      "epoch": 20.975609756097562,
      "grad_norm": 0.13347753882408142,
      "learning_rate": 0.00014230769230769228,
      "loss": 0.0044,
      "step": 215
    },
    {
      "epoch": 21.073170731707318,
      "grad_norm": 0.21013303101062775,
      "learning_rate": 0.00014153846153846153,
      "loss": 0.0088,
      "step": 216
    },
    {
      "epoch": 21.170731707317074,
      "grad_norm": 0.12360604107379913,
      "learning_rate": 0.00014076923076923074,
      "loss": 0.0058,
      "step": 217
    },
    {
      "epoch": 21.26829268292683,
      "grad_norm": 0.09981042146682739,
      "learning_rate": 0.00014,
      "loss": 0.0036,
      "step": 218
    },
    {
      "epoch": 21.365853658536587,
      "grad_norm": 0.054985515773296356,
      "learning_rate": 0.00013923076923076923,
      "loss": 0.0035,
      "step": 219
    },
    {
      "epoch": 21.463414634146343,
      "grad_norm": 0.14250998198986053,
      "learning_rate": 0.00013846153846153845,
      "loss": 0.0045,
      "step": 220
    },
    {
      "epoch": 21.5609756097561,
      "grad_norm": 0.12123237550258636,
      "learning_rate": 0.00013769230769230766,
      "loss": 0.005,
      "step": 221
    },
    {
      "epoch": 21.658536585365855,
      "grad_norm": 0.06571654230356216,
      "learning_rate": 0.0001369230769230769,
      "loss": 0.0036,
      "step": 222
    },
    {
      "epoch": 21.75609756097561,
      "grad_norm": 0.20715154707431793,
      "learning_rate": 0.00013615384615384615,
      "loss": 0.0069,
      "step": 223
    },
    {
      "epoch": 21.853658536585368,
      "grad_norm": 0.1710038185119629,
      "learning_rate": 0.00013538461538461537,
      "loss": 0.005,
      "step": 224
    },
    {
      "epoch": 21.951219512195124,
      "grad_norm": 0.17628610134124756,
      "learning_rate": 0.0001346153846153846,
      "loss": 0.0062,
      "step": 225
    },
    {
      "epoch": 22.048780487804876,
      "grad_norm": 0.21827223896980286,
      "learning_rate": 0.00013384615384615385,
      "loss": 0.0077,
      "step": 226
    },
    {
      "epoch": 22.146341463414632,
      "grad_norm": 0.061781205236911774,
      "learning_rate": 0.00013307692307692307,
      "loss": 0.0039,
      "step": 227
    },
    {
      "epoch": 22.24390243902439,
      "grad_norm": 0.0737641230225563,
      "learning_rate": 0.00013230769230769229,
      "loss": 0.0035,
      "step": 228
    },
    {
      "epoch": 22.341463414634145,
      "grad_norm": 0.08022163063287735,
      "learning_rate": 0.00013153846153846153,
      "loss": 0.0032,
      "step": 229
    },
    {
      "epoch": 22.4390243902439,
      "grad_norm": 0.1344948261976242,
      "learning_rate": 0.00013076923076923077,
      "loss": 0.0029,
      "step": 230
    },
    {
      "epoch": 22.536585365853657,
      "grad_norm": 0.21718360483646393,
      "learning_rate": 0.00013,
      "loss": 0.0058,
      "step": 231
    },
    {
      "epoch": 22.634146341463413,
      "grad_norm": 0.12511593103408813,
      "learning_rate": 0.00012923076923076923,
      "loss": 0.0061,
      "step": 232
    },
    {
      "epoch": 22.73170731707317,
      "grad_norm": 0.11083739250898361,
      "learning_rate": 0.00012846153846153845,
      "loss": 0.0037,
      "step": 233
    },
    {
      "epoch": 22.829268292682926,
      "grad_norm": 0.07160737365484238,
      "learning_rate": 0.00012769230769230766,
      "loss": 0.0036,
      "step": 234
    },
    {
      "epoch": 22.926829268292682,
      "grad_norm": 0.08976711332798004,
      "learning_rate": 0.0001269230769230769,
      "loss": 0.0047,
      "step": 235
    },
    {
      "epoch": 23.024390243902438,
      "grad_norm": 0.11744901537895203,
      "learning_rate": 0.00012615384615384615,
      "loss": 0.0062,
      "step": 236
    },
    {
      "epoch": 23.121951219512194,
      "grad_norm": 0.1935085654258728,
      "learning_rate": 0.00012538461538461537,
      "loss": 0.0062,
      "step": 237
    },
    {
      "epoch": 23.21951219512195,
      "grad_norm": 0.05573684722185135,
      "learning_rate": 0.0001246153846153846,
      "loss": 0.0031,
      "step": 238
    },
    {
      "epoch": 23.317073170731707,
      "grad_norm": 0.17934264242649078,
      "learning_rate": 0.00012384615384615383,
      "loss": 0.0047,
      "step": 239
    },
    {
      "epoch": 23.414634146341463,
      "grad_norm": 0.07053852826356888,
      "learning_rate": 0.00012307692307692307,
      "loss": 0.0038,
      "step": 240
    },
    {
      "epoch": 23.51219512195122,
      "grad_norm": 0.10860396176576614,
      "learning_rate": 0.0001223076923076923,
      "loss": 0.0036,
      "step": 241
    },
    {
      "epoch": 23.609756097560975,
      "grad_norm": 0.14227239787578583,
      "learning_rate": 0.00012153846153846153,
      "loss": 0.0046,
      "step": 242
    },
    {
      "epoch": 23.70731707317073,
      "grad_norm": 0.06873846054077148,
      "learning_rate": 0.00012076923076923076,
      "loss": 0.0029,
      "step": 243
    },
    {
      "epoch": 23.804878048780488,
      "grad_norm": 0.07374627143144608,
      "learning_rate": 0.00011999999999999999,
      "loss": 0.0035,
      "step": 244
    },
    {
      "epoch": 23.902439024390244,
      "grad_norm": 0.06644406169652939,
      "learning_rate": 0.00011923076923076922,
      "loss": 0.0034,
      "step": 245
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.21576206386089325,
      "learning_rate": 0.00011846153846153845,
      "loss": 0.006,
      "step": 246
    },
    {
      "epoch": 24.097560975609756,
      "grad_norm": 0.050486378371715546,
      "learning_rate": 0.00011769230769230768,
      "loss": 0.0034,
      "step": 247
    },
    {
      "epoch": 24.195121951219512,
      "grad_norm": 0.07521204650402069,
      "learning_rate": 0.00011692307692307691,
      "loss": 0.0038,
      "step": 248
    },
    {
      "epoch": 24.29268292682927,
      "grad_norm": 0.07287891209125519,
      "learning_rate": 0.00011615384615384615,
      "loss": 0.0029,
      "step": 249
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 0.09718434512615204,
      "learning_rate": 0.00011538461538461538,
      "loss": 0.0034,
      "step": 250
    },
    {
      "epoch": 24.48780487804878,
      "grad_norm": 0.08999607712030411,
      "learning_rate": 0.0001146153846153846,
      "loss": 0.0027,
      "step": 251
    },
    {
      "epoch": 24.585365853658537,
      "grad_norm": 0.06896111369132996,
      "learning_rate": 0.00011384615384615383,
      "loss": 0.003,
      "step": 252
    },
    {
      "epoch": 24.682926829268293,
      "grad_norm": 0.06402768939733505,
      "learning_rate": 0.00011307692307692307,
      "loss": 0.0028,
      "step": 253
    },
    {
      "epoch": 24.78048780487805,
      "grad_norm": 0.1097211018204689,
      "learning_rate": 0.0001123076923076923,
      "loss": 0.0035,
      "step": 254
    },
    {
      "epoch": 24.878048780487806,
      "grad_norm": 0.04717998579144478,
      "learning_rate": 0.00011153846153846153,
      "loss": 0.0036,
      "step": 255
    },
    {
      "epoch": 24.975609756097562,
      "grad_norm": 0.09648976475000381,
      "learning_rate": 0.00011076923076923076,
      "loss": 0.0039,
      "step": 256
    },
    {
      "epoch": 25.073170731707318,
      "grad_norm": 0.1949314922094345,
      "learning_rate": 0.00010999999999999998,
      "loss": 0.0051,
      "step": 257
    },
    {
      "epoch": 25.170731707317074,
      "grad_norm": 0.0910739153623581,
      "learning_rate": 0.00010923076923076922,
      "loss": 0.0031,
      "step": 258
    },
    {
      "epoch": 25.26829268292683,
      "grad_norm": 0.190155029296875,
      "learning_rate": 0.00010846153846153845,
      "loss": 0.0032,
      "step": 259
    },
    {
      "epoch": 25.365853658536587,
      "grad_norm": 0.060113243758678436,
      "learning_rate": 0.00010769230769230768,
      "loss": 0.003,
      "step": 260
    },
    {
      "epoch": 25.463414634146343,
      "grad_norm": 0.04832625016570091,
      "learning_rate": 0.00010692307692307692,
      "loss": 0.0025,
      "step": 261
    },
    {
      "epoch": 25.5609756097561,
      "grad_norm": 0.08690734952688217,
      "learning_rate": 0.00010615384615384615,
      "loss": 0.0036,
      "step": 262
    },
    {
      "epoch": 25.658536585365855,
      "grad_norm": 0.06999407708644867,
      "learning_rate": 0.00010538461538461537,
      "loss": 0.0035,
      "step": 263
    },
    {
      "epoch": 25.75609756097561,
      "grad_norm": 0.08342140913009644,
      "learning_rate": 0.0001046153846153846,
      "loss": 0.0029,
      "step": 264
    },
    {
      "epoch": 25.853658536585368,
      "grad_norm": 0.11080503463745117,
      "learning_rate": 0.00010384615384615383,
      "loss": 0.0032,
      "step": 265
    },
    {
      "epoch": 25.951219512195124,
      "grad_norm": 0.053908318281173706,
      "learning_rate": 0.00010307692307692307,
      "loss": 0.0028,
      "step": 266
    },
    {
      "epoch": 26.048780487804876,
      "grad_norm": 0.1892625391483307,
      "learning_rate": 0.0001023076923076923,
      "loss": 0.0065,
      "step": 267
    },
    {
      "epoch": 26.146341463414632,
      "grad_norm": 0.044477999210357666,
      "learning_rate": 0.00010153846153846153,
      "loss": 0.0027,
      "step": 268
    },
    {
      "epoch": 26.24390243902439,
      "grad_norm": 0.04847439378499985,
      "learning_rate": 0.00010076923076923075,
      "loss": 0.0027,
      "step": 269
    },
    {
      "epoch": 26.341463414634145,
      "grad_norm": 0.10920931398868561,
      "learning_rate": 9.999999999999999e-05,
      "loss": 0.0035,
      "step": 270
    },
    {
      "epoch": 26.4390243902439,
      "grad_norm": 0.06852617859840393,
      "learning_rate": 9.923076923076922e-05,
      "loss": 0.0029,
      "step": 271
    },
    {
      "epoch": 26.536585365853657,
      "grad_norm": 0.07692811638116837,
      "learning_rate": 9.846153846153845e-05,
      "loss": 0.0031,
      "step": 272
    },
    {
      "epoch": 26.634146341463413,
      "grad_norm": 0.10554207116365433,
      "learning_rate": 9.769230769230768e-05,
      "loss": 0.0044,
      "step": 273
    },
    {
      "epoch": 26.73170731707317,
      "grad_norm": 0.05735970288515091,
      "learning_rate": 9.692307692307692e-05,
      "loss": 0.0023,
      "step": 274
    },
    {
      "epoch": 26.829268292682926,
      "grad_norm": 0.08315723389387131,
      "learning_rate": 9.615384615384615e-05,
      "loss": 0.0041,
      "step": 275
    },
    {
      "epoch": 26.926829268292682,
      "grad_norm": 0.05595504119992256,
      "learning_rate": 9.538461538461537e-05,
      "loss": 0.0028,
      "step": 276
    },
    {
      "epoch": 27.024390243902438,
      "grad_norm": 0.07014201581478119,
      "learning_rate": 9.46153846153846e-05,
      "loss": 0.0036,
      "step": 277
    },
    {
      "epoch": 27.121951219512194,
      "grad_norm": 0.05355706810951233,
      "learning_rate": 9.384615384615384e-05,
      "loss": 0.0016,
      "step": 278
    },
    {
      "epoch": 27.21951219512195,
      "grad_norm": 0.05720780789852142,
      "learning_rate": 9.307692307692307e-05,
      "loss": 0.0027,
      "step": 279
    },
    {
      "epoch": 27.317073170731707,
      "grad_norm": 0.06259215623140335,
      "learning_rate": 9.23076923076923e-05,
      "loss": 0.003,
      "step": 280
    },
    {
      "epoch": 27.414634146341463,
      "grad_norm": 0.041881661862134933,
      "learning_rate": 9.153846153846153e-05,
      "loss": 0.0021,
      "step": 281
    },
    {
      "epoch": 27.51219512195122,
      "grad_norm": 0.044630326330661774,
      "learning_rate": 9.076923076923075e-05,
      "loss": 0.0025,
      "step": 282
    },
    {
      "epoch": 27.609756097560975,
      "grad_norm": 0.04635176807641983,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.0027,
      "step": 283
    },
    {
      "epoch": 27.70731707317073,
      "grad_norm": 0.06779193133115768,
      "learning_rate": 8.923076923076922e-05,
      "loss": 0.0028,
      "step": 284
    },
    {
      "epoch": 27.804878048780488,
      "grad_norm": 0.05590251460671425,
      "learning_rate": 8.846153846153845e-05,
      "loss": 0.003,
      "step": 285
    },
    {
      "epoch": 27.902439024390244,
      "grad_norm": 0.09179370850324631,
      "learning_rate": 8.76923076923077e-05,
      "loss": 0.0037,
      "step": 286
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.10675082355737686,
      "learning_rate": 8.692307692307692e-05,
      "loss": 0.0038,
      "step": 287
    },
    {
      "epoch": 28.097560975609756,
      "grad_norm": 0.045973051339387894,
      "learning_rate": 8.615384615384614e-05,
      "loss": 0.0029,
      "step": 288
    },
    {
      "epoch": 28.195121951219512,
      "grad_norm": 0.04690638557076454,
      "learning_rate": 8.538461538461537e-05,
      "loss": 0.0022,
      "step": 289
    },
    {
      "epoch": 28.29268292682927,
      "grad_norm": 0.08696191012859344,
      "learning_rate": 8.46153846153846e-05,
      "loss": 0.0031,
      "step": 290
    },
    {
      "epoch": 28.390243902439025,
      "grad_norm": 0.04060940444469452,
      "learning_rate": 8.384615384615384e-05,
      "loss": 0.0032,
      "step": 291
    },
    {
      "epoch": 28.48780487804878,
      "grad_norm": 0.03713899850845337,
      "learning_rate": 8.307692307692307e-05,
      "loss": 0.002,
      "step": 292
    },
    {
      "epoch": 28.585365853658537,
      "grad_norm": 0.029087984934449196,
      "learning_rate": 8.23076923076923e-05,
      "loss": 0.0013,
      "step": 293
    },
    {
      "epoch": 28.682926829268293,
      "grad_norm": 0.07473056763410568,
      "learning_rate": 8.153846153846152e-05,
      "loss": 0.0027,
      "step": 294
    },
    {
      "epoch": 28.78048780487805,
      "grad_norm": 0.04304232448339462,
      "learning_rate": 8.076923076923076e-05,
      "loss": 0.002,
      "step": 295
    },
    {
      "epoch": 28.878048780487806,
      "grad_norm": 0.04454323276877403,
      "learning_rate": 7.999999999999999e-05,
      "loss": 0.0027,
      "step": 296
    },
    {
      "epoch": 28.975609756097562,
      "grad_norm": 0.05329924821853638,
      "learning_rate": 7.923076923076922e-05,
      "loss": 0.0029,
      "step": 297
    },
    {
      "epoch": 29.073170731707318,
      "grad_norm": 0.09062262624502182,
      "learning_rate": 7.846153846153845e-05,
      "loss": 0.0033,
      "step": 298
    },
    {
      "epoch": 29.170731707317074,
      "grad_norm": 0.05592973902821541,
      "learning_rate": 7.76923076923077e-05,
      "loss": 0.0029,
      "step": 299
    },
    {
      "epoch": 29.26829268292683,
      "grad_norm": 0.044346895068883896,
      "learning_rate": 7.692307692307691e-05,
      "loss": 0.0017,
      "step": 300
    },
    {
      "epoch": 29.365853658536587,
      "grad_norm": 0.0707179456949234,
      "learning_rate": 7.615384615384614e-05,
      "loss": 0.0029,
      "step": 301
    },
    {
      "epoch": 29.463414634146343,
      "grad_norm": 0.03876892477273941,
      "learning_rate": 7.538461538461537e-05,
      "loss": 0.002,
      "step": 302
    },
    {
      "epoch": 29.5609756097561,
      "grad_norm": 0.03855730965733528,
      "learning_rate": 7.461538461538462e-05,
      "loss": 0.0023,
      "step": 303
    },
    {
      "epoch": 29.658536585365855,
      "grad_norm": 0.03984161093831062,
      "learning_rate": 7.384615384615384e-05,
      "loss": 0.0021,
      "step": 304
    },
    {
      "epoch": 29.75609756097561,
      "grad_norm": 0.11723265051841736,
      "learning_rate": 7.307692307692307e-05,
      "loss": 0.004,
      "step": 305
    },
    {
      "epoch": 29.853658536585368,
      "grad_norm": 0.046631816774606705,
      "learning_rate": 7.23076923076923e-05,
      "loss": 0.0023,
      "step": 306
    },
    {
      "epoch": 29.951219512195124,
      "grad_norm": 0.04022593796253204,
      "learning_rate": 7.153846153846153e-05,
      "loss": 0.0019,
      "step": 307
    },
    {
      "epoch": 30.048780487804876,
      "grad_norm": 0.13396456837654114,
      "learning_rate": 7.076923076923076e-05,
      "loss": 0.0052,
      "step": 308
    },
    {
      "epoch": 30.146341463414632,
      "grad_norm": 0.045965906232595444,
      "learning_rate": 7e-05,
      "loss": 0.0024,
      "step": 309
    },
    {
      "epoch": 30.24390243902439,
      "grad_norm": 0.05475849658250809,
      "learning_rate": 6.923076923076922e-05,
      "loss": 0.0025,
      "step": 310
    },
    {
      "epoch": 30.341463414634145,
      "grad_norm": 0.05342675372958183,
      "learning_rate": 6.846153846153845e-05,
      "loss": 0.002,
      "step": 311
    },
    {
      "epoch": 30.4390243902439,
      "grad_norm": 0.11388915777206421,
      "learning_rate": 6.769230769230768e-05,
      "loss": 0.0027,
      "step": 312
    },
    {
      "epoch": 30.536585365853657,
      "grad_norm": 0.04718553274869919,
      "learning_rate": 6.692307692307693e-05,
      "loss": 0.0022,
      "step": 313
    },
    {
      "epoch": 30.634146341463413,
      "grad_norm": 0.042263057082891464,
      "learning_rate": 6.615384615384614e-05,
      "loss": 0.0022,
      "step": 314
    },
    {
      "epoch": 30.73170731707317,
      "grad_norm": 0.05214238539338112,
      "learning_rate": 6.538461538461539e-05,
      "loss": 0.003,
      "step": 315
    },
    {
      "epoch": 30.829268292682926,
      "grad_norm": 0.06639880686998367,
      "learning_rate": 6.461538461538462e-05,
      "loss": 0.0026,
      "step": 316
    },
    {
      "epoch": 30.926829268292682,
      "grad_norm": 0.043850094079971313,
      "learning_rate": 6.384615384615383e-05,
      "loss": 0.002,
      "step": 317
    },
    {
      "epoch": 31.024390243902438,
      "grad_norm": 0.11537597328424454,
      "learning_rate": 6.307692307692308e-05,
      "loss": 0.0054,
      "step": 318
    },
    {
      "epoch": 31.121951219512194,
      "grad_norm": 0.04962730407714844,
      "learning_rate": 6.23076923076923e-05,
      "loss": 0.0026,
      "step": 319
    },
    {
      "epoch": 31.21951219512195,
      "grad_norm": 0.04793107137084007,
      "learning_rate": 6.153846153846154e-05,
      "loss": 0.0026,
      "step": 320
    },
    {
      "epoch": 31.317073170731707,
      "grad_norm": 0.05299641191959381,
      "learning_rate": 6.0769230769230765e-05,
      "loss": 0.0027,
      "step": 321
    },
    {
      "epoch": 31.414634146341463,
      "grad_norm": 0.05395784229040146,
      "learning_rate": 5.9999999999999995e-05,
      "loss": 0.0022,
      "step": 322
    },
    {
      "epoch": 31.51219512195122,
      "grad_norm": 0.05085204914212227,
      "learning_rate": 5.9230769230769225e-05,
      "loss": 0.0022,
      "step": 323
    },
    {
      "epoch": 31.609756097560975,
      "grad_norm": 0.06026561185717583,
      "learning_rate": 5.8461538461538454e-05,
      "loss": 0.0025,
      "step": 324
    },
    {
      "epoch": 31.70731707317073,
      "grad_norm": 0.041444700211286545,
      "learning_rate": 5.769230769230769e-05,
      "loss": 0.0024,
      "step": 325
    },
    {
      "epoch": 31.804878048780488,
      "grad_norm": 0.049821823835372925,
      "learning_rate": 5.6923076923076914e-05,
      "loss": 0.0023,
      "step": 326
    },
    {
      "epoch": 31.902439024390244,
      "grad_norm": 0.14033843576908112,
      "learning_rate": 5.615384615384615e-05,
      "loss": 0.0018,
      "step": 327
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.09228775650262833,
      "learning_rate": 5.538461538461538e-05,
      "loss": 0.0047,
      "step": 328
    },
    {
      "epoch": 32.09756097560975,
      "grad_norm": 0.04064151644706726,
      "learning_rate": 5.461538461538461e-05,
      "loss": 0.0021,
      "step": 329
    },
    {
      "epoch": 32.19512195121951,
      "grad_norm": 0.03897314518690109,
      "learning_rate": 5.384615384615384e-05,
      "loss": 0.0026,
      "step": 330
    },
    {
      "epoch": 32.292682926829265,
      "grad_norm": 0.042987626045942307,
      "learning_rate": 5.3076923076923076e-05,
      "loss": 0.0021,
      "step": 331
    },
    {
      "epoch": 32.390243902439025,
      "grad_norm": 0.05065033957362175,
      "learning_rate": 5.23076923076923e-05,
      "loss": 0.0022,
      "step": 332
    },
    {
      "epoch": 32.48780487804878,
      "grad_norm": 0.045515626668930054,
      "learning_rate": 5.1538461538461536e-05,
      "loss": 0.002,
      "step": 333
    },
    {
      "epoch": 32.58536585365854,
      "grad_norm": 0.04399142414331436,
      "learning_rate": 5.0769230769230766e-05,
      "loss": 0.0021,
      "step": 334
    },
    {
      "epoch": 32.68292682926829,
      "grad_norm": 0.07398364692926407,
      "learning_rate": 4.9999999999999996e-05,
      "loss": 0.0025,
      "step": 335
    },
    {
      "epoch": 32.78048780487805,
      "grad_norm": 0.04242634028196335,
      "learning_rate": 4.9230769230769225e-05,
      "loss": 0.0025,
      "step": 336
    },
    {
      "epoch": 32.8780487804878,
      "grad_norm": 0.043755486607551575,
      "learning_rate": 4.846153846153846e-05,
      "loss": 0.0025,
      "step": 337
    },
    {
      "epoch": 32.97560975609756,
      "grad_norm": 0.041808608919382095,
      "learning_rate": 4.7692307692307685e-05,
      "loss": 0.0023,
      "step": 338
    },
    {
      "epoch": 33.073170731707314,
      "grad_norm": 0.12088567763566971,
      "learning_rate": 4.692307692307692e-05,
      "loss": 0.0047,
      "step": 339
    },
    {
      "epoch": 33.170731707317074,
      "grad_norm": 0.051383569836616516,
      "learning_rate": 4.615384615384615e-05,
      "loss": 0.0021,
      "step": 340
    },
    {
      "epoch": 33.26829268292683,
      "grad_norm": 0.0488077774643898,
      "learning_rate": 4.5384615384615374e-05,
      "loss": 0.0029,
      "step": 341
    },
    {
      "epoch": 33.36585365853659,
      "grad_norm": 0.035875871777534485,
      "learning_rate": 4.461538461538461e-05,
      "loss": 0.0026,
      "step": 342
    },
    {
      "epoch": 33.46341463414634,
      "grad_norm": 0.04228038713335991,
      "learning_rate": 4.384615384615385e-05,
      "loss": 0.0018,
      "step": 343
    },
    {
      "epoch": 33.5609756097561,
      "grad_norm": 0.05372331291437149,
      "learning_rate": 4.307692307692307e-05,
      "loss": 0.0023,
      "step": 344
    },
    {
      "epoch": 33.65853658536585,
      "grad_norm": 0.04052098095417023,
      "learning_rate": 4.23076923076923e-05,
      "loss": 0.0022,
      "step": 345
    },
    {
      "epoch": 33.75609756097561,
      "grad_norm": 0.04911820590496063,
      "learning_rate": 4.153846153846154e-05,
      "loss": 0.0024,
      "step": 346
    },
    {
      "epoch": 33.853658536585364,
      "grad_norm": 0.03887583315372467,
      "learning_rate": 4.076923076923076e-05,
      "loss": 0.0021,
      "step": 347
    },
    {
      "epoch": 33.951219512195124,
      "grad_norm": 0.04255398362874985,
      "learning_rate": 3.9999999999999996e-05,
      "loss": 0.0028,
      "step": 348
    },
    {
      "epoch": 34.048780487804876,
      "grad_norm": 0.06119697168469429,
      "learning_rate": 3.9230769230769226e-05,
      "loss": 0.002,
      "step": 349
    },
    {
      "epoch": 34.146341463414636,
      "grad_norm": 0.039664071053266525,
      "learning_rate": 3.8461538461538456e-05,
      "loss": 0.0023,
      "step": 350
    },
    {
      "epoch": 34.24390243902439,
      "grad_norm": 0.04274529963731766,
      "learning_rate": 3.7692307692307686e-05,
      "loss": 0.0023,
      "step": 351
    },
    {
      "epoch": 34.34146341463415,
      "grad_norm": 0.0462997667491436,
      "learning_rate": 3.692307692307692e-05,
      "loss": 0.0023,
      "step": 352
    },
    {
      "epoch": 34.4390243902439,
      "grad_norm": 0.04057782143354416,
      "learning_rate": 3.615384615384615e-05,
      "loss": 0.0024,
      "step": 353
    },
    {
      "epoch": 34.53658536585366,
      "grad_norm": 0.04275265708565712,
      "learning_rate": 3.538461538461538e-05,
      "loss": 0.0026,
      "step": 354
    },
    {
      "epoch": 34.63414634146341,
      "grad_norm": 0.04322094842791557,
      "learning_rate": 3.461538461538461e-05,
      "loss": 0.0024,
      "step": 355
    },
    {
      "epoch": 34.73170731707317,
      "grad_norm": 0.038884177803993225,
      "learning_rate": 3.384615384615384e-05,
      "loss": 0.0022,
      "step": 356
    },
    {
      "epoch": 34.829268292682926,
      "grad_norm": 0.047525789588689804,
      "learning_rate": 3.307692307692307e-05,
      "loss": 0.0026,
      "step": 357
    },
    {
      "epoch": 34.926829268292686,
      "grad_norm": 0.04596763476729393,
      "learning_rate": 3.230769230769231e-05,
      "loss": 0.0018,
      "step": 358
    },
    {
      "epoch": 35.02439024390244,
      "grad_norm": 0.061601899564266205,
      "learning_rate": 3.153846153846154e-05,
      "loss": 0.0024,
      "step": 359
    },
    {
      "epoch": 35.1219512195122,
      "grad_norm": 0.05028028413653374,
      "learning_rate": 3.076923076923077e-05,
      "loss": 0.0025,
      "step": 360
    },
    {
      "epoch": 35.21951219512195,
      "grad_norm": 0.04337937384843826,
      "learning_rate": 2.9999999999999997e-05,
      "loss": 0.0024,
      "step": 361
    },
    {
      "epoch": 35.31707317073171,
      "grad_norm": 0.03966502472758293,
      "learning_rate": 2.9230769230769227e-05,
      "loss": 0.0017,
      "step": 362
    },
    {
      "epoch": 35.41463414634146,
      "grad_norm": 0.034202031791210175,
      "learning_rate": 2.8461538461538457e-05,
      "loss": 0.0016,
      "step": 363
    },
    {
      "epoch": 35.51219512195122,
      "grad_norm": 0.03951697051525116,
      "learning_rate": 2.769230769230769e-05,
      "loss": 0.0021,
      "step": 364
    },
    {
      "epoch": 35.609756097560975,
      "grad_norm": 0.05304348096251488,
      "learning_rate": 2.692307692307692e-05,
      "loss": 0.002,
      "step": 365
    },
    {
      "epoch": 35.707317073170735,
      "grad_norm": 0.03674296289682388,
      "learning_rate": 2.615384615384615e-05,
      "loss": 0.0018,
      "step": 366
    },
    {
      "epoch": 35.80487804878049,
      "grad_norm": 0.04152822494506836,
      "learning_rate": 2.5384615384615383e-05,
      "loss": 0.0025,
      "step": 367
    },
    {
      "epoch": 35.90243902439025,
      "grad_norm": 0.04095344617962837,
      "learning_rate": 2.4615384615384613e-05,
      "loss": 0.0026,
      "step": 368
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.11404453963041306,
      "learning_rate": 2.3846153846153843e-05,
      "loss": 0.0057,
      "step": 369
    },
    {
      "epoch": 36.09756097560975,
      "grad_norm": 0.05595753341913223,
      "learning_rate": 2.3076923076923076e-05,
      "loss": 0.0023,
      "step": 370
    },
    {
      "epoch": 36.19512195121951,
      "grad_norm": 0.04070118069648743,
      "learning_rate": 2.2307692307692305e-05,
      "loss": 0.0023,
      "step": 371
    },
    {
      "epoch": 36.292682926829265,
      "grad_norm": 0.04095188528299332,
      "learning_rate": 2.1538461538461535e-05,
      "loss": 0.002,
      "step": 372
    },
    {
      "epoch": 36.390243902439025,
      "grad_norm": 0.04720524698495865,
      "learning_rate": 2.076923076923077e-05,
      "loss": 0.0023,
      "step": 373
    },
    {
      "epoch": 36.48780487804878,
      "grad_norm": 0.040464211255311966,
      "learning_rate": 1.9999999999999998e-05,
      "loss": 0.0018,
      "step": 374
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 0.05260774865746498,
      "learning_rate": 1.9230769230769228e-05,
      "loss": 0.0026,
      "step": 375
    },
    {
      "epoch": 36.68292682926829,
      "grad_norm": 0.042890142649412155,
      "learning_rate": 1.846153846153846e-05,
      "loss": 0.0018,
      "step": 376
    },
    {
      "epoch": 36.78048780487805,
      "grad_norm": 0.04100382328033447,
      "learning_rate": 1.769230769230769e-05,
      "loss": 0.0024,
      "step": 377
    },
    {
      "epoch": 36.8780487804878,
      "grad_norm": 0.0390608087182045,
      "learning_rate": 1.692307692307692e-05,
      "loss": 0.0018,
      "step": 378
    },
    {
      "epoch": 36.97560975609756,
      "grad_norm": 0.050791941583156586,
      "learning_rate": 1.6153846153846154e-05,
      "loss": 0.0027,
      "step": 379
    },
    {
      "epoch": 37.073170731707314,
      "grad_norm": 0.07499855756759644,
      "learning_rate": 1.5384615384615384e-05,
      "loss": 0.0028,
      "step": 380
    },
    {
      "epoch": 37.170731707317074,
      "grad_norm": 0.04554305970668793,
      "learning_rate": 1.4615384615384614e-05,
      "loss": 0.0017,
      "step": 381
    },
    {
      "epoch": 37.26829268292683,
      "grad_norm": 0.04392881318926811,
      "learning_rate": 1.3846153846153845e-05,
      "loss": 0.0019,
      "step": 382
    },
    {
      "epoch": 37.36585365853659,
      "grad_norm": 0.045052140951156616,
      "learning_rate": 1.3076923076923075e-05,
      "loss": 0.0017,
      "step": 383
    },
    {
      "epoch": 37.46341463414634,
      "grad_norm": 0.043781619518995285,
      "learning_rate": 1.2307692307692306e-05,
      "loss": 0.0022,
      "step": 384
    },
    {
      "epoch": 37.5609756097561,
      "grad_norm": 0.042604900896549225,
      "learning_rate": 1.1538461538461538e-05,
      "loss": 0.0018,
      "step": 385
    },
    {
      "epoch": 37.65853658536585,
      "grad_norm": 0.0342622809112072,
      "learning_rate": 1.0769230769230768e-05,
      "loss": 0.0019,
      "step": 386
    },
    {
      "epoch": 37.75609756097561,
      "grad_norm": 0.044928885996341705,
      "learning_rate": 9.999999999999999e-06,
      "loss": 0.0025,
      "step": 387
    },
    {
      "epoch": 37.853658536585364,
      "grad_norm": 0.0492098405957222,
      "learning_rate": 9.23076923076923e-06,
      "loss": 0.0024,
      "step": 388
    },
    {
      "epoch": 37.951219512195124,
      "grad_norm": 0.04556036368012428,
      "learning_rate": 8.46153846153846e-06,
      "loss": 0.0027,
      "step": 389
    },
    {
      "epoch": 38.048780487804876,
      "grad_norm": 0.11135473847389221,
      "learning_rate": 7.692307692307692e-06,
      "loss": 0.0044,
      "step": 390
    },
    {
      "epoch": 38.146341463414636,
      "grad_norm": 0.03255600482225418,
      "learning_rate": 6.9230769230769225e-06,
      "loss": 0.0018,
      "step": 391
    },
    {
      "epoch": 38.24390243902439,
      "grad_norm": 0.042040884494781494,
      "learning_rate": 6.153846153846153e-06,
      "loss": 0.0021,
      "step": 392
    },
    {
      "epoch": 38.34146341463415,
      "grad_norm": 0.043250035494565964,
      "learning_rate": 5.384615384615384e-06,
      "loss": 0.0023,
      "step": 393
    },
    {
      "epoch": 38.4390243902439,
      "grad_norm": 0.044460248202085495,
      "learning_rate": 4.615384615384615e-06,
      "loss": 0.0028,
      "step": 394
    },
    {
      "epoch": 38.53658536585366,
      "grad_norm": 0.04101918265223503,
      "learning_rate": 3.846153846153846e-06,
      "loss": 0.0017,
      "step": 395
    },
    {
      "epoch": 38.63414634146341,
      "grad_norm": 0.05923481285572052,
      "learning_rate": 3.0769230769230766e-06,
      "loss": 0.0024,
      "step": 396
    },
    {
      "epoch": 38.73170731707317,
      "grad_norm": 0.03242106735706329,
      "learning_rate": 2.3076923076923077e-06,
      "loss": 0.0015,
      "step": 397
    },
    {
      "epoch": 38.829268292682926,
      "grad_norm": 0.04717056825757027,
      "learning_rate": 1.5384615384615383e-06,
      "loss": 0.0026,
      "step": 398
    },
    {
      "epoch": 38.926829268292686,
      "grad_norm": 0.037750449031591415,
      "learning_rate": 7.692307692307691e-07,
      "loss": 0.0019,
      "step": 399
    },
    {
      "epoch": 39.02439024390244,
      "grad_norm": 0.07845955342054367,
      "learning_rate": 0.0,
      "loss": 0.0035,
      "step": 400
    }
  ],
  "logging_steps": 1,
  "max_steps": 400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.58968585846784e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
